{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pytorch_forecasting as pf\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, SwinModel\n",
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for sampling \n",
    "num_samples = None\n",
    "\n",
    "# Params for Train_test_split \n",
    "train_test_split_rto = 0.3\n",
    "\n",
    "# Dataset\n",
    "window_size = 30\n",
    "predict_length = 7\n",
    "batch_size = 64\n",
    "\n",
    "# Model\n",
    "d_model = 128\n",
    "dropout = 0.3\n",
    "nhead = 4\n",
    "num_layers = 4\n",
    "d_ff = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/066/0663713001.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/054/0541518023.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0505221004</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/050/0505221004.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687003</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/068/0685687003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687004</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/068/0685687004.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n",
       "3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n",
       "4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n",
       "\n",
       "      price  sales_channel_id                         img_path  is_valid  \n",
       "0  0.050831                 2  ./HnM/images/066/0663713001.jpg         1  \n",
       "1  0.030492                 2  ./HnM/images/054/0541518023.jpg         1  \n",
       "2  0.015237                 2  ./HnM/images/050/0505221004.jpg         1  \n",
       "3  0.016932                 2  ./HnM/images/068/0685687003.jpg         1  \n",
       "4  0.016932                 2  ./HnM/images/068/0685687004.jpg         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"HnM/transactions_train.csv\", dtype={\"article_id\":str})\n",
    "df_raw[\"img_path\"] = df_raw[\"article_id\"].apply(lambda x: f'./HnM/images/{x[:3]}/{x}.jpg')\n",
    "df_raw[\"is_valid\"] = df_raw[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104547\n",
      "20804\n"
     ]
    }
   ],
   "source": [
    "# More than quantile 0.9\n",
    "print(df_raw[\"article_id\"].nunique())\n",
    "q_valid_prod = df_raw.groupby([\"article_id\", \"t_dat\"], as_index=False).size().drop(\"size\", axis=1)\n",
    "q_valid_prod = q_valid_prod.groupby(\"article_id\").size().sort_values(ascending=False)\n",
    "q_valid_prod = q_valid_prod[q_valid_prod>q_valid_prod.quantile(0.8)].index\n",
    "print(len(q_valid_prod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/066/0663713001.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/054/0541518023.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...</td>\n",
       "      <td>0688873012</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>1</td>\n",
       "      <td>./HnM/images/068/0688873012.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...</td>\n",
       "      <td>0501323011</td>\n",
       "      <td>0.053373</td>\n",
       "      <td>1</td>\n",
       "      <td>./HnM/images/050/0501323011.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...</td>\n",
       "      <td>0598859003</td>\n",
       "      <td>0.045746</td>\n",
       "      <td>2</td>\n",
       "      <td>./HnM/images/059/0598859003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...  0688873012   \n",
       "3  2018-09-20  00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...  0501323011   \n",
       "4  2018-09-20  00083cda041544b2fbb0e0d2905ad17da7cf1007526fb4...  0598859003   \n",
       "\n",
       "      price  sales_channel_id                         img_path  is_valid  \n",
       "0  0.050831                 2  ./HnM/images/066/0663713001.jpg         1  \n",
       "1  0.030492                 2  ./HnM/images/054/0541518023.jpg         1  \n",
       "2  0.030492                 1  ./HnM/images/068/0688873012.jpg         1  \n",
       "3  0.053373                 1  ./HnM/images/050/0501323011.jpg         1  \n",
       "4  0.045746                 2  ./HnM/images/059/0598859003.jpg         1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get valid data only\n",
    "df_sample = df_raw[\n",
    "    (df_raw[\"is_valid\"] == 1)\n",
    "    &(df_raw[\"article_id\"].isin(q_valid_prod))\n",
    "    ]\n",
    "\n",
    "# Get sample articld_id\n",
    "num_samples = num_samples if num_samples else df_sample[\"article_id\"].nunique()\n",
    "sample_id_li = df_sample.groupby([\"article_id\", \"t_dat\"], as_index=False).agg(_=(\"price\", \"count\")) # Get uinque dates per each article\n",
    "sample_id_li = sample_id_li.groupby(\"article_id\").size().sort_values(ascending=False)[:num_samples].index\n",
    "\n",
    "# Sample\n",
    "df_sample = df_sample[df_sample[\"article_id\"].isin(sample_id_li)].reset_index(drop=True)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To datetime\n",
    "df_prep = df_sample.copy()\n",
    "df_prep[\"t_dat\"] = pd.to_datetime(df_prep[\"t_dat\"])\n",
    "\n",
    "# Preprocess\n",
    "### Explode dates\n",
    "def func(x):\n",
    "    full_date = pd.DataFrame(pd.date_range(x[\"t_dat\"].min(), x[\"t_dat\"].max(), freq=\"d\"), columns=[\"t_dat\"])\n",
    "    x = x.merge(full_date, on=\"t_dat\", how=\"right\").reset_index(drop=True)\n",
    "    x[\"article_id\"] = x[\"article_id\"].unique()[0]\n",
    "    x[\"sales\"] = x[\"sales\"].fillna(0)\n",
    "    return x\n",
    "\n",
    "df_prep = df_prep.groupby([\"article_id\", \"t_dat\"], as_index=False).agg(sales=(\"price\", \"count\"))\n",
    "df_prep = df_prep.groupby(\"article_id\", as_index=False).apply(lambda x: func(x)).reset_index(drop=True)\n",
    "df_prep[\"time_idx\"] = df_prep.groupby(\"article_id\").cumcount()\n",
    "\n",
    "### LabelEncode image path\n",
    "imgpath_encoder = LabelEncoder()\n",
    "df_prep[\"img_path\"] = df_prep[\"article_id\"].apply(lambda x: f'HnM/images/{x[:3]}/{x}.jpg')\n",
    "df_prep[\"img_path\"] = imgpath_encoder.fit_transform(df_prep[\"img_path\"])\n",
    "\n",
    "# Train test split\n",
    "num_train = int(np.round(num_samples * train_test_split_rto))\n",
    "sample_id_li_train = sample_id_li[:num_train]\n",
    "\n",
    "df_train = df_prep[df_prep[\"article_id\"].isin(sample_id_li_train)].reset_index(drop=True)\n",
    "df_valid = df_prep[~df_prep[\"article_id\"].isin(sample_id_li_train)].reset_index(drop=True)\n",
    "assert df_train.shape[0] + df_valid.shape[0] == df_prep.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, imgpath_encoder, predict_length):\n",
    "        self.dataset = dataset\n",
    "        self.imgpath_encoder = imgpath_encoder\n",
    "        self.predict_length = predict_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset = self.dataset[idx][0][\"x_cont\"]\n",
    "        seq = dataset[:, -1]\n",
    "        x = seq[:-self.predict_length].unsqueeze(-1)\n",
    "        y = seq[-self.predict_length:].unsqueeze(-1)\n",
    "\n",
    "        img_path = dataset[:,0][0].type(torch.int).unsqueeze(0)\n",
    "        img_path = self.imgpath_encoder.inverse_transform(img_path)\n",
    "        print(img_path)\n",
    "        raise\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        for n, path in enumerate(img_path):\n",
    "            img = transform(Image.open(path).convert(\"RGB\"))\n",
    "            img_tensor = img if n == 0 else torch.vstack([img_tensor, img])\n",
    "        \n",
    "        return {\"x\":x, \"y\":y, \"img\":img_tensor}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HnM/images/010/0108775015.jpg']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/paper_v1.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m MultimodalDataset(valid_dataset_raw, imgpath_encoder, predict_length)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m valid_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(valid_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataset))\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/paper_v1.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgpath_encoder\u001b[39m.\u001b[39minverse_transform(img_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(img_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m n, path \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(img_path):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "train_dataset_raw = pf.TimeSeriesDataSet(\n",
    "    data=df_train,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"sales\",\n",
    "    group_ids=[\"article_id\"],\n",
    "    static_reals=[\"img_path\"],\n",
    "    min_encoder_length=window_size,\n",
    "    max_encoder_length=window_size,\n",
    "    min_prediction_idx=predict_length,\n",
    "    max_prediction_length=predict_length,\n",
    "    time_varying_unknown_reals=[\"sales\"],\n",
    "    # target_normalizer=None,\n",
    "    categorical_encoders={\n",
    "        \"article_id\": None,\n",
    "    },\n",
    "    scalers={\"img_path\":None}\n",
    ")\n",
    "valid_dataset_raw = pf.TimeSeriesDataSet.from_dataset(train_dataset_raw, df_valid)\n",
    "\n",
    "train_dataset = MultimodalDataset(train_dataset_raw, imgpath_encoder, predict_length)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = MultimodalDataset(valid_dataset_raw, imgpath_encoder, predict_length)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # PE(pos, 2i) = sin(pos/10000^{2i/d_model}), \n",
    "    # PE(pos, 2i+1) = cos(pos/10000^{2i/d_model})\n",
    "    def __init__(self, max_len, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).reshape(-1,1).to(device)\n",
    "        i = torch.arange(d_model).to(device)//2\n",
    "        exp_term = 2*i/d_model\n",
    "        div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "        self.pos_encoded = position / div_term\n",
    "\n",
    "        self.pos_encoded[:, 0::2] = torch.sin(self.pos_encoded[:, 0::2])\n",
    "        self.pos_encoded[:, 1::2] = torch.cos(self.pos_encoded[:, 1::2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.pos_encoded[:x.shape[1], :]\n",
    "        return self.dropout(output)\n",
    "    \n",
    "class Mask(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_padding_mask(self, arr):\n",
    "        res = torch.eq(arr, 0).type(torch.FloatTensor).to(device)\n",
    "        res = torch.where(res==1, -torch.inf, 0)\n",
    "        return res\n",
    "    \n",
    "    def get_lookahead_mask(self, arr):\n",
    "        seq_len = arr.shape[1]\n",
    "        mask = torch.triu(torch.ones((seq_len, seq_len))*-1e-9, 1).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, arr):\n",
    "        padding_mask = self.get_padding_mask(arr)\n",
    "        lookahead_mask = self.get_lookahead_mask(arr)\n",
    "        return padding_mask, lookahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformer(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, dropout, nhead, d_ff, num_layers, swin_transformer):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_mask = Mask()\n",
    "        self.linear1 = torch.nn.Linear(1, d_model)\n",
    "        self.enc_pos_encoding = PositionalEncoding(max_seq_len, d_model, dropout)\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        self.swin_transformer = swin_transformer\n",
    "        self.attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "        self.linear2 = torch.nn.Linear(self.swin_transformer.config.hidden_size, d_model)\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(d_model, d_model)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(d_model, d_model)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear3 = torch.nn.Linear(d_model*49, d_model)\n",
    "        self.linear4 = torch.nn.Linear(d_model, predict_length)\n",
    "    \n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # Encoding\n",
    "        # enc_padding_mask, _ = self.enc_mask(enc_input.squeeze())\n",
    "        linear1_ = self.linear1(enc_input)\n",
    "        enc_pos_encoding_ = self.enc_pos_encoding(linear1_)\n",
    "        # encoder_ = self.encoder(enc_pos_encoding_, src_key_padding_mask=enc_padding_mask)\n",
    "        encoder_ = self.encoder(enc_pos_encoding_)\n",
    "        \n",
    "        # Decoding\n",
    "        ### Self attention\n",
    "        swin_transformer_ = self.swin_transformer(dec_input).last_hidden_state\n",
    "        linear2_ = self.linear2(swin_transformer_)\n",
    "\n",
    "        ### Cross attention\n",
    "        attn_, attn_weight = self.attn(query=linear2_, key=encoder_, value=encoder_)\n",
    "        layernorm_ = self.layernorm(linear2_ + attn_)\n",
    "\n",
    "        ### Feed forward\n",
    "        relu1_ = self.relu1(self.fc1(layernorm_))\n",
    "        relu2_ = self.relu2(self.fc2(relu1_))\n",
    "\n",
    "        # Final\n",
    "        flatten_ = self.flatten(relu2_)\n",
    "        linear3_ = self.linear3(flatten_)\n",
    "        linear4_ = self.linear4(linear3_)\n",
    "        \n",
    "\n",
    "        return linear4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from transformers import SwinModel\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "swin_transformer = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "swin_transformer.to(device)\n",
    "\n",
    "model = MultimodalTransformer(window_size, d_model, dropout, nhead, d_ff, num_layers, swin_transformer)\n",
    "# model = torch.nn.DataParallel(model, output_device=1)\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 30, 1])\n",
      "torch.Size([64, 3, 224, 224])\n",
      "tensor([[-8.8018e-02, -5.8104e-02, -5.2780e-03,  5.5865e-02, -9.9650e-02,\n",
      "          9.1390e-02,  2.4734e-02],\n",
      "        [-2.0067e-02, -8.1871e-02, -4.9101e-02,  4.2356e-03, -1.1786e-01,\n",
      "          8.3833e-02,  3.1956e-03],\n",
      "        [-3.0159e-02, -2.9640e-02, -9.3836e-03,  1.2075e-02, -4.2337e-02,\n",
      "          5.9468e-02,  1.2219e-02],\n",
      "        [-3.4092e-02, -7.5626e-02, -4.7786e-02, -4.8521e-02, -6.4422e-02,\n",
      "          9.6413e-02,  1.6364e-02],\n",
      "        [-6.8928e-02, -6.4887e-02, -3.0160e-02,  7.4031e-03, -8.7608e-02,\n",
      "          1.3236e-01,  3.2139e-02],\n",
      "        [-2.8419e-02, -8.6400e-02, -6.4012e-02, -1.7384e-02, -1.0226e-01,\n",
      "          1.3433e-01, -1.1174e-01],\n",
      "        [-5.7114e-02, -8.5768e-02, -3.5214e-02, -8.7600e-02, -4.2149e-02,\n",
      "          1.0672e-01, -1.7981e-02],\n",
      "        [-6.6314e-02, -3.4071e-02, -2.0043e-02,  1.8899e-02, -4.9322e-02,\n",
      "          5.7989e-02,  1.6925e-02],\n",
      "        [-3.5268e-02, -1.5112e-02,  1.1598e-03,  8.3514e-03, -7.0878e-02,\n",
      "          1.4860e-01, -4.8862e-02],\n",
      "        [-3.0668e-02, -7.5663e-02, -3.2549e-02, -1.0203e-03, -7.8283e-02,\n",
      "          7.7088e-02,  4.6348e-02],\n",
      "        [-5.0014e-02, -3.4504e-02, -2.6257e-02,  4.1630e-02, -6.1322e-02,\n",
      "          1.4542e-01, -7.2923e-02],\n",
      "        [-7.8770e-02, -9.3109e-03, -3.7984e-02, -2.2621e-02, -5.7845e-02,\n",
      "          1.0685e-01, -3.4557e-02],\n",
      "        [-9.7699e-02, -7.7662e-02, -7.4084e-04, -3.1797e-04, -9.8834e-02,\n",
      "          1.0333e-01, -7.8936e-03],\n",
      "        [-5.0447e-02, -5.5410e-02, -3.5977e-02,  4.1035e-03, -5.6648e-02,\n",
      "          6.5266e-02,  1.6315e-02],\n",
      "        [-9.9341e-02, -6.9247e-02, -4.2288e-02, -4.1206e-03, -9.8729e-02,\n",
      "          1.2363e-01,  4.1647e-02],\n",
      "        [ 1.4338e-02, -1.9236e-02,  9.9246e-03,  4.5335e-02, -8.0378e-02,\n",
      "          1.6947e-01,  7.2701e-03],\n",
      "        [-8.7949e-02, -6.8690e-02,  2.4798e-02,  2.4835e-02, -6.0585e-02,\n",
      "          1.2393e-01,  1.2966e-02],\n",
      "        [-4.7595e-03, -5.8383e-02,  1.4906e-02, -2.1389e-02, -8.0677e-02,\n",
      "          9.0648e-02,  2.4902e-02],\n",
      "        [-3.4433e-02, -3.4363e-02, -6.2900e-03, -1.0060e-02, -2.5441e-02,\n",
      "          8.5316e-02,  6.6089e-04],\n",
      "        [-4.5057e-02, -4.9249e-02, -9.6609e-03,  5.7063e-03, -1.3818e-01,\n",
      "          9.5074e-02,  1.3900e-02],\n",
      "        [-6.4203e-02, -4.9897e-02,  3.1964e-02,  4.9862e-02, -7.0123e-02,\n",
      "          7.4041e-02,  5.3694e-03],\n",
      "        [-7.0747e-02, -7.5842e-02, -6.2438e-02,  1.3877e-02, -8.6998e-02,\n",
      "          1.3414e-01, -2.2562e-02],\n",
      "        [-5.7976e-02, -4.7140e-02,  4.6422e-03, -1.1390e-02, -6.9138e-02,\n",
      "          9.7658e-02,  3.7516e-02],\n",
      "        [-6.1432e-02, -4.0941e-02, -8.2925e-06,  2.5095e-02, -6.7943e-02,\n",
      "          1.1994e-01,  1.5581e-03],\n",
      "        [-6.5832e-02, -7.9824e-02, -3.9551e-02,  6.2490e-02, -1.1187e-01,\n",
      "          1.0786e-01,  4.3977e-02],\n",
      "        [-2.7971e-02, -1.8776e-02, -4.7869e-02,  3.7446e-02, -7.8393e-02,\n",
      "          1.4138e-01, -5.0536e-02],\n",
      "        [-7.3453e-02, -7.7232e-02,  6.1758e-03,  8.2475e-03, -1.1077e-01,\n",
      "          8.9215e-02,  5.3585e-02],\n",
      "        [-6.2284e-02, -3.8933e-02,  3.8352e-03, -1.7988e-02, -7.5417e-02,\n",
      "          5.3745e-02,  3.0132e-02],\n",
      "        [-8.1328e-02, -9.4750e-02, -7.8662e-02,  1.1914e-02, -1.2974e-01,\n",
      "          4.7566e-02, -4.8473e-03],\n",
      "        [-5.2572e-02, -7.7551e-02, -6.5561e-02,  2.0215e-02, -1.2028e-01,\n",
      "          9.7799e-02,  3.4829e-02],\n",
      "        [-5.5068e-02, -6.1700e-02, -2.4194e-02,  2.0864e-02, -4.7773e-02,\n",
      "          1.0358e-01, -2.9354e-02],\n",
      "        [-1.0916e-01, -6.6953e-02, -6.5901e-03,  2.4406e-02, -1.0942e-01,\n",
      "          1.3637e-01,  1.2326e-02],\n",
      "        [-7.4268e-02, -6.1830e-02, -1.6742e-02, -1.6635e-02, -9.2755e-02,\n",
      "          6.8715e-02,  2.3196e-02],\n",
      "        [-6.6026e-02, -5.3812e-02,  1.6556e-02,  2.7373e-02, -2.7180e-02,\n",
      "          1.0380e-01, -8.3431e-03],\n",
      "        [-1.2724e-01, -1.6597e-02, -3.1070e-02,  1.6722e-02, -2.2079e-02,\n",
      "          3.7598e-02, -3.6746e-02],\n",
      "        [-9.6497e-02, -1.0293e-01, -2.6455e-02,  2.1333e-02, -8.7341e-02,\n",
      "          1.4118e-01,  3.7285e-02],\n",
      "        [-3.0686e-02, -6.8522e-02, -1.0109e-02,  8.0016e-03, -9.8178e-02,\n",
      "          1.2400e-01,  1.5766e-02],\n",
      "        [-1.0914e-01, -7.7136e-02, -6.5417e-02, -3.0811e-02, -1.1278e-01,\n",
      "          1.1994e-01, -1.8943e-02],\n",
      "        [-3.8153e-02, -2.9980e-02,  1.7875e-02, -4.2023e-02, -4.1432e-02,\n",
      "          8.8679e-02,  9.7752e-03],\n",
      "        [-1.0166e-01, -6.4066e-02, -4.1239e-02,  1.4524e-02, -1.4902e-01,\n",
      "          7.9631e-02,  5.2409e-02],\n",
      "        [-1.1517e-01, -1.0639e-01, -8.2246e-02, -9.7947e-03, -1.2154e-01,\n",
      "          6.6832e-02,  3.8728e-02],\n",
      "        [-7.1731e-02, -3.8969e-02, -1.2640e-02,  6.3102e-03, -6.2110e-02,\n",
      "          1.1136e-01, -2.2513e-02],\n",
      "        [-7.0183e-02, -7.5334e-02, -5.6272e-02, -3.5466e-04, -6.7358e-02,\n",
      "          1.6136e-01,  9.6068e-03],\n",
      "        [-7.3673e-02, -8.0575e-02, -6.0438e-02,  2.0538e-02, -9.7106e-02,\n",
      "          9.9937e-02,  1.1974e-02],\n",
      "        [-6.4738e-02, -4.2913e-02, -1.4132e-02, -2.0722e-02, -7.8586e-02,\n",
      "          1.0540e-01, -5.0269e-02],\n",
      "        [-6.0474e-02, -5.9241e-02, -3.8952e-02, -2.3483e-02, -2.2067e-02,\n",
      "          9.8385e-02,  1.3014e-02],\n",
      "        [-8.8135e-02, -5.2425e-02, -5.6424e-02,  1.8865e-02, -1.4889e-01,\n",
      "          1.1310e-01,  3.3876e-02],\n",
      "        [-6.9398e-02, -7.0274e-02,  2.8460e-04, -5.6483e-03, -9.4516e-02,\n",
      "          1.0775e-01, -1.9551e-03],\n",
      "        [-5.5149e-02, -8.8640e-02, -3.1833e-03,  2.2210e-02, -1.2390e-01,\n",
      "          1.1712e-01,  8.8375e-02],\n",
      "        [-1.0556e-02, -5.4096e-02, -1.3672e-02, -4.0185e-02, -3.1850e-02,\n",
      "          7.3812e-02,  5.6447e-03],\n",
      "        [-9.7014e-02, -8.3006e-02,  9.5069e-03,  5.6339e-03, -1.3690e-01,\n",
      "          1.2479e-01,  3.3943e-02],\n",
      "        [-7.5920e-02, -1.2045e-01, -2.1016e-02, -5.0575e-03, -8.1967e-02,\n",
      "          1.2493e-01,  2.0489e-02],\n",
      "        [-1.2735e-01, -7.2447e-02, -2.8930e-02,  3.5480e-02, -1.2645e-01,\n",
      "          1.0405e-01,  4.9666e-02],\n",
      "        [-7.8606e-02, -8.6585e-02, -2.3015e-03, -1.4896e-02, -1.0707e-01,\n",
      "          1.0790e-01,  6.8617e-02],\n",
      "        [-1.1052e-01, -2.1052e-02, -1.3311e-02,  1.2504e-02, -1.2638e-01,\n",
      "          1.3732e-01,  2.9934e-02],\n",
      "        [-7.5630e-02, -7.2607e-02, -1.7100e-02,  2.1823e-02, -6.2437e-02,\n",
      "          1.0620e-01, -9.9242e-03],\n",
      "        [-6.5061e-02, -7.1685e-02, -2.4850e-02,  1.4882e-02, -6.6274e-02,\n",
      "          8.7267e-02, -1.5931e-02],\n",
      "        [-5.5751e-02, -5.1506e-02, -5.2327e-02, -2.5209e-03, -1.1566e-01,\n",
      "          7.8893e-02,  2.7082e-02],\n",
      "        [-7.3574e-02, -7.6891e-02,  1.9830e-02,  2.4507e-03, -7.8644e-02,\n",
      "          9.8459e-02,  1.5083e-02],\n",
      "        [-7.5587e-02, -8.2467e-02, -9.6275e-03,  1.3415e-02, -4.2594e-02,\n",
      "          1.1166e-01, -3.1906e-02],\n",
      "        [-1.0420e-01, -1.1079e-01, -2.2158e-02,  5.1122e-03, -3.7072e-02,\n",
      "          1.1121e-01,  6.9807e-02],\n",
      "        [-4.6701e-02, -5.3720e-02, -2.2159e-02,  1.4786e-02, -5.1234e-02,\n",
      "          1.1500e-01, -4.5177e-03],\n",
      "        [-5.2478e-02, -7.6710e-02, -2.0696e-02,  2.8144e-02, -8.8001e-02,\n",
      "          1.0548e-01,  9.9992e-05],\n",
      "        [-5.1190e-02, -2.5865e-02,  5.2905e-03,  2.6106e-02, -6.1974e-02,\n",
      "          6.5396e-02, -2.1210e-02]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/paper_v1.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_loss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     mean_train_loss \u001b[39m=\u001b[39m train()\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/paper_v1.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m pred \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39mto(device), img\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(pred)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m pred \u001b[39m=\u001b[39m train_dataset_raw\u001b[39m.\u001b[39;49mtarget_normalizer\u001b[39m.\u001b[39;49minverse_transform(pred)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/paper_v1.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_forecasting/data/encoders.py:581\u001b[0m, in \u001b[0;36mTorchNormalizer.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minverse_transform\u001b[39m(\u001b[39mself\u001b[39m, y: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    572\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39m    Inverse scale.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39m        torch.Tensor: de-scaled data\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(\u001b[39mdict\u001b[39;49m(prediction\u001b[39m=\u001b[39;49my, target_scale\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parameters()\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_forecasting/data/encoders.py:606\u001b[0m, in \u001b[0;36mTorchNormalizer.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    603\u001b[0m     norm \u001b[39m=\u001b[39m norm\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39m# transform\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m*\u001b[39;49m norm[:, \u001b[39m1\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m] \u001b[39m+\u001b[39m norm[:, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    608\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minverse_preprocess(y)\n\u001b[1;32m    610\u001b[0m \u001b[39m# return correct shape\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "temp = None\n",
    "loss_li = []\n",
    "\n",
    "def train():\n",
    "    global temp\n",
    "    total_loss = 0\n",
    "    for n, (data, valid) in enumerate(zip(train_dataloader, valid_dataloader)):\n",
    "        model.train(True)\n",
    "        clear_output(wait=True)\n",
    "        x, y, img = data[\"x\"], data[\"y\"].squeeze(), data[\"img\"]\n",
    "\n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x.to(device), img.to(device))\n",
    "        pred = train_dataset_raw.target_normalizer.inverse_transform(pred)\n",
    "        \n",
    "        loss = loss_fn(pred, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss / (n+1)\n",
    "\n",
    "        # # Plot loss\n",
    "        # loss_li.append(mean_loss)\n",
    "        # plt.plot(loss_li)\n",
    "\n",
    "        # Plot valid prediction\n",
    "        model.eval()\n",
    "        x, y, img = valid[\"x\"], valid[\"y\"].squeeze(), valid[\"img\"]\n",
    "        pred = model(x.to(device), img.to(device))\n",
    "        pred = train_dataset_raw.target_normalizer.inverse_transform(pred)\n",
    "        loss = torch.nn.MSELoss(reduction=\"none\")(pred, y.to(device))\n",
    "        loss = loss.mean(axis=1)\n",
    "        best_idx = loss.argmin()\n",
    "        plt.plot(pred[best_idx].cpu().detach().numpy(), label=\"pred\")\n",
    "        plt.plot(y[best_idx].cpu().detach().numpy(), label=\"y\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\r{mean_loss}\", end=\"\")\n",
    "\n",
    "    return mean_loss\n",
    "    \n",
    "for epoch in range(1):\n",
    "    mean_train_loss = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
