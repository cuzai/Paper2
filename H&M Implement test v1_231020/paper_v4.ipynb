{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "\n",
    "from transformers import SwinModel\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for sampling \n",
    "num_samples = None\n",
    "\n",
    "# Params for Train_test_split \n",
    "train_test_split_rto = 0.3\n",
    "\n",
    "# Dataset\n",
    "window_size = 365\n",
    "predict_length = 30\n",
    "batch_size = 128\n",
    "\n",
    "# Model\n",
    "d_model = 128\n",
    "dropout = 0.3\n",
    "nhead = 4\n",
    "num_layers = 4\n",
    "d_ff = 512\n",
    "# d_model = 512\n",
    "# dropout = 0.3\n",
    "# nhead = 8\n",
    "# num_layers = 6\n",
    "# d_ff = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0505221004</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687003</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687004</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n",
       "3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n",
       "4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n",
       "\n",
       "      price  sales_channel_id  \n",
       "0  0.050831                 2  \n",
       "1  0.030492                 2  \n",
       "2  0.015237                 2  \n",
       "3  0.016932                 2  \n",
       "4  0.016932                 2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"../HnM/transactions_train.csv\", dtype={\"article_id\":str}) # Read data\n",
    "# df_raw = df_raw.iloc[:1000]\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104547\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>prod_life</th>\n",
       "      <th>trans_date_cnt</th>\n",
       "      <th>nonzero_date_rto</th>\n",
       "      <th>img_path</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_dat  article_id     price  sales  time_idx  year  month  day  \\\n",
       "0 2018-09-20  0108775015  0.008445   30.0         0  2018      8   19   \n",
       "1 2018-09-21  0108775015  0.008194   48.0         1  2018      8   20   \n",
       "2 2018-09-22  0108775015  0.008458   11.0         2  2018      8   21   \n",
       "3 2018-09-23  0108775015  0.008425   26.0         3  2018      8   22   \n",
       "4 2018-09-24  0108775015  0.008406   33.0         4  2018      8   23   \n",
       "\n",
       "   dayofweek   min_date   max_date  prod_life  trans_date_cnt  \\\n",
       "0          3 2018-09-20 2020-07-22        672             672   \n",
       "1          4 2018-09-20 2020-07-22        672             672   \n",
       "2          5 2018-09-20 2020-07-22        672             672   \n",
       "3          6 2018-09-20 2020-07-22        672             672   \n",
       "4          0 2018-09-20 2020-07-22        672             672   \n",
       "\n",
       "   nonzero_date_rto                          img_path  is_valid  \n",
       "0               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "1               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "2               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "3               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "4               1.0  ../HnM/images/010/0108775015.jpg         1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_engineered = df_raw.copy()\n",
    "print(df_engineered[\"article_id\"].nunique())\n",
    "\n",
    "# Generate sales\n",
    "df_engineered[\"t_dat\"] = pd.to_datetime(df_engineered[\"t_dat\"])\n",
    "df_engineered = df_engineered.groupby([\"t_dat\", \"article_id\"], as_index=False).agg(price=(\"price\", \"mean\"), sales=(\"customer_id\", \"count\"))\n",
    "\n",
    "# Explode dates\n",
    "def func(x):\n",
    "    full_date = pd.DataFrame(pd.date_range(x[\"t_dat\"].min(), x[\"t_dat\"].max(), freq=\"d\"), columns=[\"t_dat\"])\n",
    "    x = x.merge(full_date, on=\"t_dat\", how=\"right\").reset_index(drop=True)\n",
    "    x[\"article_id\"] = x[\"article_id\"].unique()[0]\n",
    "    # x[\"sales\"] = x[\"sales\"].fillna(1e-5)\n",
    "    # x[\"price\"] = x[\"price\"].fillna(1e-5)\n",
    "    x[\"sales\"] = x[\"sales\"].fillna(0)\n",
    "    x[\"price\"] = x[\"price\"].fillna(0)\n",
    "    return x\n",
    "df_engineered = df_engineered.groupby(\"article_id\", as_index=False).apply(lambda x: func(x)).reset_index(drop=True)\n",
    "df_engineered[\"time_idx\"] = df_engineered.groupby(\"article_id\").cumcount()\n",
    "\n",
    "# Deal date\n",
    "df_engineered[\"year\"] = df_engineered[\"t_dat\"].dt.year\n",
    "df_engineered[\"month\"] = df_engineered[\"t_dat\"].dt.month - 1\n",
    "df_engineered[\"day\"] = df_engineered[\"t_dat\"].dt.day - 1\n",
    "df_engineered[\"dayofweek\"] = df_engineered[\"t_dat\"].dt.dayofweek\n",
    "\n",
    "# Deal product life\n",
    "df_engineered[\"min_date\"]= df_engineered.groupby(\"article_id\")[\"t_dat\"].transform(\"min\")\n",
    "df_engineered[\"max_date\"]= df_engineered.groupby(\"article_id\")[\"t_dat\"].transform(\"max\")\n",
    "df_engineered[\"prod_life\"] = (df_engineered[\"max_date\"] - df_engineered[\"min_date\"]).dt.days + 1\n",
    "\n",
    "# Generate transaction date count\n",
    "df_engineered[\"trans_date_cnt\"] = df_engineered.groupby([\"article_id\"])[\"t_dat\"].transform(\"count\")\n",
    "\n",
    "# Generate non-zero dates rto\n",
    "df_engineered[\"nonzero_date_rto\"] = df_engineered[\"trans_date_cnt\"] / df_engineered[\"prod_life\"]\n",
    "\n",
    "# Deal image\n",
    "df_engineered[\"img_path\"] = df_engineered[\"article_id\"].apply(lambda x: f'../HnM/images/{x[:3]}/{x}.jpg') # Generate image path\n",
    "df_engineered[\"is_valid\"] = df_engineered[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0) # Check whether the article has corresponding image file\n",
    "\n",
    "df_engineered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>prod_life</th>\n",
       "      <th>trans_date_cnt</th>\n",
       "      <th>nonzero_date_rto</th>\n",
       "      <th>img_path</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_dat  article_id     price  sales  time_idx  year  month  day  \\\n",
       "0 2018-09-20  0108775015  0.008445   30.0         0  2018      8   19   \n",
       "1 2018-09-21  0108775015  0.008194   48.0         1  2018      8   20   \n",
       "2 2018-09-22  0108775015  0.008458   11.0         2  2018      8   21   \n",
       "3 2018-09-23  0108775015  0.008425   26.0         3  2018      8   22   \n",
       "4 2018-09-24  0108775015  0.008406   33.0         4  2018      8   23   \n",
       "\n",
       "   dayofweek   min_date   max_date  prod_life  trans_date_cnt  \\\n",
       "0          3 2018-09-20 2020-07-22        672             672   \n",
       "1          4 2018-09-20 2020-07-22        672             672   \n",
       "2          5 2018-09-20 2020-07-22        672             672   \n",
       "3          6 2018-09-20 2020-07-22        672             672   \n",
       "4          0 2018-09-20 2020-07-22        672             672   \n",
       "\n",
       "   nonzero_date_rto                          img_path  is_valid  \n",
       "0               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "1               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "2               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "3               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "4               1.0  ../HnM/images/010/0108775015.jpg         1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df_engineered.copy()\n",
    "\n",
    "# Filtering\n",
    "df_filtered = df_filtered[df_filtered[\"is_valid\"] == 1] # Valid if having corresponding image\n",
    "df_filtered = df_filtered[df_filtered[\"prod_life\"] >= 1] \n",
    "# df_filtered = df_filtered[df_filtered[\"sales\"] >= predict_length + 1] # Length should be at least greater than prediction length\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>min_date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>prod_life</th>\n",
       "      <th>trans_date_cnt</th>\n",
       "      <th>nonzero_date_rto</th>\n",
       "      <th>img_path</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-21</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008194</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-22</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008458</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-24</td>\n",
       "      <td>0108775015</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>2020-07-22</td>\n",
       "      <td>672</td>\n",
       "      <td>672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../HnM/images/010/0108775015.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_dat  article_id     price  sales  time_idx  year  month  day  \\\n",
       "0 2018-09-20  0108775015  0.008445   30.0         0  2018      8   19   \n",
       "1 2018-09-21  0108775015  0.008194   48.0         1  2018      8   20   \n",
       "2 2018-09-22  0108775015  0.008458   11.0         2  2018      8   21   \n",
       "3 2018-09-23  0108775015  0.008425   26.0         3  2018      8   22   \n",
       "4 2018-09-24  0108775015  0.008406   33.0         4  2018      8   23   \n",
       "\n",
       "   dayofweek   min_date   max_date  prod_life  trans_date_cnt  \\\n",
       "0          3 2018-09-20 2020-07-22        672             672   \n",
       "1          4 2018-09-20 2020-07-22        672             672   \n",
       "2          5 2018-09-20 2020-07-22        672             672   \n",
       "3          6 2018-09-20 2020-07-22        672             672   \n",
       "4          0 2018-09-20 2020-07-22        672             672   \n",
       "\n",
       "   nonzero_date_rto                          img_path  is_valid  \n",
       "0               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "1               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "2               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "3               1.0  ../HnM/images/010/0108775015.jpg         1  \n",
       "4               1.0  ../HnM/images/010/0108775015.jpg         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled = df_filtered.copy()\n",
    "\n",
    "# Sample by sales amount\n",
    "sample_id_li = df_sampled.groupby(\"article_id\").agg({\"sales\":\"sum\"}).sort_values(\"sales\", ascending=False) # Sort article_id by number of sales\n",
    "sample_id_li = sample_id_li.iloc[:num_samples].index if num_samples else sample_id_li.index # Slice article_id\n",
    "df_sampled = df_sampled[df_sampled[\"article_id\"].isin(sample_id_li)].reset_index(drop=True)\n",
    "\n",
    "df_sampled = df_sampled.sort_values([\"article_id\", \"t_dat\"])\n",
    "\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = df_sampled.copy()\n",
    "\n",
    "### LabelEncode image path\n",
    "imgpath_encoder = LabelEncoder()\n",
    "df_post[\"img_path\"] = imgpath_encoder.fit_transform(df_post[\"img_path\"])\n",
    "\n",
    "# Train test split\n",
    "num_samples = num_samples if num_samples else df_post[\"article_id\"].nunique()\n",
    "num_train = int(np.round(num_samples * train_test_split_rto))\n",
    "sample_id_li_train = sample_id_li[:num_train]\n",
    "\n",
    "df_train = df_post[df_post[\"article_id\"].isin(sample_id_li_train)].reset_index(drop=True)\n",
    "df_valid = df_post[~df_post[\"article_id\"].isin(sample_id_li_train)].reset_index(drop=True)\n",
    "assert df_train.shape[0] + df_valid.shape[0] == df_post.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1281: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 17653 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__img_path': 2}, {'__group_id__img_path': 61}, {'__group_id__img_path': 62}, {'__group_id__img_path': 103}, {'__group_id__img_path': 125}, {'__group_id__img_path': 129}, {'__group_id__img_path': 176}, {'__group_id__img_path': 177}, {'__group_id__img_path': 180}, {'__group_id__img_path': 181}]\n",
      "  warnings.warn(\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1281: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 81338 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__img_path': 2}, {'__group_id__img_path': 7}, {'__group_id__img_path': 11}, {'__group_id__img_path': 12}, {'__group_id__img_path': 14}, {'__group_id__img_path': 16}, {'__group_id__img_path': 19}, {'__group_id__img_path': 20}, {'__group_id__img_path': 21}, {'__group_id__img_path': 22}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "train_dataset = pf.TimeSeriesDataSet(\n",
    "    data=df_train,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"sales\",\n",
    "    group_ids=[\"img_path\"],\n",
    "    # static_reals=[\"img_path\"], # image is a static information which does not change by time\n",
    "    min_encoder_length=window_size,\n",
    "    max_encoder_length=window_size,\n",
    "    min_prediction_idx=predict_length,\n",
    "    max_prediction_length=predict_length,\n",
    "    time_varying_unknown_reals=[\"sales\", \"month\", \"day\", \"dayofweek\"],\n",
    "    # target_normalizer=None,\n",
    "    scalers={\n",
    "        \"month\":None, \"day\":None, \"dayofweek\":None},\n",
    "    categorical_encoders={\n",
    "        \"img_path\":NaNLabelEncoder(add_nan=True)\n",
    "        }\n",
    ")\n",
    "valid_dataset = pf.TimeSeriesDataSet.from_dataset(train_dataset, df_post, predict=True, stop_randomization=True)\n",
    "\n",
    "train_dataloader = train_dataset.to_dataloader(batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = valid_dataset.to_dataloader(train=False, batch_size=2, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imgpath_encoder.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(train_dataloader, \"train_dataloader.pkl\")\n",
    "joblib.dump(valid_dataloader, \"valid_dataloader.pkl\")\n",
    "joblib.dump(train_dataset, \"train_dataset.pkl\")\n",
    "joblib.dump(imgpath_encoder, \"imgpath_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "train_dataset = joblib.load(\"../train_dataset.pkl\")\n",
    "train_dataloader = joblib.load(\"../train_dataloader.pkl\")\n",
    "valid_dataloader = joblib.load(\"../valid_dataloader.pkl\")\n",
    "imgpath_encoder = joblib.load(\"../imgpath_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # PE(pos, 2i) = sin(pos/10000^{2i/d_model}), \n",
    "    # PE(pos, 2i+1) = cos(pos/10000^{2i/d_model})\n",
    "    def __init__(self, max_len, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).reshape(-1,1).to(device)\n",
    "        i = torch.arange(d_model).to(device)//2\n",
    "        exp_term = 2*i/d_model\n",
    "        div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "        self.pos_encoded = position / div_term\n",
    "\n",
    "        self.pos_encoded[:, 0::2] = torch.sin(self.pos_encoded[:, 0::2])\n",
    "        self.pos_encoded[:, 1::2] = torch.cos(self.pos_encoded[:, 1::2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.pos_encoded[:x.shape[1], :]\n",
    "        return self.dropout(output)\n",
    "    \n",
    "class Mask(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_padding_mask(self, arr):\n",
    "        res = torch.eq(arr, 0).type(torch.FloatTensor).to(device)\n",
    "        res = torch.where(res==1, -torch.inf, 0)\n",
    "        return res\n",
    "    \n",
    "    def get_lookahead_mask(self, arr):\n",
    "        seq_len = arr.shape[1]\n",
    "        mask = torch.triu(torch.ones((seq_len, seq_len))*-1e-9, 1).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, arr):\n",
    "        padding_mask = self.get_padding_mask(arr)\n",
    "        lookahead_mask = self.get_lookahead_mask(arr)\n",
    "        return padding_mask, lookahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformerDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, batch_first):\n",
    "        super().__init__()\n",
    "        # self.swin_transformer = swin_transformer\n",
    "        self.attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=batch_first)\n",
    "        # self.linear2 = torch.nn.Linear(self.swin_transformer.config.hidden_size, d_model)\n",
    "        self.layernorm = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "    \n",
    "    def forward(self, dec_input, enc_output, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # ### Self attention\n",
    "        # swin_transformer_ = self.swin_transformer(dec_input).last_hidden_state\n",
    "        # print(swin_transformer_.shape)\n",
    "        # linear2_ = self.linear2(swin_transformer_)\n",
    "\n",
    "        ### Cross attention\n",
    "        attn_, attn_weight = self.attn(query=dec_input, key=enc_output, value=enc_output)\n",
    "        layernorm_ = self.layernorm(dec_input + attn_)\n",
    "\n",
    "        ### Feed forward\n",
    "        relu1_ = self.relu1(self.fc1(layernorm_))\n",
    "        relu2_ = self.relu2(self.fc2(relu1_))\n",
    "        ff = layernorm_ + relu2_\n",
    "\n",
    "        return ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformer(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, dropout, nhead, d_ff, num_layers, swin_transformer):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_mask = Mask()\n",
    "        self.linear_embedding = torch.nn.Linear(1, d_model)\n",
    "        self.month_embedding = torch.nn.Embedding(num_embeddings=12, embedding_dim=d_model)\n",
    "        self.day_embedding = torch.nn.Embedding(num_embeddings=31, embedding_dim=d_model)\n",
    "        self.dayofweek_embedding = torch.nn.Embedding(num_embeddings=7, embedding_dim=d_model)\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(d_model*4, d_model)\n",
    "        self.enc_pos_encoding = PositionalEncoding(max_seq_len, d_model, dropout)\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        self.swin_transformer = swin_transformer\n",
    "        self.linear2 = torch.nn.Linear(self.swin_transformer.config.hidden_size, d_model)\n",
    "        self.decoder = torch.nn.TransformerDecoder(torch.nn.TransformerDecoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "        # self.decoder = torch.nn.TransformerDecoder(MultimodalTransformerDecoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear3 = torch.nn.Linear(d_model*49, d_model)\n",
    "        self.linear4 = torch.nn.Linear(d_model, predict_length)\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        # Encoding\n",
    "        sales, month, day, dayofweek = enc_input\n",
    "        enc_padding_mask, _ = self.enc_mask(sales.squeeze())\n",
    "        linear_embedding_ = self.linear_embedding(sales)\n",
    "        month_embedding = self.month_embedding(month)\n",
    "        day_embedding = self.day_embedding(day)\n",
    "        dayofweek_embedding = self.dayofweek_embedding(dayofweek)\n",
    "\n",
    "        enc_input = torch.concat([linear_embedding_, month_embedding, day_embedding, dayofweek_embedding], axis=-1)\n",
    "        enc_input = self.relu1 = torch.nn.ReLU()(self.linear1(enc_input))\n",
    "\n",
    "        enc_pos_encoding_ = self.enc_pos_encoding(enc_input)\n",
    "        enc_output_ = self.encoder(enc_pos_encoding_, src_key_padding_mask=enc_padding_mask)\n",
    "        \n",
    "        # Decoding\n",
    "        swin_transformer_ = self.swin_transformer(dec_input).last_hidden_state\n",
    "        dec_output_ = self.relu1 = torch.nn.ReLU()(self.linear2(swin_transformer_))\n",
    "\n",
    "        dec_output = self.decoder(tgt=dec_output_, memory=enc_output_)\n",
    "\n",
    "        # Final\n",
    "        flatten_ = self.flatten(dec_output)\n",
    "        linear3_ = self.relu1 = torch.nn.ReLU()(self.linear3(flatten_))\n",
    "        linear4_ = self.relu1 = torch.nn.ReLU()(self.linear4(linear3_))\n",
    "        \n",
    "        return linear4_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformerFromDataset(BaseModelWithCovariates):\n",
    "    def __init__(self, imgpath_encoder, predict_length, swin_transformer, window_size, d_model, dropout, nhead, d_ff, num_layers, \n",
    "                 static_categoricals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, static_reals, \n",
    "                 time_varying_reals_encoder,  time_varying_reals_decoder, x_reals, x_categoricals, embedding_labels, embedding_paddings, \n",
    "                 categorical_groups, embedding_sizes, **kwargs):\n",
    "        self.save_hyperparameters()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.imgpath_encoder = imgpath_encoder\n",
    "        self.predict_length = predict_length\n",
    "        self.network = MultimodalTransformer(window_size, d_model, dropout, nhead, d_ff, num_layers, swin_transformer)\n",
    "        # self.network.to(device)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Gather time series data\n",
    "        sales = data[0][\"encoder_cont\"][:, :, 0].unsqueeze(-1) # shape: (batch_size, window_size, 1)\n",
    "        month = data[0][\"encoder_cont\"][:, :, 1].type(torch.int)\n",
    "        day = data[0][\"encoder_cont\"][:, :, 2].type(torch.int)\n",
    "        dayofweek = data[0][\"encoder_cont\"][:, :, 3].type(torch.int)\n",
    "        y = data[1][0] # shape: (batch_size, predict_length)\n",
    "\n",
    "        # Gather image data\n",
    "        img_path = data[0][\"groups\"].squeeze() # Label encoded image_path → shape: (batch_size, ) \n",
    "        img_path = self.imgpath_encoder.inverse_transform(img_path) # The real image path e.g) 'HnM/images/068/0687169002.jpg' → shape: (batch_size, )\n",
    "\n",
    "        # Process image data\n",
    "        img_li = []\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) # Transform image based on ImageNet standard\n",
    "\n",
    "        for n, path in enumerate(img_path): # Iterate images\n",
    "            img = transform(Image.open(path).convert(\"RGB\")) # Transform an image\n",
    "            img_li.append(img)\n",
    "        img_tensor = torch.stack(img_li, dim=0) # Put all the images together\n",
    "        \n",
    "        # Prediction\n",
    "        pred = self.network(\n",
    "            (sales.to(device), month.to(device), day.to(device), dayofweek.to(device)), \n",
    "            img_tensor.to(device)\n",
    "            )\n",
    "        pred = self.transform_output(prediction=pred, target_scale=data[0][\"target_scale\"].to(device)) # Inverse transform the output\n",
    "        \n",
    "        return pred, y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'swin_transformer' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['swin_transformer'])`.\n",
      "  rank_zero_warn(\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "swin_transformer = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\") # Get pre-trained SwinTransformer\n",
    "swin_transformer.to(device)\n",
    "\n",
    "model = MultimodalTransformerFromDataset.from_dataset(\n",
    "    train_dataset,\n",
    "    predict_length=predict_length,\n",
    "    swin_transformer=swin_transformer,\n",
    "    window_size=window_size,\n",
    "    d_model=d_model,\n",
    "    dropout=dropout,\n",
    "    nhead=nhead,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    imgpath_encoder= imgpath_encoder\n",
    "    )\n",
    "model.to(device); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(1e-5))\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "# loss_fn_ = torch.nn.MSELoss(reduction=\"none\")\n",
    "# train_loss_li, valid_loss_li = [], []\n",
    "\n",
    "\n",
    "# def plot_loss(train_loss_li, valid_loss_li):\n",
    "#     plt.plot(train_loss_li, label=\"train\")\n",
    "#     plt.plot(valid_loss_li, label=\"valid\")\n",
    "#     plt.title(\"loss\")\n",
    "#     plt.legend()\n",
    "\n",
    "# def plot_bestsample(loss, pred, y, iter, msg):\n",
    "#     loss = loss.mean(axis=1) # Shape: (batch_size, )\n",
    "#     _, best_idx_li = torch.sort(loss)\n",
    "#     for best_idx in best_idx_li:\n",
    "#         # best_pred = torch.round(pred[best_idx])\n",
    "#         best_pred = pred[best_idx]\n",
    "#         best_pred = best_pred.cpu().detach().numpy() # Sales is always int → Shape: (predict_length, )\n",
    "#         best_pred[best_pred < 0] = 0 # Sales never becomes negative\n",
    "#         best_y = y[best_idx].cpu().detach().numpy()\n",
    "#         if (np.max(best_y) < 10): # If predicted value is all 0, consider as not the best\n",
    "#             continue\n",
    "#         break\n",
    "    \n",
    "#     plt.plot(best_pred, label=\"pred\")\n",
    "#     plt.plot(best_y, label=\"y\", color=\"gray\", alpha=0.3)\n",
    "#     plt.title(f\"{iter}th iter: Best example amongst {msg} dataset\")\n",
    "#     plt.legend()\n",
    "\n",
    "# def train():\n",
    "#     total_train_loss, total_valid_loss = 0, 0\n",
    "#     for n, train_data in enumerate(train_dataloader):\n",
    "#         clear_output(wait=True)\n",
    "\n",
    "#         # Train\n",
    "#         model.train(True)\n",
    "#         optimizer.zero_grad()\n",
    "#         train_pred, train_y = model(train_data)\n",
    "\n",
    "#         # Get train loss\n",
    "#         train_loss = loss_fn(train_pred, train_y) # Shape: (batch_size, predict_length)\n",
    "#         train_loss.backward()\n",
    "#         train_loss_raw = loss_fn_(train_pred, train_y)\n",
    "#         total_train_loss += train_loss.item()\n",
    "#         train_loss_li.append(total_train_loss/(n+1))\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         valid_data = next(iter(valid_dataloader))\n",
    "#         valid_pred, valid_y = model(valid_data)\n",
    "\n",
    "#         # Get validation loss\n",
    "#         valid_loss = loss_fn(valid_pred, valid_y)\n",
    "#         valid_loss_raw = loss_fn_(valid_pred, valid_y)\n",
    "#         total_valid_loss += valid_loss.item()\n",
    "#         valid_loss_li.append(total_valid_loss/(n+1))\n",
    "\n",
    "#         # Plot\n",
    "#         plt.figure(figsize=(18,5))\n",
    "#         plt.subplot(1,3,1); plot_loss(train_loss_li, valid_loss_li)\n",
    "#         plt.subplot(1,3,2); plot_bestsample(train_loss_raw, train_pred, train_y, n, \"TRAIN\")\n",
    "#         plt.subplot(1,3,3); plot_bestsample(valid_loss_raw, valid_pred, valid_y, n, \"VALID\")\n",
    "#         plt.show()\n",
    "\n",
    "#         # Report\n",
    "#         print(f\"\\r {n}/{len(train_dataloader)} → train_loss: {np.mean(train_loss_li)}, valid_loss: {np.mean(valid_loss_li)}\", end=\"\")\n",
    "            \n",
    "# for epoch in range(10):\n",
    "#     mean_train_loss = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './HnM/images/049/0490460004.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/H&M Implement test v1_231020/paper_v4.ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     mean_train_loss \u001b[39m=\u001b[39m train(); train_loss_li\u001b[39m.\u001b[39mappend(mean_train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     mean_valid_loss \u001b[39m=\u001b[39m val(); valid_loss_li\u001b[39m.\u001b[39mappend(mean_valid_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m → train_loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(mean_train_loss)\u001b[39m}\u001b[39;00m\u001b[39m, valid_loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(mean_valid_loss)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/H&M Implement test v1_231020/paper_v4.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m train_pred, train_y \u001b[39m=\u001b[39m model(train_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Get train loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(train_pred, train_y) \u001b[39m# Shape: (batch_size, predict_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/H&M Implement test v1_231020/paper_v4.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m ]) \u001b[39m# Transform image based on ImageNet standard\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m n, path \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(img_path): \u001b[39m# Iterate images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     img \u001b[39m=\u001b[39m transform(Image\u001b[39m.\u001b[39;49mopen(path)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)) \u001b[39m# Transform an image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     img_li\u001b[39m.\u001b[39mappend(img)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/H%26M%20Implement%20test%20v1_231020/paper_v4.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m img_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(img_li, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m# Put all the images together\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './HnM/images/049/0490460004.jpg'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_fn_ = torch.nn.MSELoss(reduction=\"none\")\n",
    "train_loss_li, valid_loss_li = [], []\n",
    "temp = None\n",
    "\n",
    "def plot_loss(train_loss_li, valid_loss_li):\n",
    "    plt.plot(train_loss_li, label=\"train\")\n",
    "    plt.plot(valid_loss_li, label=\"valid\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "def plot_bestsample(loss, pred, y, iter, msg):\n",
    "    loss = loss.mean(axis=1) # Shape: (batch_size, )\n",
    "    _, best_idx_li = torch.sort(loss)\n",
    "    for best_idx in best_idx_li:\n",
    "        # best_pred = torch.round(pred[best_idx])\n",
    "        best_pred = pred[best_idx]\n",
    "        best_pred = best_pred.cpu().detach().numpy() # Sales is always int → Shape: (predict_length, )\n",
    "        best_pred[best_pred < 0] = 0 # Sales never becomes negative\n",
    "        best_y = y[best_idx].cpu().detach().numpy()\n",
    "        if (np.max(best_y) < 10): # If predicted value is all 0, consider as not the best\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    plt.plot(best_pred, label=\"pred\")\n",
    "    plt.plot(best_y, label=\"y\", color=\"gray\", alpha=0.3)\n",
    "    plt.title(f\"{iter}th iter: Best example amongst {msg} dataset\")\n",
    "    plt.legend()\n",
    "\n",
    "def train():\n",
    "    global temp\n",
    "    total_train_loss = 0\n",
    "    # for n, (train_data, valid_data) in enumerate(zip(train_dataloader, valid_dataloader)):\n",
    "    for n, train_data in enumerate(train_dataloader):\n",
    "        # Train\n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "        train_pred, train_y = model(train_data)\n",
    "\n",
    "        # Get train loss\n",
    "        train_loss = loss_fn(train_pred, train_y) # Shape: (batch_size, predict_length)\n",
    "        train_loss.backward()\n",
    "        total_train_loss += train_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        print(f\"\\r {n}/{len(train_dataloader)} → train_loss: {total_train_loss / (n+1)}\", end=\"\")\n",
    "    return total_train_loss / len(train_dataloader)\n",
    "\n",
    "def val():\n",
    "    total_valid_loss = 0\n",
    "    # for n, (train_data, valid_data) in enumerate(zip(train_dataloader, valid_dataloader)):\n",
    "    for n, valid_data in enumerate(valid_dataloader):\n",
    "        model.eval()\n",
    "        # Predict\n",
    "        valid_pred, valid_y = model(valid_data)\n",
    "\n",
    "        # Get train loss\n",
    "        valid_loss = loss_fn(valid_pred, valid_y) # Shape: (batch_size, predict_length)\n",
    "        total_valid_loss += valid_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        print(f\"\\r {n}/{len(train_dataloader)} → valid_loss: {total_valid_loss / (n+1)}\", end=\"\")\n",
    "    return total_valid_loss / len(train_dataloader)\n",
    "\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    mean_train_loss = train(); train_loss_li.append(mean_train_loss)\n",
    "    mean_valid_loss = val(); valid_loss_li.append(mean_valid_loss)\n",
    "    print(f\"\\r {e}/{epoch} → train_loss: {np.mean(mean_train_loss)}, valid_loss: {np.mean(mean_valid_loss)}\", end=\"\")\n",
    "    clear_output(wait=True)\n",
    "    plot_loss(train_loss_li, valid_loss_li)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
