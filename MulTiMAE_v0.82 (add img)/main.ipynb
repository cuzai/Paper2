{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from transformers import ViTModel, AutoImageProcessor, BertModel, AutoTokenizer\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 200\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"],\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"],\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 32\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "is_identical = False\n",
    "\n",
    "d_model = {\"encoder\":128, \"decoder\":64}\n",
    "d_ff = {\"encoder\":128, \"decoder\":64}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"target\": 0.25, \"temporal\":0.25, \"img\":0.25, \"nlp\":0.75}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 945.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 200, 1])\n",
      "sales_remain_idx torch.Size([2, 50])\n",
      "sales_masked_idx torch.Size([2, 150])\n",
      "sales_revert_idx torch.Size([2, 200])\n",
      "sales_remain_padding_mask torch.Size([2, 50])\n",
      "sales_masked_padding_mask torch.Size([2, 150])\n",
      "sales_revert_padding_mask torch.Size([2, 200])\n",
      "day torch.Size([2, 200])\n",
      "day_remain_idx torch.Size([2, 50])\n",
      "day_masked_idx torch.Size([2, 150])\n",
      "day_revert_idx torch.Size([2, 200])\n",
      "day_remain_padding_mask torch.Size([2, 50])\n",
      "day_masked_padding_mask torch.Size([2, 150])\n",
      "day_revert_padding_mask torch.Size([2, 200])\n",
      "dow torch.Size([2, 200])\n",
      "dow_remain_idx torch.Size([2, 50])\n",
      "dow_masked_idx torch.Size([2, 150])\n",
      "dow_revert_idx torch.Size([2, 200])\n",
      "dow_remain_padding_mask torch.Size([2, 50])\n",
      "dow_masked_padding_mask torch.Size([2, 150])\n",
      "dow_revert_padding_mask torch.Size([2, 200])\n",
      "month torch.Size([2, 200])\n",
      "month_remain_idx torch.Size([2, 50])\n",
      "month_masked_idx torch.Size([2, 150])\n",
      "month_revert_idx torch.Size([2, 200])\n",
      "month_remain_padding_mask torch.Size([2, 50])\n",
      "month_masked_padding_mask torch.Size([2, 150])\n",
      "month_revert_padding_mask torch.Size([2, 200])\n",
      "holiday torch.Size([2, 200])\n",
      "holiday_remain_idx torch.Size([2, 50])\n",
      "holiday_masked_idx torch.Size([2, 150])\n",
      "holiday_revert_idx torch.Size([2, 200])\n",
      "holiday_remain_padding_mask torch.Size([2, 50])\n",
      "holiday_masked_padding_mask torch.Size([2, 150])\n",
      "holiday_revert_padding_mask torch.Size([2, 200])\n",
      "price torch.Size([2, 200, 1])\n",
      "price_remain_idx torch.Size([2, 50])\n",
      "price_masked_idx torch.Size([2, 150])\n",
      "price_revert_idx torch.Size([2, 200])\n",
      "price_remain_padding_mask torch.Size([2, 50])\n",
      "price_masked_padding_mask torch.Size([2, 150])\n",
      "price_revert_padding_mask torch.Size([2, 200])\n",
      "img_path torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info, tokenizer), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info, tokenizer))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import copy\n",
    "\n",
    "def get_positional_encoding(d_hidn, n_seq=1000):\n",
    "    def cal_angle(position, i_hidn):\n",
    "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "    res = torch.tensor(sinusoid_table).to(torch.float32)\n",
    "    return res\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        return self.dropout(x)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, activation):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiheadBlockAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model, self.nhead = d_model, nhead\n",
    "        \n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn = torch.nn.functional.softmax(QK/math.sqrt(self.d_model//self.nhead), dim=-1)\n",
    "        \n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn @ V\n",
    "        \n",
    "        # Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, -1, seq_len, d_model)\n",
    "        attn_output = attn_output.squeeze()\n",
    "\n",
    "        return attn_output, attn\n",
    "\n",
    "\n",
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "\n",
    "    def forward(self, key, val):\n",
    "        assert key == self.col\n",
    "\n",
    "        return self.embedding(val)\n",
    "    \n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        assert key == self.col\n",
    "\n",
    "        embedding = self.img_model(val).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        # return embedding[:, 1:, :]\n",
    "        return embedding\n",
    "\n",
    "class TemporalRemain(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.pos_enc = pos_enc\n",
    "    \n",
    "    def forward(self, key, val, idx_dict):\n",
    "        assert self.col == key\n",
    "\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Get remain data\n",
    "        remain_idx = idx_dict[f\"{key}_remain_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "        val = torch.gather(val, index=remain_idx, dim=1)\n",
    "\n",
    "        return val\n",
    "\n",
    "class ImgRemain(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.pos_enc = pos_enc\n",
    "    \n",
    "    def __forward(self, key, val, remain_rto, device):\n",
    "        assert self.col == key\n",
    "\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Get remain, masked, revert idx\n",
    "        num_remain = int(val.shape[1] * remain_rto)\n",
    "        noise = torch.rand(val.shape[:-1]).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=1)\n",
    "\n",
    "        remain_idx = shuffle_idx[:, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "        remain_padding_mask = torch.ones(remain_idx.shape).to(device)\n",
    "\n",
    "        # Apply mask\n",
    "        val = torch.gather(val, index=remain_idx.unsqueeze(-1).repeat(1, 1, val.shape[-1]), dim=1)\n",
    "        \n",
    "        return val, remain_idx, masked_idx, revert_idx, remain_padding_mask\n",
    "\n",
    "    def forward(self, key, val, remain_rto, device):\n",
    "        assert self.col == key\n",
    "\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Get remain, masked, revert idx\n",
    "        num_remain = int((val.shape[1]-1) * remain_rto)\n",
    "        noise = torch.rand(val.shape[0], val.shape[1]-1).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=1)\n",
    "\n",
    "        remain_idx = shuffle_idx[:, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "        # Apply mask\n",
    "        global_token = val[:, :1, :]\n",
    "        val = torch.gather(val[:, 1:, :], index=remain_idx.unsqueeze(-1).repeat(1, 1, val.shape[-1]), dim=1)\n",
    "        val = torch.cat([global_token, val], dim=1)\n",
    "\n",
    "        remain_padding_mask = torch.ones(val.shape[:-1]).to(device)\n",
    "        \n",
    "        return val, remain_idx, masked_idx, revert_idx, remain_padding_mask\n",
    "\n",
    "\n",
    "class ModalityEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, modality, modality_embedding):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.modality = modality[self.col]\n",
    "        self.modality_embedding = modality_embedding\n",
    "    \n",
    "    def forward(self, key, val, device):\n",
    "        assert self.col == key\n",
    "        modality = torch.zeros(val.shape[1]).to(device) + self.modality\n",
    "        modality = self.modality_embedding(modality.to(torch.int))\n",
    "\n",
    "        val += modality\n",
    "        return val\n",
    "\n",
    "\n",
    "class Encoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=d_ff, dropout=dropout, batch_first=True, activation=activation, norm_first=True), num_layers)\n",
    "\n",
    "    def forward(self, data_dict, padding_mask_dict, target_cols):\n",
    "        concat_li, padding_mask_li = [], []\n",
    "        for col in target_cols:\n",
    "            concat_li.append(data_dict[col])\n",
    "            padding_mask_li.append(padding_mask_dict[f\"{col}_remain_padding_mask\"])\n",
    "        \n",
    "        concat = torch.cat(concat_li, dim=1)\n",
    "        padding_mask = torch.cat(padding_mask_li, dim=1)\n",
    "        padding_mask = torch.where(padding_mask==1, 0, -torch.inf)\n",
    "        assert concat.shape[1] == padding_mask.shape[1]\n",
    "\n",
    "        encoding = self.encoder(concat, src_key_padding_mask=padding_mask)\n",
    "\n",
    "        return encoding\n",
    "\n",
    "\n",
    "class Split(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, target_cols, target_data, reference_dict):\n",
    "        result_dict = {}\n",
    "        start_idx = 0\n",
    "\n",
    "        for col in target_cols:\n",
    "            length = reference_dict[col].shape[1]\n",
    "            result_dict[col] = target_data[:, start_idx:start_idx+length, :]\n",
    "            start_idx += length\n",
    "        \n",
    "        assert start_idx == target_data.shape[1]\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "\n",
    "class Revert(torch.nn.Module):\n",
    "    def __init__(self, col, mask_token, pos_enc):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.mask_token = mask_token\n",
    "        self.pos_enc = pos_enc\n",
    "\n",
    "    def forward(self, key, val, idx_dict, padding_mask_dict):\n",
    "        assert self.col == key\n",
    "\n",
    "        # Replace remain padding to mask token\n",
    "        remain_padding_mask = padding_mask_dict[f\"{key}_remain_padding_mask\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "        val = torch.where(remain_padding_mask==1, val, self.mask_token)\n",
    "\n",
    "        # Append mask token\n",
    "        revert_idx = idx_dict[f\"{key}_revert_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "        if key == \"img_path\":\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(val.shape[0], revert_idx.shape[1]-val.shape[1]+1, 1)\n",
    "        else:\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(val.shape[0], revert_idx.shape[1]-val.shape[1], 1)\n",
    "        val = torch.cat([val, mask_token], dim=1)\n",
    "\n",
    "        # assert revert_idx.shape == val.shape\n",
    "\n",
    "        # Apply revert\n",
    "        if key == \"img_path\":\n",
    "            global_token = val[:, :1, :]\n",
    "            val = torch.gather(val[:, 1:, :], index=revert_idx, dim=1)\n",
    "            val = torch.cat([global_token, val], dim=1)\n",
    "        else:\n",
    "            val = torch.gather(val, index=revert_idx, dim=1)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        return val\n",
    "\n",
    "\n",
    "class TemporalDecoder(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.layers = torch.nn.ModuleList([copy.deepcopy(TemporalDecoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols):\n",
    "        assert self.col == key\n",
    "        tgt = val\n",
    "        \n",
    "        # Temporal memory\n",
    "        memory_li = []\n",
    "        for col in temporal_cols:\n",
    "            if col == key: continue\n",
    "            else:\n",
    "                memory_li.append(data_dict[col])\n",
    "        memory = torch.stack(memory_li, dim=-2)\n",
    "\n",
    "        # Img memory\n",
    "        for col in img_cols:\n",
    "            img_data = data_dict[col].unsqueeze(1).repeat(1, memory.shape[1], 1, 1)\n",
    "            memory = torch.cat([memory, img_data], dim=-2)\n",
    "\n",
    "        # Apply decoder\n",
    "        padding_mask = padding_mask_dict[f\"{key}_revert_padding_mask\"]\n",
    "        padding_mask = torch.where(padding_mask==1, 0, -torch.inf)\n",
    "        for mod in self.layers:\n",
    "            tgt, self_attn_weight, cross_attn_weight = mod(tgt, memory, padding_mask)\n",
    "        \n",
    "        return tgt, self_attn_weight, cross_attn_weight\n",
    "\n",
    "class ImgDecoder(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.layers = torch.nn.ModuleList([copy.deepcopy(ImgDecoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, key, val, data_dict, padding_mask_dict, temporal_cols):\n",
    "        assert self.col == key\n",
    "        tgt = val\n",
    "        \n",
    "        # Memory and padding_mask\n",
    "        memory_li, padding_mask_li = [], []\n",
    "        for col in temporal_cols:\n",
    "            memory_li.append(data_dict[col])\n",
    "            padding_mask_li.append(padding_mask_dict[f\"{col}_revert_padding_mask\"])\n",
    "        memory = torch.cat(memory_li, dim=1)\n",
    "        memory_padding_mask = torch.cat(padding_mask_li, dim=1)\n",
    "        \n",
    "        # Apply decoder\n",
    "        memory_padding_mask = torch.where(memory_padding_mask==1, 0, -torch.inf)\n",
    "\n",
    "        for mod in self.layers:\n",
    "            tgt, self_attn_weight, cross_attn_weight = mod(tgt, memory, memory_padding_mask)\n",
    "\n",
    "        return tgt, self_attn_weight, cross_attn_weight\n",
    "\n",
    "class TemporalDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.cross_attn = MultiheadBlockAttention(d_model, nhead, dropout)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm3 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm4 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_padding_mask):\n",
    "        # cross attention\n",
    "        x = tgt\n",
    "        \n",
    "        # cross_attn_output, cross_attn_weight = self._ca_block(self.norm1(x.unsqueeze(-2)), self.norm2(memory), self.norm2(memory))\n",
    "        cross_attn_output, cross_attn_weight = self._ca_block(self.norm1(x.unsqueeze(-2)), memory, memory)\n",
    "        x = x + cross_attn_output\n",
    "        \n",
    "        self_attn_output, self_attn_weight = self._sa_block(self.norm3(x), tgt_padding_mask)\n",
    "        x = x + self_attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm4(x))\n",
    "\n",
    "        return x, self_attn_weight, cross_attn_weight\n",
    "    \n",
    "    def _ca_block(self, query, key, value):\n",
    "        x, attn_weight = self.cross_attn(query, key, value)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "    def _sa_block(self, src, padding_mask):\n",
    "        x, attn_weight = self.self_attn(src,src,src,\n",
    "                           key_padding_mask=padding_mask)\n",
    "        return self.dropout2(x), attn_weight\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "class ImgDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.cross_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm3 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm4 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, memory_padding_mask):\n",
    "        # cross attention\n",
    "        x = tgt\n",
    "        \n",
    "        cross_attn_output, cross_attn_weight = self._ca_block(self.norm1(x), memory, memory, memory_padding_mask)\n",
    "        x = x + cross_attn_output\n",
    "    \n",
    "        self_attn_output, self_attn_weight = self._sa_block(self.norm3(x))\n",
    "        x = x + self_attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm4(x))\n",
    "\n",
    "        return x, self_attn_weight, cross_attn_weight\n",
    "    \n",
    "    def _ca_block(self, query, key, value, padding_mask):\n",
    "        x, attn_weight = self.cross_attn(query, key, value, key_padding_mask=padding_mask)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "    def _sa_block(self, src):\n",
    "        x, attn_weight = self.self_attn(src,src,src)\n",
    "        return self.dropout2(x), attn_weight\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "\n",
    "class TemporalOutput(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, 1))\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, num_cls))\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        assert self.col == key\n",
    "\n",
    "        return self.output(val)\n",
    "\n",
    "class ImgOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.col = col\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_model),\n",
    "            torch.nn.Linear(d_model, 3*patch_size*patch_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        return self.output(val)\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                d_model, num_layers, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.data_info, self.label_encoder_dict = data_info, label_encoder_dict\n",
    "        self.temporal_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        self.img_cols = self.data_info.modality_info[\"img\"]\n",
    "        self.total_cols = self.temporal_cols + self.img_cols\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.temporal_embedding_dict = self.init_process(mod=TemporalEmbedding, target_cols=self.temporal_cols, args=[self.data_info, self.label_encoder_dict, d_model[\"encoder\"]])\n",
    "        self.img_embedding_dict = self.init_process(mod=ImgEmbedding, target_cols=self.img_cols, args=[d_model[\"encoder\"]])\n",
    "\n",
    "        # 2. Remain mask\n",
    "        encoder_pos_enc = PositionalEncoding(d_model[\"encoder\"], dropout)\n",
    "        self.temporal_remain_dict = self.init_process(mod=TemporalRemain, target_cols=self.temporal_cols, args=[encoder_pos_enc])\n",
    "        self.img_remain_dict = self.init_process(mod=ImgRemain, target_cols=self.img_cols, args=[encoder_pos_enc])\n",
    "\n",
    "        # 3. Modality embedding\n",
    "        num_modality = len(self.total_cols)\n",
    "        modality = {col:n for n, col in enumerate(self.total_cols)}\n",
    "        encoder_modality_embedding = torch.nn.Embedding(num_modality, d_model[\"encoder\"])\n",
    "        self.encoder_modality_embedding_dict = self.init_process(mod=ModalityEmbedding, target_cols=self.total_cols, args=[modality, encoder_modality_embedding])\n",
    "\n",
    "        # 4. Encoding\n",
    "        self.encoding = Encoding(d_model[\"encoder\"], nhead, d_ff[\"encoder\"], dropout, activation, num_layers[\"encoder\"])\n",
    "        self.to_decoder_dims = torch.nn.Linear(d_model[\"encoder\"], d_model[\"decoder\"])\n",
    "\n",
    "        # 5. Split\n",
    "        self.split = Split()\n",
    "\n",
    "        # 6. Revert\n",
    "        mask_token = torch.nn.Parameter(torch.rand(1, d_model[\"decoder\"]))\n",
    "        decoder_pos_enc = PositionalEncoding(d_model[\"decoder\"], dropout)\n",
    "        self.revert_dict = self.init_process(mod=Revert, target_cols=self.total_cols, args=[mask_token, decoder_pos_enc])\n",
    "\n",
    "        # 7. Modality embedding\n",
    "        decoder_modality_embedding = torch.nn.Embedding(num_modality, d_model[\"decoder\"])\n",
    "        self.decoder_modality_embedding_dict = self.init_process(mod=ModalityEmbedding, target_cols=self.total_cols, args=[modality, decoder_modality_embedding])\n",
    "\n",
    "        # 8. Decoding\n",
    "        self.temporal_decoding = self.init_process(mod=TemporalDecoder, target_cols=self.temporal_cols, args=[d_model[\"decoder\"], nhead, d_ff[\"decoder\"], dropout, activation, num_layers[\"decoder\"]])\n",
    "        self.img_decoding = self.init_process(mod=ImgDecoder, target_cols=self.img_cols, args=[d_model[\"decoder\"], nhead, d_ff[\"decoder\"], dropout, activation, num_layers[\"decoder\"]])\n",
    "\n",
    "        # 9. Output\n",
    "        self.temporal_output = self.init_process(mod=TemporalOutput, target_cols=self.temporal_cols, args=[self.data_info, self.label_encoder_dict, d_model[\"decoder\"]])\n",
    "        self.img_output = self.init_process(mod=ImgOutput, target_cols=self.img_cols, args=[d_model[\"decoder\"]])\n",
    "    \n",
    "    def forward(self, data_input, remain_rto, device):\n",
    "        data_dict, idx_dict, padding_mask_dict = self.to_gpu(data_input)\n",
    "\n",
    "        # 1. Embedding\n",
    "        temporal_embedding_dict = self.apply_process(data=data_dict, mod=self.temporal_embedding_dict, target_cols=self.temporal_cols)\n",
    "        img_embedding_dict = self.apply_process(data=data_dict, mod=self.img_embedding_dict, target_cols=self.img_cols)\n",
    "\n",
    "        # 2. Remain mask\n",
    "        temporal_remain_dict = self.apply_process(data=temporal_embedding_dict, mod=self.temporal_remain_dict, target_cols=self.temporal_cols, args=[idx_dict])\n",
    "        img_remain_dict = self.apply_process(data=img_embedding_dict, mod=self.img_remain_dict, target_cols=self.img_cols, args=[remain_rto[\"img\"], device])\n",
    "        remain_dict, idx_dict, padding_mask_dict = self.tidy_remain(temporal_remain_dict, img_remain_dict, idx_dict, padding_mask_dict)\n",
    "        \n",
    "        # 3. Modality embedding\n",
    "        encoder_modality_embedding_dict = self.apply_process(data=temporal_remain_dict, mod=self.encoder_modality_embedding_dict, target_cols=self.total_cols, args=[device])\n",
    "\n",
    "        # 4. Encoding\n",
    "        encoding = self.encoding(encoder_modality_embedding_dict, padding_mask_dict, self.total_cols)\n",
    "        encoding = self.to_decoder_dims(encoding)\n",
    "\n",
    "        # 5. Split\n",
    "        encoding = self.split(self.total_cols, encoding, encoder_modality_embedding_dict)\n",
    "\n",
    "        # 6. Revert\n",
    "        revert_dict = self.apply_process(data=encoding, mod=self.revert_dict, target_cols=self.total_cols, args=[idx_dict, padding_mask_dict])\n",
    "\n",
    "        # 7. Modality embedding\n",
    "        decoder_modality_embedding_dict = self.apply_process(data=revert_dict, mod=self.decoder_modality_embedding_dict, target_cols=self.total_cols, args=[device])\n",
    "\n",
    "        # 8. Decoding\n",
    "        self_attn_weight_dict, cross_attn_weight_dict = {}, {}\n",
    "        temporal_decoding_dict = self.apply_process(data=decoder_modality_embedding_dict, mod=self.temporal_decoding, target_cols=self.temporal_cols, args=[decoder_modality_embedding_dict, padding_mask_dict, self.temporal_cols, self.img_cols])\n",
    "        temporal_decoding_dict, self_attn_weight_dict, cross_attn_weight_dict = self.tidy_decoding(temporal_decoding_dict, self_attn_weight_dict, cross_attn_weight_dict)\n",
    "        img_decoding_dict = self.apply_process(data=decoder_modality_embedding_dict, mod=self.img_decoding, target_cols=self.img_cols, args=[decoder_modality_embedding_dict, padding_mask_dict, self.temporal_cols])\n",
    "        img_decoding_dict, self_attn_weight_dict, cross_attn_weight_dict = self.tidy_decoding(img_decoding_dict, self_attn_weight_dict, cross_attn_weight_dict)\n",
    "\n",
    "        # 9. Output\n",
    "        temporal_output = self.apply_process(data=temporal_decoding_dict, mod=self.temporal_output, target_cols=self.temporal_cols)\n",
    "        img_output = self.apply_process(data=img_decoding_dict, mod=self.img_output, target_cols=self.img_cols)\n",
    "\n",
    "        return temporal_output, img_output, self_attn_weight_dict, cross_attn_weight_dict, idx_dict, padding_mask_dict\n",
    "    \n",
    "    \n",
    "    def to_gpu(self, data_input):\n",
    "        data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        for key, val in data_input.items():\n",
    "            if key in self.total_cols:\n",
    "                data_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"idx\"):\n",
    "                idx_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"padding_mask\"):\n",
    "                padding_mask_dict[key] = data_input[key].to(device)\n",
    "            \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def init_process(self, mod, target_cols, args=[]):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod(col, *args)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def apply_process(self, data, mod, target_cols, args=[]):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod[col](col, data[col], *args)\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "\n",
    "    def tidy_remain(self, temporal_remain_dict, img_remain_dict, idx_dict, padding_mask_dict):\n",
    "        remain_dict = temporal_remain_dict\n",
    "        for key, val in img_remain_dict.items():\n",
    "            img_remain, remain_idx, masked_idx, revert_idx, remain_padding_mask = val\n",
    "            remain_dict.update({key: img_remain})\n",
    "            idx_dict.update({\n",
    "                f\"{key}_remain_idx\": remain_idx,\n",
    "                f\"{key}_masked_idx\": masked_idx,\n",
    "                f\"{key}_revert_idx\": revert_idx,\n",
    "                })\n",
    "            padding_mask_dict.update({\n",
    "                f\"{key}_remain_padding_mask\": remain_padding_mask\n",
    "            })\n",
    "        return remain_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def tidy_decoding(self, decoding_dict_input, self_attn_weight_dict, cross_attn_weight_dict):\n",
    "        decoding_dict = {}\n",
    "        for key, val in decoding_dict_input.items():\n",
    "            decoding_dict[key], self_attn_weight, cross_attn_weight = val\n",
    "            self_attn_weight_dict.update({key:self_attn_weight})\n",
    "            cross_attn_weight_dict.update({key:cross_attn_weight})\n",
    "\n",
    "        return decoding_dict, self_attn_weight_dict, cross_attn_weight_dict\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "   Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================================\n",
      "     Transformer         Encoding-1       [2, 350, 128]         199,168         199,168\n",
      "     Transformer           Linear-2        [2, 350, 64]           8,256           8,256\n",
      "     Transformer            Split-3                                   0               0\n",
      "=======================================================================================\n",
      "Total params: 207,424\n",
      "Trainable params: 207,424\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'---------------------------------------------------------------------------------------\\n   Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\\n=======================================================================================\\n     Transformer         Encoding-1       [2, 350, 128]         199,168         199,168\\n     Transformer           Linear-2        [2, 350, 64]           8,256           8,256\\n     Transformer            Split-3                                   0               0\\n=======================================================================================\\nTotal params: 207,424\\nTrainable params: 207,424\\nNon-trainable params: 0\\n---------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout, \"gelu\")\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"img_embedding_dict.img_path.img_model\" in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def patchify(imgs, patch_size=16):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size=16):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def get_temporal_loss(temporal_output, data, idx_dict, padding_mask_dict, data_info, inverse_scale=False):\n",
    "    loss_dict, masked_pred_dict, masked_y_dict = {}, {}, {}\n",
    "    for key, val in temporal_output.items():\n",
    "        pred = val.squeeze()\n",
    "        y = data[key].to(device).squeeze()\n",
    "\n",
    "        if inverse_scale and key in data_info.processing_info[\"scaling_cols\"]:\n",
    "            pred = pred.detach().cpu()\n",
    "            y = y.detach().cpu()\n",
    "\n",
    "            new_pred, new_y = [], []\n",
    "            scaler = data[f\"{key}_scaler\"]\n",
    "\n",
    "            for i, s in enumerate(scaler):\n",
    "                new_pred.append(torch.from_numpy(s.inverse_transform(pred[i].reshape(-1,1).numpy())))\n",
    "                new_y.append(torch.from_numpy(s.inverse_transform(y[i].reshape(-1,1).numpy())))\n",
    "            \n",
    "            new_pred = torch.stack(new_pred, dim=0).squeeze().to(device)\n",
    "            new_y = torch.stack(new_y, dim=0).squeeze().to(device)\n",
    "            \n",
    "            assert new_pred.shape == pred.shape and new_y.shape == y.shape\n",
    "\n",
    "            pred, y = new_pred, new_y\n",
    "\n",
    "        masked_idx = idx_dict[f\"{key}_masked_idx\"].squeeze()\n",
    "        padding_mask = padding_mask_dict[f\"{key}_masked_padding_mask\"].squeeze()\n",
    "\n",
    "        masked_y = torch.gather(y, index=masked_idx, dim=1)\n",
    "\n",
    "        if key in data_info.processing_info[\"scaling_cols\"]:\n",
    "            masked_pred = torch.gather(pred, index=masked_idx, dim=1)\n",
    "            loss = mse_loss(masked_pred, masked_y)\n",
    "        elif key in data_info.processing_info[\"embedding_cols\"]:\n",
    "            masked_idx = masked_idx.unsqueeze(-1).repeat(1, 1, pred.shape[-1])\n",
    "            masked_pred = torch.gather(pred, index=masked_idx, dim=1)\n",
    "\n",
    "            masked_pred = masked_pred.reshape(-1, masked_pred.shape[-1])\n",
    "            masked_y = masked_y.reshape(-1).to(torch.long)\n",
    "\n",
    "            loss = ce_loss(masked_pred, masked_y).reshape(padding_mask.shape)\n",
    "\n",
    "        loss = loss * padding_mask\n",
    "        loss = sum(loss.view(-1)) / sum(padding_mask.view(-1))\n",
    "        loss_dict[key] = loss\n",
    "\n",
    "        masked_pred_dict[key] = masked_pred.reshape(padding_mask.shape[0], padding_mask.shape[1], -1)\n",
    "        masked_y_dict[key] = masked_y.reshape(padding_mask.shape[0], padding_mask.shape[1], -1)\n",
    "\n",
    "    return loss_dict, masked_pred_dict, masked_y_dict\n",
    "\n",
    "def get_img_loss(img_output, data, idx_dict, padding_mask_dict, data_info):\n",
    "    loss_dict, masked_pred_dict, masked_y_dict = {}, {}, {}\n",
    "    for key, val in img_output.items():\n",
    "        pred = val[:, 1:, :]\n",
    "        y = patchify(data[key]).to(device)\n",
    "\n",
    "        masked_idx = idx_dict[f\"{key}_masked_idx\"].squeeze()\n",
    "        masked_idx = masked_idx.unsqueeze(-1).repeat(1, 1, pred.shape[-1])\n",
    "\n",
    "        masked_y = torch.gather(y, index=masked_idx, dim=1)\n",
    "        masked_pred = torch.gather(pred, index=masked_idx, dim=1)\n",
    "\n",
    "        loss = mse_loss(masked_pred, masked_y).mean()\n",
    "        loss_dict[key] = loss\n",
    "\n",
    "        masked_y_dict[key] = masked_y\n",
    "        masked_pred_dict[key] = masked_pred\n",
    "        \n",
    "    return loss_dict, masked_pred_dict, masked_y_dict\n",
    "\n",
    "def train(e):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_li_dict, mean_loss_li_dict = defaultdict(list), defaultdict(list)\n",
    "    eval_loss_li_dict, eval_mean_loss_li_dict = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        temporal_output, img_output, self_attn_dict, cross_attn_dict, idx_dict, padding_mask_dict = model(data, remain_rto, device)\n",
    "        \n",
    "        temporal_loss_dict, temporal_masked_pred_dict, temporal_masked_y_dict = get_temporal_loss(temporal_output, data, idx_dict, padding_mask_dict, data_info)\n",
    "        img_loss_dict, img_masked_pred_dict, img_masked_y_dict = get_img_loss(img_output, data, idx_dict, padding_mask_dict, data_info)\n",
    "\n",
    "        temporal_total_loss = torch.nansum(torch.stack(list(temporal_loss_dict.values())))\n",
    "        img_total_loss = torch.nansum(torch.stack(list(img_loss_dict.values())))\n",
    "        \n",
    "        temporal_loss_dict[\"total\"] = temporal_total_loss\n",
    "        img_loss_dict[\"total\"] = img_total_loss\n",
    "\n",
    "        loss = temporal_total_loss + img_total_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            eval_temporal_loss_dict, eval_temporal_masked_pred_dict, eval_temporal_masked_y_dict = get_temporal_loss(temporal_output, data, idx_dict, padding_mask_dict, data_info, inverse_scale=True)\n",
    "            eval_img_loss_dict, eval_img_masked_pred_dict, eval_img_masked_y_dict = get_img_loss(img_output, data, idx_dict, padding_mask_dict, data_info)\n",
    "        eval_temporal_loss = torch.nansum(torch.stack(list(eval_temporal_loss_dict.values())))\n",
    "        eval_img_loss = torch.nansum(torch.stack(list(eval_img_loss_dict.values())))\n",
    "        \n",
    "        eval_temporal_loss_dict[\"total\"] = eval_temporal_loss\n",
    "        eval_img_loss_dict[\"total\"] = eval_img_loss\n",
    "\n",
    "        # Plot\n",
    "        if n % 20 == 0:\n",
    "            plt.figure(figsize=(25,13))\n",
    "            clear_output(wait=True)\n",
    "            nrows, ncols = 8, 4\n",
    "\n",
    "            # Plot total loss\n",
    "            plt.subplot(nrows, ncols, 1)\n",
    "            total_loss = temporal_loss_dict[\"total\"].item()\n",
    "            loss_li_dict[\"total\"].append(total_loss)\n",
    "            mean_loss_li_dict[\"total\"].append(np.array(loss_li_dict[\"total\"]).mean())\n",
    "            plt.plot(mean_loss_li_dict[\"total\"])\n",
    "            plt.title(f'Total loss: {mean_loss_li_dict[\"total\"][-1]}')\n",
    "\n",
    "            # Plot eval total loss\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            eval_total_loss = eval_temporal_loss_dict[\"total\"].item()\n",
    "            eval_loss_li_dict[\"total\"].append(eval_total_loss)\n",
    "            eval_mean_loss_li_dict[\"total\"].append(np.array(eval_loss_li_dict[\"total\"]).mean())\n",
    "            plt.plot(eval_mean_loss_li_dict[\"total\"])\n",
    "            plt.title(f'Total eval loss: {eval_mean_loss_li_dict[\"total\"][-1]}')\n",
    "\n",
    "            # Plot sample\n",
    "            plot_idx = 5\n",
    "            idx = 0\n",
    "            for n, (key, val) in enumerate(temporal_output.items()):\n",
    "                # Plot loss\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                sample_loss = temporal_loss_dict[key].item()\n",
    "                loss_li_dict[key].append(sample_loss)\n",
    "                mean_loss_li_dict[key].append(np.array(loss_li_dict[key]).mean())\n",
    "                plt.plot(mean_loss_li_dict[key])\n",
    "                plt.title(f\"{key} loss: {mean_loss_li_dict[key][-1]}\")\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval loss\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                eval_sample_loss = eval_temporal_loss_dict[key].item()\n",
    "                eval_loss_li_dict[key].append(eval_sample_loss)\n",
    "                eval_mean_loss_li_dict[key].append(np.array(eval_loss_li_dict[key]).mean())\n",
    "                plt.plot(eval_mean_loss_li_dict[key])\n",
    "                plt.title(f\"{key} eval loss: {eval_mean_loss_li_dict[key][-1]}\")\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval sample\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                eval_sample_y = eval_temporal_masked_y_dict[key].detach().cpu()\n",
    "                eval_sample_pred = eval_temporal_masked_pred_dict[key].detach().cpu()\n",
    "                if key in data_info.processing_info[\"embedding_cols\"]:\n",
    "                    eval_sample_pred = torch.argmax(eval_sample_pred, dim=-1)\n",
    "                plt.plot(eval_sample_y[idx], label=\"y\")\n",
    "                plt.plot(eval_sample_pred[idx], label=\"pred\")\n",
    "                plt.title(f\"{key} eval sample\")\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval heatmap\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                target_col = data_info.modality_info[\"target\"][0]\n",
    "                if key == target_col:\n",
    "                    weight = self_attn_dict[target_col][idx]\n",
    "                    sns.heatmap(weight.detach().cpu())\n",
    "                else:\n",
    "                    weight = cross_attn_dict[target_col][idx]\n",
    "                    weight = torch.mean(weight, dim=0).squeeze()[:, n]\n",
    "                    plt.plot(weight.detach().cpu())\n",
    "\n",
    "                plot_idx += 1\n",
    "\n",
    "            for key, val in img_output.items():\n",
    "                # Plot loss\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                sample_loss = img_loss_dict[key].item()\n",
    "                loss_li_dict[key].append(sample_loss)\n",
    "                mean_loss_li_dict[key].append(np.array(loss_li_dict[key]).mean())\n",
    "                plt.plot(mean_loss_li_dict[key])\n",
    "                plt.title(f\"{key} loss: {mean_loss_li_dict[key][-1]}\")\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval loss\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                eval_sample_loss = eval_img_loss_dict[key].item()\n",
    "                eval_loss_li_dict[key].append(eval_sample_loss)\n",
    "                eval_mean_loss_li_dict[key].append(np.array(eval_loss_li_dict[key]).mean())\n",
    "                plt.plot(eval_mean_loss_li_dict[key])\n",
    "                plt.title(f\"{key} eval loss: {eval_mean_loss_li_dict[key][-1]}\")\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval sample\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                eval_sample_y = patchify(data[key])[idx]\n",
    "                eval_sample_pred = eval_img_masked_pred_dict[key].detach().cpu()[idx]\n",
    "                masked_idx = idx_dict[f\"{key}_masked_idx\"][idx]\n",
    "\n",
    "                eval_sample_y[masked_idx] = eval_sample_pred\n",
    "                eval_sample_y = unpatchify(eval_sample_y.unsqueeze(0))\n",
    "                eval_sample_y = torch.permute(eval_sample_y.squeeze(), (1,2,0))\n",
    "                plt.imshow(eval_sample_y)\n",
    "                plot_idx += 1\n",
    "\n",
    "                # Plot eval heatmap\n",
    "                plt.subplot(nrows, ncols, plot_idx)\n",
    "                patch_size = 16\n",
    "                weight = cross_attn_dict[target_col][idx]\n",
    "                weight = torch.mean(weight, dim=0).squeeze()[:, n:].detach().cpu()\n",
    "                weight = torch.mean(weight, dim=0).numpy()\n",
    "                weight = weight[1:].reshape(224//patch_size,224//patch_size)\n",
    "                weight = cv2.resize(weight / weight.max(), data[f\"{key}_raw\"][idx].size)[..., np.newaxis]\n",
    "                plt.imshow(weight)\n",
    "\n",
    "                plot_idx += 1\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "epoch = 3\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
