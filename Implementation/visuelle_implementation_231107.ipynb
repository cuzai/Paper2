{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd; pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import holidays\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import pytorch_forecasting as pf\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import SwinModel\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General params\n",
    "random_state = 0\n",
    "\n",
    "torch.manual_seed(random_state)\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>external_code</th>\n",
       "      <th>season</th>\n",
       "      <th>category</th>\n",
       "      <th>release_date</th>\n",
       "      <th>day</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>image_path</th>\n",
       "      <th>color</th>\n",
       "      <th>fabric</th>\n",
       "      <th>extra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004695</td>\n",
       "      <td>0.073239</td>\n",
       "      <td>0.061972</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.046009</td>\n",
       "      <td>0.043192</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>0.019718</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.010329</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>1</td>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>PE17/00001.png</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.109859</td>\n",
       "      <td>0.128638</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>0.082629</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.063850</td>\n",
       "      <td>0.052582</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>0.138967</td>\n",
       "      <td>0.159624</td>\n",
       "      <td>0.055399</td>\n",
       "      <td>2</td>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>PE17/00002.png</td>\n",
       "      <td>brown</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.207512</td>\n",
       "      <td>0.177465</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.015023</td>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.010329</td>\n",
       "      <td>0.005634</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>3</td>\n",
       "      <td>SS17</td>\n",
       "      <td>culottes</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>PE17/00003.png</td>\n",
       "      <td>blue</td>\n",
       "      <td>scuba crepe</td>\n",
       "      <td>hem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.041315</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.023474</td>\n",
       "      <td>0.016901</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.020657</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>4</td>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>PE17/00004.png</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>sleeveless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006573</td>\n",
       "      <td>0.098592</td>\n",
       "      <td>0.125822</td>\n",
       "      <td>0.120188</td>\n",
       "      <td>0.068545</td>\n",
       "      <td>0.046948</td>\n",
       "      <td>0.043192</td>\n",
       "      <td>0.034742</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.029108</td>\n",
       "      <td>0.033803</td>\n",
       "      <td>0.009390</td>\n",
       "      <td>5</td>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>PE17/00005.png</td>\n",
       "      <td>grey</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.004695  0.073239  0.061972  0.066667  0.046009  0.043192  0.026291   \n",
       "1  0.005634  0.109859  0.128638  0.135211  0.082629  0.098592  0.063850   \n",
       "2  0.002817  0.207512  0.177465  0.095775  0.041315  0.030047  0.015023   \n",
       "3  0.000939  0.044131  0.046948  0.041315  0.028169  0.031925  0.031925   \n",
       "4  0.006573  0.098592  0.125822  0.120188  0.068545  0.046948  0.043192   \n",
       "\n",
       "          7         8         9        10        11  external_code season  \\\n",
       "0  0.019718  0.012207  0.014085  0.010329  0.009390              1   SS17   \n",
       "1  0.052582  0.034742  0.138967  0.159624  0.055399              2   SS17   \n",
       "2  0.006573  0.010329  0.005634  0.002817  0.001878              3   SS17   \n",
       "3  0.023474  0.016901  0.028169  0.020657  0.009390              4   SS17   \n",
       "4  0.034742  0.030047  0.029108  0.033803  0.009390              5   SS17   \n",
       "\n",
       "      category release_date       day      week  month      year  \\\n",
       "0  long sleeve   2016-12-01  0.500000  0.923077    1.0  0.998514   \n",
       "1  long sleeve   2016-12-01  0.500000  0.923077    1.0  0.998514   \n",
       "2     culottes   2016-12-02  0.666667  0.923077    1.0  0.998514   \n",
       "3  long sleeve   2016-12-02  0.666667  0.923077    1.0  0.998514   \n",
       "4  long sleeve   2016-12-02  0.666667  0.923077    1.0  0.998514   \n",
       "\n",
       "       image_path   color       fabric       extra  \n",
       "0  PE17/00001.png  yellow      acrylic         hem  \n",
       "1  PE17/00002.png   brown      acrylic         hem  \n",
       "2  PE17/00003.png    blue  scuba crepe         hem  \n",
       "3  PE17/00004.png  yellow      acrylic  sleeveless  \n",
       "4  PE17/00005.png    grey      acrylic         hem  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"../visuelle/train.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>category</th>\n",
       "      <th>release_day</th>\n",
       "      <th>release_week</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_year</th>\n",
       "      <th>img_path</th>\n",
       "      <th>color</th>\n",
       "      <th>fabric</th>\n",
       "      <th>extra</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>../visuelle/images/PE17/00001.png</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>../visuelle/images/PE17/00002.png</td>\n",
       "      <td>brown</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SS17</td>\n",
       "      <td>culottes</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>../visuelle/images/PE17/00003.png</td>\n",
       "      <td>blue</td>\n",
       "      <td>scuba crepe</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>../visuelle/images/PE17/00004.png</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>sleeveless</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>../visuelle/images/PE17/00005.png</td>\n",
       "      <td>grey</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  season     category  release_day  release_week  release_month  release_year  \\\n",
       "0   SS17  long sleeve     0.500000      0.923077            1.0      0.998514   \n",
       "1   SS17  long sleeve     0.500000      0.923077            1.0      0.998514   \n",
       "2   SS17     culottes     0.666667      0.923077            1.0      0.998514   \n",
       "3   SS17  long sleeve     0.666667      0.923077            1.0      0.998514   \n",
       "4   SS17  long sleeve     0.666667      0.923077            1.0      0.998514   \n",
       "\n",
       "                            img_path   color       fabric       extra  \\\n",
       "0  ../visuelle/images/PE17/00001.png  yellow      acrylic         hem   \n",
       "1  ../visuelle/images/PE17/00002.png   brown      acrylic         hem   \n",
       "2  ../visuelle/images/PE17/00003.png    blue  scuba crepe         hem   \n",
       "3  ../visuelle/images/PE17/00004.png  yellow      acrylic  sleeveless   \n",
       "4  ../visuelle/images/PE17/00005.png    grey      acrylic         hem   \n",
       "\n",
       "   time_idx  sales  \n",
       "0         0    5.0  \n",
       "1         0    6.0  \n",
       "2         0    3.0  \n",
       "3         0    1.0  \n",
       "4         0    7.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep = df_raw.copy().drop([\"external_code\", \"release_date\"], axis=1).rename(columns={i:f\"release_{i}\" for i in [\"day\", \"week\", \"month\", \"year\"]}).rename(columns={\"image_path\":\"img_path\"})\n",
    "df_prep = pd.melt(df_prep, id_vars=[\"season\", \"category\", \"release_day\", \"release_week\", \"release_month\", \"release_year\", \"img_path\", \"color\", \"fabric\", \"extra\"], value_vars=[f\"{i}\" for i in range(12)], var_name=\"time_idx\", value_name=\"sales\")\n",
    "df_prep[\"img_path\"] = \"../visuelle/images/\" + df_prep[\"img_path\"]\n",
    "df_prep[\"time_idx\"] = df_prep[\"time_idx\"].astype(int)\n",
    "\n",
    "df_prep[\"-1\"] = df_prep.groupby(\"img_path\")[\"time_idx\"].shift(-1).dropna()\n",
    "assert len(df_prep[df_prep[\"-1\"] - df_prep[\"time_idx\"] > 1]) == 0\n",
    "df_prep = df_prep.drop(\"-1\", axis=1)\n",
    "df_prep[\"sales\"] = df_prep[\"sales\"] * np.load(\"../visuelle/normalization_scale.npy\")\n",
    "\n",
    "import joblib\n",
    "joblib.dump(df_prep, \"df_prep.pkl\")\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>category</th>\n",
       "      <th>release_day</th>\n",
       "      <th>release_week</th>\n",
       "      <th>release_month</th>\n",
       "      <th>release_year</th>\n",
       "      <th>img_path</th>\n",
       "      <th>color</th>\n",
       "      <th>fabric</th>\n",
       "      <th>extra</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>2701</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>2702</td>\n",
       "      <td>brown</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SS17</td>\n",
       "      <td>culottes</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>2703</td>\n",
       "      <td>blue</td>\n",
       "      <td>scuba crepe</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>2704</td>\n",
       "      <td>yellow</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>sleeveless</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SS17</td>\n",
       "      <td>long sleeve</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998514</td>\n",
       "      <td>2705</td>\n",
       "      <td>grey</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60955</th>\n",
       "      <td>AW19</td>\n",
       "      <td>sheath dress</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2696</td>\n",
       "      <td>black</td>\n",
       "      <td>scuba crepe</td>\n",
       "      <td>racerback</td>\n",
       "      <td>11</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60956</th>\n",
       "      <td>AW19</td>\n",
       "      <td>kimono dress</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2697</td>\n",
       "      <td>black</td>\n",
       "      <td>nice</td>\n",
       "      <td>zipper</td>\n",
       "      <td>11</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60957</th>\n",
       "      <td>AW19</td>\n",
       "      <td>medium coat</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2698</td>\n",
       "      <td>black</td>\n",
       "      <td>foam rubber</td>\n",
       "      <td>sleeve</td>\n",
       "      <td>11</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60958</th>\n",
       "      <td>AW19</td>\n",
       "      <td>kimono dress</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2699</td>\n",
       "      <td>green</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>11</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60959</th>\n",
       "      <td>AW19</td>\n",
       "      <td>kimono dress</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2700</td>\n",
       "      <td>red</td>\n",
       "      <td>acrylic</td>\n",
       "      <td>hem</td>\n",
       "      <td>11</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60960 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season      category  release_day  release_week  release_month  \\\n",
       "0       SS17   long sleeve     0.500000      0.923077       1.000000   \n",
       "1       SS17   long sleeve     0.500000      0.923077       1.000000   \n",
       "2       SS17      culottes     0.666667      0.923077       1.000000   \n",
       "3       SS17   long sleeve     0.666667      0.923077       1.000000   \n",
       "4       SS17   long sleeve     0.666667      0.923077       1.000000   \n",
       "...      ...           ...          ...           ...            ...   \n",
       "60955   AW19  sheath dress     0.333333      0.826923       0.833333   \n",
       "60956   AW19  kimono dress     0.333333      0.826923       0.833333   \n",
       "60957   AW19   medium coat     0.500000      0.826923       0.833333   \n",
       "60958   AW19  kimono dress     0.666667      0.826923       0.833333   \n",
       "60959   AW19  kimono dress     0.666667      0.826923       0.833333   \n",
       "\n",
       "       release_year  img_path   color       fabric       extra  time_idx  \\\n",
       "0          0.998514      2701  yellow      acrylic         hem         0   \n",
       "1          0.998514      2702   brown      acrylic         hem         0   \n",
       "2          0.998514      2703    blue  scuba crepe         hem         0   \n",
       "3          0.998514      2704  yellow      acrylic  sleeveless         0   \n",
       "4          0.998514      2705    grey      acrylic         hem         0   \n",
       "...             ...       ...     ...          ...         ...       ...   \n",
       "60955      1.000000      2696   black  scuba crepe   racerback        11   \n",
       "60956      1.000000      2697   black         nice      zipper        11   \n",
       "60957      1.000000      2698   black  foam rubber      sleeve        11   \n",
       "60958      1.000000      2699   green      acrylic         hem        11   \n",
       "60959      1.000000      2700     red      acrylic         hem        11   \n",
       "\n",
       "       sales  \n",
       "0        5.0  \n",
       "1        6.0  \n",
       "2        3.0  \n",
       "3        1.0  \n",
       "4        7.0  \n",
       "...      ...  \n",
       "60955   30.0  \n",
       "60956   16.0  \n",
       "60957   22.0  \n",
       "60958  105.0  \n",
       "60959   60.0  \n",
       "\n",
       "[60960 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep = joblib.load(\"df_prep.pkl\")\n",
    "df_train = df_prep.copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_train[\"img_path\"] = encoder.fit_transform(df_train[\"img_path\"]) + 1\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 3 # a whole year\n",
    "max_encoder_length = 9\n",
    "\n",
    "# For training/evaluation splits\n",
    "training_cutoff = df_train[\"time_idx\"].max() - max_prediction_length # validation on 2020\n",
    "\n",
    "# Create training set\n",
    "training_dataset = pf.TimeSeriesDataSet(\n",
    "    df_train[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"sales\",\n",
    "    group_ids=[\"img_path\"], # static covariates\n",
    "    min_encoder_length=max_encoder_length//2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_reals = [\"img_path\", \"release_day\", \"release_week\", \"release_month\", \"release_year\"],\n",
    "    static_categoricals = [\"season\", \"category\", \"color\", \"fabric\", \"extra\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=['sales'],\n",
    "    # target_normalizer=pf.data.GroupNormalizer(\n",
    "    #     groups=[\"img_path\"], transformation=\"softplus\"\n",
    "    # ),  # use softplus transformation and normalize by group\n",
    "    target_normalizer = None,\n",
    "    # lags={'sales': [7, 30, 365]}, # add lagged values of target variable\n",
    "    scalers = {\"img_path\":None},\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time for each series\n",
    "validation_dataset = pf.TimeSeriesDataSet.from_dataset(training_dataset, # dataset from which to copy parameters (encoders, scalers, ...)\n",
    "                                            df_train, # data from which new dataset will be generated\n",
    "                                            predict=True, # predict the decoder length on the last entries in the time index\n",
    "                                            stop_randomization=True)\n",
    "\n",
    "# create training and validation dataloaders for model\n",
    "batch_size = 64\n",
    "train_dataloader = training_dataset.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "val_dataloader = validation_dataset.to_dataloader(train=False, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "# print(len(train_dataloader))\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Timeseries models share a number of common characteristics. This module implements these in a common base class.\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Callable, Dict, Iterable, List, Literal, Optional, Tuple, Union\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks import BasePredictionWriter, LearningRateFinder\n",
    "from lightning.pytorch.trainer.states import RunningStage\n",
    "from lightning.pytorch.utilities.parsing import AttributeDict, get_init_args\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.lib.function_base import iterable\n",
    "import pandas as pd\n",
    "import pytorch_optimizer\n",
    "from pytorch_optimizer import Ranger21\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import rnn\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "import yaml\n",
    "\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import EncoderNormalizer, GroupNormalizer, MultiNormalizer, NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import (\n",
    "    MAE,\n",
    "    MASE,\n",
    "    SMAPE,\n",
    "    DistributionLoss,\n",
    "    MultiHorizonMetric,\n",
    "    MultiLoss,\n",
    "    QuantileLoss,\n",
    "    convert_torchmetric_to_pytorch_forecasting_metric,\n",
    ")\n",
    "from pytorch_forecasting.metrics.base_metrics import Metric\n",
    "from pytorch_forecasting.models.nn.embeddings import MultiEmbedding\n",
    "from pytorch_forecasting.utils import (\n",
    "    InitialParameterRepresenterMixIn,\n",
    "    OutputMixIn,\n",
    "    TupleOutputMixIn,\n",
    "    apply_to_list,\n",
    "    concat_sequences,\n",
    "    create_mask,\n",
    "    get_embedding_size,\n",
    "    groupby_apply,\n",
    "    to_list,\n",
    ")\n",
    "\n",
    "# todo: compile models\n",
    "\n",
    "\n",
    "def _torch_cat_na(x: List[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Concatenate tensor along ``dim=0`` and add nans along ``dim=1`` if necessary.\n",
    "\n",
    "    Allows concatenation of tensors where ``dim=1`` are not equal.\n",
    "    Missing values are filled up with ``nan``.\n",
    "\n",
    "    Args:\n",
    "        x (List[torch.Tensor]): list of tensors to concatenate along dimension 0\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: concatenated tensor\n",
    "    \"\"\"\n",
    "    if x[0].ndim > 1:\n",
    "        first_lens = [xi.shape[1] for xi in x]\n",
    "        max_first_len = max(first_lens)\n",
    "        if max_first_len > min(first_lens):\n",
    "            x = [\n",
    "                xi\n",
    "                if xi.shape[1] == max_first_len\n",
    "                else torch.cat(\n",
    "                    [\n",
    "                        xi,\n",
    "                        torch.full(\n",
    "                            (xi.shape[0], max_first_len - xi.shape[1], *xi.shape[2:]), float(\"nan\"), device=xi.device\n",
    "                        ),\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "                for xi in x\n",
    "            ]\n",
    "\n",
    "    # check if remaining dimensions are all equal\n",
    "    if x[0].ndim > 2:\n",
    "        remaining_dimensions_equal = all([all([xi.size(i) == x[0].size(i) for xi in x]) for i in range(2, x[0].ndim)])\n",
    "    else:\n",
    "        remaining_dimensions_equal = True\n",
    "\n",
    "    # deaggregate\n",
    "    if remaining_dimensions_equal:\n",
    "        return torch.cat(x, dim=0)\n",
    "    else:\n",
    "        # make list instead but warn\n",
    "        warnings.warn(\n",
    "            f\"Not all dimensions are equal for tensors shapes. Example tensor {x[0].shape}. \"\n",
    "            \"Returning list instead of torch.Tensor.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        return [xii for xi in x for xii in xi]\n",
    "\n",
    "\n",
    "\n",
    "def _concatenate_output(\n",
    "    output: List[Dict[str, List[Union[List[torch.Tensor], torch.Tensor, bool, int, str, np.ndarray]]]]\n",
    ") -> Dict[str, Union[torch.Tensor, np.ndarray, List[Union[torch.Tensor, int, bool, str]]]]:\n",
    "    \"\"\"\n",
    "    Concatenate multiple batches of output dictionary.\n",
    "\n",
    "    Args:\n",
    "        output (List[Dict[str, List[Union[List[torch.Tensor], torch.Tensor, bool, int, str, np.ndarray]]]]):\n",
    "            list of outputs to concatenate. Each entry corresponds to a batch.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[torch.Tensor, np.ndarray, List[Union[torch.Tensor, int, bool, str]]]]:\n",
    "            concatenated output\n",
    "    \"\"\"\n",
    "    output_cat = {}\n",
    "    for name in output[0].keys():\n",
    "        v0 = output[0][name]\n",
    "        # concatenate simple tensors\n",
    "        if isinstance(v0, torch.Tensor):\n",
    "            output_cat[name] = _torch_cat_na([out[name] for out in output])\n",
    "        # concatenate list of tensors\n",
    "        elif isinstance(v0, (tuple, list)) and len(v0) > 0:\n",
    "            output_cat[name] = []\n",
    "            for target_id in range(len(v0)):\n",
    "                if isinstance(v0[target_id], torch.Tensor):\n",
    "                    output_cat[name].append(_torch_cat_na([out[name][target_id] for out in output]))\n",
    "                else:\n",
    "                    try:\n",
    "                        output_cat[name].append(np.concatenate([out[name][target_id] for out in output], axis=0))\n",
    "                    except ValueError:\n",
    "                        output_cat[name] = [item for out in output for item in out[name][target_id]]\n",
    "        # flatten list for everything else\n",
    "        else:\n",
    "            try:\n",
    "                output_cat[name] = np.concatenate([out[name] for out in output], axis=0)\n",
    "            except ValueError:\n",
    "                if iterable(output[0][name]):\n",
    "                    output_cat[name] = [item for out in output for item in out[name]]\n",
    "                else:\n",
    "                    output_cat[name] = [out[name] for out in output]\n",
    "\n",
    "    if isinstance(output[0], OutputMixIn):\n",
    "        output_cat = output[0].__class__(**output_cat)\n",
    "    return output_cat\n",
    "\n",
    "\n",
    "\n",
    "STAGE_STATES = {\n",
    "    RunningStage.TRAINING: \"train\",\n",
    "    RunningStage.VALIDATING: \"val\",\n",
    "    RunningStage.TESTING: \"test\",\n",
    "    RunningStage.PREDICTING: \"predict\",\n",
    "    RunningStage.SANITY_CHECKING: \"sanity_check\",\n",
    "}\n",
    "\n",
    "# return type of predict function\n",
    "PredictTuple = namedtuple(\n",
    "    \"prediction\", [\"output\", \"x\", \"index\", \"decoder_lengths\", \"y\"], defaults=(None, None, None, None, None)\n",
    ")\n",
    "\n",
    "\n",
    "class Prediction(PredictTuple, OutputMixIn):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class PredictCallback(BasePredictionWriter):\n",
    "    \"\"\"Internally used callback to capture predictions and optionally write them to disk.\"\"\"\n",
    "\n",
    "    # see base class predict function for documentation of parameters\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: Union[str, Tuple[str, str]] = \"prediction\",\n",
    "        return_index: bool = False,\n",
    "        return_decoder_lengths: bool = False,\n",
    "        return_y: bool = False,\n",
    "        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"batch\",\n",
    "        return_x: bool = False,\n",
    "        mode_kwargs: Dict[str, Any] = None,\n",
    "        output_dir: Optional[str] = None,\n",
    "        predict_kwargs: Dict[str, Any] = None,\n",
    "    ) -> None:\n",
    "        super().__init__(write_interval=write_interval)\n",
    "        self.mode = mode\n",
    "        self.return_decoder_lengths = return_decoder_lengths\n",
    "        self.return_x = return_x\n",
    "        self.return_index = return_index\n",
    "        self.return_y = return_y\n",
    "        self.mode_kwargs = mode_kwargs if mode_kwargs is not None else {}\n",
    "        self.predict_kwargs = predict_kwargs if predict_kwargs is not None else {}\n",
    "        self.output_dir = output_dir\n",
    "        self._reset_data()\n",
    "\n",
    "    def _reset_data(self, result: bool = True):\n",
    "        # reset data objects to save results into\n",
    "        self._output = []\n",
    "        self._decode_lengths = []\n",
    "        self._x_list = []\n",
    "        self._index = []\n",
    "        self._y = []\n",
    "        if result:\n",
    "            self._result = []\n",
    "\n",
    "    def on_predict_batch_end(\n",
    "        self,\n",
    "        trainer: Trainer,\n",
    "        pl_module: LightningModule,\n",
    "        outputs: Any,\n",
    "        batch: Any,\n",
    "        batch_idx: int,\n",
    "        dataloader_idx: int = 0,\n",
    "    ) -> None:\n",
    "        # extract predictions form output\n",
    "        x = batch[0]\n",
    "        out = outputs\n",
    "\n",
    "        lengths = x[\"decoder_lengths\"]\n",
    "\n",
    "        nan_mask = create_mask(lengths.max(), lengths)\n",
    "        if isinstance(self.mode, (tuple, list)):\n",
    "            if self.mode[0] == \"raw\":\n",
    "                out = out[self.mode[1]]\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"If a tuple is specified, the first element must be 'raw' - got {self.mode[0]} instead\"\n",
    "                )\n",
    "        elif self.mode == \"prediction\":\n",
    "            out = pl_module.to_prediction(out, **self.mode_kwargs)\n",
    "            # mask non-predictions\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = [\n",
    "                    o.masked_fill(nan_mask, torch.tensor(float(\"nan\"))) if o.dtype == torch.float else o for o in out\n",
    "                ]\n",
    "            elif out.dtype == torch.float:  # only floats can be filled with nans\n",
    "                out = out.masked_fill(nan_mask, torch.tensor(float(\"nan\")))\n",
    "        elif self.mode == \"quantiles\":\n",
    "            out = pl_module.to_quantiles(out, **self.mode_kwargs)\n",
    "            # mask non-predictions\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = [\n",
    "                    o.masked_fill(nan_mask.unsqueeze(-1), torch.tensor(float(\"nan\"))) if o.dtype == torch.float else o\n",
    "                    for o in out\n",
    "                ]\n",
    "            elif out.dtype == torch.float:\n",
    "                out = out.masked_fill(nan_mask.unsqueeze(-1), torch.tensor(float(\"nan\")))\n",
    "        elif self.mode == \"raw\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode {self.mode} - see docs for valid arguments\")\n",
    "\n",
    "        self._output.append(out)\n",
    "        out = dict(output=out)\n",
    "        if self.return_x:\n",
    "            self._x_list.append(x)\n",
    "            out[\"x\"] = self._x_list[-1]\n",
    "        if self.return_index:\n",
    "            self._index.append(trainer.predict_dataloaders.dataset.x_to_index(x))\n",
    "            out[\"index\"] = self._index[-1]\n",
    "        if self.return_decoder_lengths:\n",
    "            self._decode_lengths.append(lengths)\n",
    "            out[\"decoder_lengths\"] = self._decode_lengths[-1]\n",
    "        if self.return_y:\n",
    "            self._y.append(batch[1])\n",
    "            out[\"y\"] = self._y[-1]\n",
    "\n",
    "        if isinstance(out, dict):\n",
    "            out = Prediction(**out)\n",
    "        # write to disk\n",
    "        if self.output_dir is not None:\n",
    "            super().on_predict_batch_end(trainer, pl_module, out, batch, batch_idx, dataloader_idx)\n",
    "\n",
    "\n",
    "    def write_on_batch_end(self, trainer, pl_module, prediction, batch_indices, batch, batch_idx, dataloader_idx):\n",
    "        torch.save(prediction, os.path.join(self.output_dir, f\"predictions_{batch_idx}.pt\"))\n",
    "        self._reset_data()\n",
    "\n",
    "\n",
    "    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n",
    "        torch.save(predictions, os.path.join(self.output_dir, \"predictions.pt\"))\n",
    "        self._reset_data()\n",
    "\n",
    "\n",
    "    def on_predict_epoch_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
    "        output = self._output\n",
    "        if len(output) > 0:\n",
    "            # concatenate output (of different batches)\n",
    "            if isinstance(self.mode, (tuple, list)) or self.mode != \"raw\":\n",
    "                if (\n",
    "                    isinstance(output[0], (tuple, list))\n",
    "                    and len(output[0]) > 0\n",
    "                    and isinstance(output[0][0], torch.Tensor)\n",
    "                ):\n",
    "                    output = [_torch_cat_na([out[idx] for out in output]) for idx in range(len(output[0]))]\n",
    "                else:\n",
    "                    output = _torch_cat_na(output)\n",
    "            elif self.mode == \"raw\":\n",
    "                output = _concatenate_output(output)\n",
    "\n",
    "            # if len(output) > 0:\n",
    "            # generate output\n",
    "            if self.return_x or self.return_index or self.return_decoder_lengths or self.return_y:\n",
    "                output = dict(output=output)\n",
    "            if self.return_x:\n",
    "                output[\"x\"] = _concatenate_output(self._x_list)\n",
    "            if self.return_index:\n",
    "                output[\"index\"] = pd.concat(self._index, axis=0, ignore_index=True)\n",
    "            if self.return_decoder_lengths:\n",
    "                output[\"decoder_lengths\"] = torch.cat(self._decode_lengths, dim=0)\n",
    "            if self.return_y:\n",
    "                y = concat_sequences([yi[0] for yi in self._y])\n",
    "                if self._y[-1][1] is None:\n",
    "                    weight = None\n",
    "                else:\n",
    "                    weight = concat_sequences([yi[1] for yi in self._y])\n",
    "\n",
    "                output[\"y\"] = (y, weight)\n",
    "            if isinstance(output, dict):\n",
    "                output = Prediction(**output)  # save for later writing or outputting\n",
    "            self._result = output\n",
    "\n",
    "            # write to disk\n",
    "            if self.interval.on_epoch:\n",
    "                self.write_on_epoch_end(trainer, pl_module, self._output, trainer.predict_loop.epoch_batch_indices)\n",
    "            self._reset_data(result=False)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def result(self) -> Prediction:\n",
    "        if self.output_dir is None:\n",
    "            return self._result\n",
    "        else:\n",
    "            assert len(self._result) == 0, \"Cannot return result if output_dir is set\"\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(InitialParameterRepresenterMixIn, LightningModule, TupleOutputMixIn):\n",
    "    \"\"\"\n",
    "    BaseModel from which new timeseries models should inherit from.\n",
    "    The ``hparams`` of the created object will default to the parameters indicated in :py:meth:`~__init__`.\n",
    "\n",
    "    The :py:meth:`~BaseModel.forward` method should return a named tuple with at least the entry ``prediction``\n",
    "    that contains the network's output. See the function's documentation for more details.\n",
    "\n",
    "    The idea of the base model is that common methods do not have to be re-implemented for every new architecture.\n",
    "    The class is a [LightningModule](https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html)\n",
    "    and follows its conventions. However, there are important additions:\n",
    "\n",
    "        * You need to specify a ``loss`` attribute that stores the function to calculate the\n",
    "          :py:class:`~pytorch_forecasting.metrics.MultiHorizonLoss` for backpropagation.\n",
    "        * The :py:meth:`~BaseModel.from_dataset` method can be used to initialize a network using the specifications\n",
    "          of a dataset. Often, parameters such as the number of features can be easily deduced from the dataset.\n",
    "          Further, the method will also store how to rescale normalized predictions into the unnormalized prediction\n",
    "          space. Override it to pass additional arguments to the __init__ method of your network that depend on your\n",
    "          dataset.\n",
    "        * The :py:meth:`~BaseModel.transform_output` method rescales the network output using the target normalizer\n",
    "          from thedataset.\n",
    "        * The :py:meth:`~BaseModel.step` method takes care of calculating the loss, logging additional metrics defined\n",
    "          in the ``logging_metrics`` attribute and plots of sample predictions. You can override this method to add\n",
    "          custom interpretations or pass extra arguments to the networks forward method.\n",
    "        * The :py:meth:`~BaseModel.on_epoch_end` method can be used to calculate summaries of each epoch such as\n",
    "          statistics on the encoder length, etc and needs to return the outputs.\n",
    "        * The :py:meth:`~BaseModel.predict` method makes predictions using a dataloader or dataset. Override it if you\n",
    "          need to pass additional arguments to ``forward`` by default.\n",
    "\n",
    "    To implement your own architecture, it is best to\n",
    "    go throught the :ref:`Using custom data and implementing custom models <new-model-tutorial>` and\n",
    "    to look at existing ones to understand what might be a good approach.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            class Network(BaseModel):\n",
    "\n",
    "                def __init__(self, my_first_parameter: int=2, loss=SMAPE()):\n",
    "                    self.save_hyperparameters()\n",
    "                    super().__init__(loss=loss)\n",
    "\n",
    "                def forward(self, x):\n",
    "                    normalized_prediction = self.module(x)\n",
    "                    prediction = self.transform_output(prediction=normalized_prediction, target_scale=x[\"target_scale\"])\n",
    "                    return self.to_network_output(prediction=prediction)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    CHECKPOINT_HYPER_PARAMS_SPECIAL_KEY = \"__special_save__\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_parameters: Dict[str, Any] = None,\n",
    "        log_interval: Union[int, float] = -1,\n",
    "        log_val_interval: Union[int, float] = None,\n",
    "        learning_rate: Union[float, List[float]] = 1e-3,\n",
    "        log_gradient_flow: bool = False,\n",
    "        loss: Metric = SMAPE(),\n",
    "        logging_metrics: nn.ModuleList = nn.ModuleList([]),\n",
    "        reduce_on_plateau_patience: int = 1000,\n",
    "        reduce_on_plateau_reduction: float = 2.0,\n",
    "        reduce_on_plateau_min_lr: float = 1e-5,\n",
    "        weight_decay: float = 0.0,\n",
    "        optimizer_params: Dict[str, Any] = None,\n",
    "        monotone_constaints: Dict[str, int] = {},\n",
    "        output_transformer: Callable = None,\n",
    "        optimizer=\"Ranger\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        BaseModel for timeseries forecasting from which to inherit from\n",
    "\n",
    "        Args:\n",
    "            log_interval (Union[int, float], optional): Batches after which predictions are logged. If < 1.0, will log\n",
    "                multiple entries per batch. Defaults to -1.\n",
    "            log_val_interval (Union[int, float], optional): batches after which predictions for validation are\n",
    "                logged. Defaults to None/log_interval.\n",
    "            learning_rate (float, optional): Learning rate. Defaults to 1e-3.\n",
    "            log_gradient_flow (bool): If to log gradient flow, this takes time and should be only done to diagnose\n",
    "                training failures. Defaults to False.\n",
    "            loss (Metric, optional): metric to optimize, can also be list of metrics. Defaults to SMAPE().\n",
    "            logging_metrics (nn.ModuleList[MultiHorizonMetric]): list of metrics that are logged during training.\n",
    "                Defaults to [].\n",
    "            reduce_on_plateau_patience (int): patience after which learning rate is reduced by a factor of 10. Defaults\n",
    "                to 1000\n",
    "            reduce_on_plateau_reduction (float): reduction in learning rate when encountering plateau. Defaults to 2.0.\n",
    "            reduce_on_plateau_min_lr (float): minimum learning rate for reduce on plateua learning rate scheduler.\n",
    "                Defaults to 1e-5\n",
    "            weight_decay (float): weight decay. Defaults to 0.0.\n",
    "            optimizer_params (Dict[str, Any]): additional parameters for the optimizer. Defaults to {}.\n",
    "            monotone_constaints (Dict[str, int]): dictionary of monotonicity constraints for continuous decoder\n",
    "                variables mapping\n",
    "                position (e.g. ``\"0\"`` for first position) to constraint (``-1`` for negative and ``+1`` for positive,\n",
    "                larger numbers add more weight to the constraint vs. the loss but are usually not necessary).\n",
    "                This constraint significantly slows down training. Defaults to {}.\n",
    "            output_transformer (Callable): transformer that takes network output and transforms it to prediction space.\n",
    "                Defaults to None which is equivalent to ``lambda out: out[\"prediction\"]``.\n",
    "            optimizer (str): Optimizer, \"ranger\", \"sgd\", \"adam\", \"adamw\" or class name of optimizer in ``torch.optim``\n",
    "                or ``pytorch_optimizer``.\n",
    "                Alternatively, a class or function can be passed which takes parameters as first argument and\n",
    "                a `lr` argument (optionally also `weight_decay`). Defaults to\n",
    "                `\"ranger\" <https://pytorch-optimizers.readthedocs.io/en/latest/optimizer_api.html#ranger21>`_.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # update hparams\n",
    "        frame = inspect.currentframe()\n",
    "        init_args = get_init_args(frame)\n",
    "        self.save_hyperparameters(\n",
    "            {name: val for name, val in init_args.items() if name not in self.hparams and name not in [\"self\"]}\n",
    "        )\n",
    "\n",
    "        # update log interval if not defined\n",
    "        if self.hparams.log_val_interval is None:\n",
    "            self.hparams.log_val_interval = self.hparams.log_interval\n",
    "\n",
    "        if not hasattr(self, \"loss\"):\n",
    "            if isinstance(loss, (tuple, list)):\n",
    "                self.loss = MultiLoss(metrics=[convert_torchmetric_to_pytorch_forecasting_metric(l) for l in loss])\n",
    "            else:\n",
    "                self.loss = convert_torchmetric_to_pytorch_forecasting_metric(loss)\n",
    "        if not hasattr(self, \"logging_metrics\"):\n",
    "            self.logging_metrics = nn.ModuleList(\n",
    "                [convert_torchmetric_to_pytorch_forecasting_metric(l) for l in logging_metrics]\n",
    "            )\n",
    "        if not hasattr(self, \"output_transformer\"):\n",
    "            self.output_transformer = output_transformer\n",
    "        if not hasattr(self, \"optimizer\"):  # callables are removed from hyperparameters, so better to save them\n",
    "            self.optimizer = self.hparams.optimizer\n",
    "        if not hasattr(self, \"dataset_parameters\"):\n",
    "            self.dataset_parameters = dataset_parameters\n",
    "\n",
    "        # delete everything from hparams that cannot be serialized with yaml.dump\n",
    "        # which is particularly important for tensorboard logging\n",
    "        hparams_to_delete = []\n",
    "        for k, v in self.hparams.items():\n",
    "            try:\n",
    "                yaml.dump(v)\n",
    "            except:  # noqa\n",
    "                hparams_to_delete.append(k)\n",
    "                if not hasattr(self, k):\n",
    "                    setattr(self, k, v)\n",
    "\n",
    "        self.hparams_special = getattr(self, \"hparams_special\", [])\n",
    "        self.hparams_special.extend(hparams_to_delete)\n",
    "        for k in hparams_to_delete:\n",
    "            del self._hparams[k]\n",
    "            del self._hparams_initial[k]\n",
    "        # epoch outputs\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.testing_step_outputs = []\n",
    "\n",
    "\n",
    "    def log(self, *args, **kwargs):\n",
    "        \"\"\"See :meth:`lightning.pytorch.core.lightning.LightningModule.log`.\"\"\"\n",
    "        # never log for prediction\n",
    "        if not self.predicting:\n",
    "            super().log(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def predicting(self) -> bool:\n",
    "        return self.current_stage is None or self.current_stage == \"predict\"\n",
    "\n",
    "    @property\n",
    "    def current_stage(self) -> str:\n",
    "        \"\"\"\n",
    "        Available inside lightning loops.\n",
    "        :return: current trainer stage. One of [\"train\", \"val\", \"test\", \"predict\", \"sanity_check\"]\n",
    "        \"\"\"\n",
    "        return STAGE_STATES.get(self.trainer.state.stage, None)\n",
    "\n",
    "    @property\n",
    "    def n_targets(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of targets to forecast.\n",
    "\n",
    "        Based on loss function.\n",
    "\n",
    "        Returns:\n",
    "            int: number of targets\n",
    "        \"\"\"\n",
    "        if isinstance(self.loss, MultiLoss):\n",
    "            return len(self.loss.metrics)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def transform_output(\n",
    "        self,\n",
    "        prediction: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        target_scale: Union[torch.Tensor, List[torch.Tensor]],\n",
    "        loss: Optional[Metric] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract prediction from network output and rescale it to real space / de-normalize it.\n",
    "\n",
    "        Args:\n",
    "            prediction (Union[torch.Tensor, List[torch.Tensor]]): normalized prediction\n",
    "            target_scale (Union[torch.Tensor, List[torch.Tensor]]): scale to rescale prediction\n",
    "            loss (Optional[Metric]): metric to use for transform\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: rescaled prediction\n",
    "        \"\"\"\n",
    "        if loss is None:\n",
    "            loss = self.loss\n",
    "        if isinstance(loss, MultiLoss):\n",
    "            out = loss.rescale_parameters(\n",
    "                prediction,\n",
    "                target_scale=target_scale,\n",
    "                encoder=self.output_transformer.normalizers,  # need to use normalizer per encoder\n",
    "            )\n",
    "        else:\n",
    "            out = loss.rescale_parameters(prediction, target_scale=target_scale, encoder=self.output_transformer)\n",
    "        return out\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def deduce_default_output_parameters(\n",
    "        dataset: TimeSeriesDataSet, kwargs: Dict[str, Any], default_loss: MultiHorizonMetric = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Deduce default parameters for output for `from_dataset()` method.\n",
    "\n",
    "        Determines ``output_size`` and ``loss`` parameters.\n",
    "\n",
    "        Args:\n",
    "            dataset (TimeSeriesDataSet): timeseries dataset\n",
    "            kwargs (Dict[str, Any]): current hyperparameters\n",
    "            default_loss (MultiHorizonMetric, optional): default loss function.\n",
    "                Defaults to :py:class:`~pytorch_forecasting.metrics.MAE`.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: dictionary with ``output_size`` and ``loss``.\n",
    "        \"\"\"\n",
    "\n",
    "        # infer output size\n",
    "        def get_output_size(normalizer, loss):\n",
    "            if isinstance(loss, QuantileLoss):\n",
    "                return len(loss.quantiles)\n",
    "            elif isinstance(normalizer, NaNLabelEncoder):\n",
    "                return len(normalizer.classes_)\n",
    "            elif isinstance(loss, DistributionLoss):\n",
    "                return len(loss.distribution_arguments)\n",
    "            else:\n",
    "                return 1  # default to 1\n",
    "\n",
    "        # handle multiple targets\n",
    "        new_kwargs = {}\n",
    "        n_targets = len(dataset.target_names)\n",
    "        if default_loss is None:\n",
    "            default_loss = MAE()\n",
    "        loss = kwargs.get(\"loss\", default_loss)\n",
    "        if n_targets > 1:  # try to infer number of ouput sizes\n",
    "            if not isinstance(loss, MultiLoss):\n",
    "                loss = MultiLoss([deepcopy(loss)] * n_targets)\n",
    "                new_kwargs[\"loss\"] = loss\n",
    "            if isinstance(loss, MultiLoss) and \"output_size\" not in kwargs:\n",
    "                new_kwargs[\"output_size\"] = [\n",
    "                    get_output_size(normalizer, l)\n",
    "                    for normalizer, l in zip(dataset.target_normalizer.normalizers, loss.metrics)\n",
    "                ]\n",
    "        elif \"output_size\" not in kwargs:\n",
    "            new_kwargs[\"output_size\"] = get_output_size(dataset.target_normalizer, loss)\n",
    "        return new_kwargs\n",
    "\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"\n",
    "        get number of parameters in model\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Train on batch.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        log, out = self.step(x, y, batch_idx)\n",
    "        self.training_step_outputs.append(log)\n",
    "        return log\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.on_epoch_end(self.training_step_outputs)\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        predict_callback = [c for c in self.trainer.callbacks if isinstance(c, PredictCallback)][0]\n",
    "        x, y = batch\n",
    "        _, out = self.step(x, y, batch_idx, **predict_callback.predict_kwargs)\n",
    "        return out  # need to return output to be able to use predict callback\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        log, out = self.step(x, y, batch_idx)\n",
    "        log.update(self.create_log(x, y, out, batch_idx))\n",
    "        self.validation_step_outputs.append(log)\n",
    "        return log\n",
    "\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.on_epoch_end(self.validation_step_outputs)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        log, out = self.step(x, y, batch_idx)\n",
    "        log.update(self.create_log(x, y, out, batch_idx))\n",
    "        self.testing_step_outputs.append(log)\n",
    "        return log\n",
    "\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.on_epoch_end(self.testing_step_outputs)\n",
    "        self.testing_step_outputs.clear()\n",
    "\n",
    "\n",
    "    def create_log(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        y: Tuple[torch.Tensor, torch.Tensor],\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        batch_idx: int,\n",
    "        prediction_kwargs: Dict[str, Any] = {},\n",
    "        quantiles_kwargs: Dict[str, Any] = {},\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create the log used in the training and validation step.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader\n",
    "            y (Tuple[torch.Tensor, torch.Tensor]): y as passed to the loss function by the dataloader\n",
    "            out (Dict[str, torch.Tensor]): output of the network\n",
    "            batch_idx (int): batch number\n",
    "            prediction_kwargs (Dict[str, Any], optional): arguments to pass to\n",
    "                :py:meth:`~pytorch_forcasting.models.base_model.BaseModel.to_prediction`. Defaults to {}.\n",
    "            quantiles_kwargs (Dict[str, Any], optional):\n",
    "                :py:meth:`~pytorch_forcasting.models.base_model.BaseModel.to_quantiles`. Defaults to {}.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: log dictionary to be returned by training and validation steps\n",
    "        \"\"\"\n",
    "        # log\n",
    "        if isinstance(self.loss, DistributionLoss):\n",
    "            prediction_kwargs.setdefault(\"n_samples\", 20)\n",
    "            prediction_kwargs.setdefault(\"use_metric\", True)\n",
    "            quantiles_kwargs.setdefault(\"n_samples\", 20)\n",
    "            quantiles_kwargs.setdefault(\"use_metric\", True)\n",
    "\n",
    "        self.log_metrics(x, y, out, prediction_kwargs=prediction_kwargs)\n",
    "        if self.log_interval > 0:\n",
    "            self.log_prediction(\n",
    "                x, out, batch_idx, prediction_kwargs=prediction_kwargs, quantiles_kwargs=quantiles_kwargs\n",
    "            )\n",
    "        return {}\n",
    "\n",
    "\n",
    "    def step(\n",
    "        self, x: Dict[str, torch.Tensor], y: Tuple[torch.Tensor, torch.Tensor], batch_idx: int, **kwargs\n",
    "    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Run for each train/val step.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader\n",
    "            y (Tuple[torch.Tensor, torch.Tensor]): y as passed to the loss function by the dataloader\n",
    "            batch_idx (int): batch number\n",
    "            **kwargs: additional arguments to pass to the network apart from ``x``\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]: tuple where the first\n",
    "                entry is a dictionary to which additional logging results can be added for consumption in the\n",
    "                ``on_epoch_end`` hook and the second entry is the model's output.\n",
    "        \"\"\"\n",
    "        # pack y sequence if different encoder lengths exist\n",
    "        if (x[\"decoder_lengths\"] < x[\"decoder_lengths\"].max()).any():\n",
    "            if isinstance(y[0], (list, tuple)):\n",
    "                y = (\n",
    "                    [\n",
    "                        rnn.pack_padded_sequence(\n",
    "                            y_part, lengths=x[\"decoder_lengths\"].cpu(), batch_first=True, enforce_sorted=False\n",
    "                        )\n",
    "                        for y_part in y[0]\n",
    "                    ],\n",
    "                    y[1],\n",
    "                )\n",
    "            else:\n",
    "                y = (\n",
    "                    rnn.pack_padded_sequence(\n",
    "                        y[0], lengths=x[\"decoder_lengths\"].cpu(), batch_first=True, enforce_sorted=False\n",
    "                    ),\n",
    "                    y[1],\n",
    "                )\n",
    "\n",
    "        if self.training and len(self.hparams.monotone_constaints) > 0:\n",
    "            # calculate gradient with respect to continous decoder features\n",
    "            x[\"decoder_cont\"].requires_grad_(True)\n",
    "            assert not torch._C._get_cudnn_enabled(), (\n",
    "                \"To use monotone constraints, wrap model and training in context \"\n",
    "                \"`torch.backends.cudnn.flags(enable=False)`\"\n",
    "            )\n",
    "            out = self(x, **kwargs)\n",
    "            prediction = out[\"prediction\"]\n",
    "\n",
    "            # handle multiple targets\n",
    "            prediction_list = to_list(prediction)\n",
    "            gradient = 0\n",
    "            # todo: should monotone constrains be applicable to certain targets?\n",
    "            for pred in prediction_list:\n",
    "                gradient = (\n",
    "                    gradient\n",
    "                    + torch.autograd.grad(\n",
    "                        outputs=pred,\n",
    "                        inputs=x[\"decoder_cont\"],\n",
    "                        grad_outputs=torch.ones_like(pred),  # t\n",
    "                        create_graph=True,  # allows usage in graph\n",
    "                        allow_unused=True,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "            # select relevant features\n",
    "            indices = torch.tensor(\n",
    "                [self.hparams.x_reals.index(name) for name in self.hparams.monotone_constaints.keys()]\n",
    "            )\n",
    "            monotonicity = torch.tensor(\n",
    "                [val for val in self.hparams.monotone_constaints.values()], dtype=gradient.dtype, device=gradient.device\n",
    "            )\n",
    "            # add additionl loss if gradient points in wrong direction\n",
    "            gradient = gradient[..., indices] * monotonicity[None, None]\n",
    "            monotinicity_loss = gradient.clamp_max(0).mean()\n",
    "            # multiply monotinicity loss by large number to ensure relevance and take to the power of 2\n",
    "            # for smoothness of loss function\n",
    "            monotinicity_loss = 10 * torch.pow(monotinicity_loss, 2)\n",
    "            if not self.predicting:\n",
    "                if isinstance(self.loss, (MASE, MultiLoss)):\n",
    "                    loss = self.loss(\n",
    "                        prediction, y, encoder_target=x[\"encoder_target\"], encoder_lengths=x[\"encoder_lengths\"]\n",
    "                    )\n",
    "                else:\n",
    "                    loss = self.loss(prediction, y)\n",
    "\n",
    "                loss = loss * (1 + monotinicity_loss)\n",
    "            else:\n",
    "                loss = None\n",
    "        else:\n",
    "            out = self(x, **kwargs)\n",
    "\n",
    "            # calculate loss\n",
    "            prediction = out[\"prediction\"]\n",
    "            if not self.predicting:\n",
    "                if isinstance(self.loss, (MASE, MultiLoss)):\n",
    "                    mase_kwargs = dict(encoder_target=x[\"encoder_target\"], encoder_lengths=x[\"encoder_lengths\"])\n",
    "                    loss = self.loss(prediction, y, **mase_kwargs)\n",
    "                else:\n",
    "                    loss = self.loss(prediction, y)\n",
    "            else:\n",
    "                loss = None\n",
    "        self.log(\n",
    "            f\"{self.current_stage}_loss\",\n",
    "            loss,\n",
    "            on_step=self.training,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            batch_size=len(x[\"decoder_target\"]),\n",
    "        )\n",
    "        log = {\"loss\": loss, \"n_samples\": x[\"decoder_lengths\"].size(0)}\n",
    "        return log, out\n",
    "\n",
    "\n",
    "    def log_metrics(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        y: torch.Tensor,\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        prediction_kwargs: Dict[str, Any] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log metrics every training/validation step.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader\n",
    "            y (torch.Tensor): y as passed to the loss function by the dataloader\n",
    "            out (Dict[str, torch.Tensor]): output of the network\n",
    "            prediction_kwargs (Dict[str, Any]): parameters for ``to_prediction()`` of the loss metric.\n",
    "        \"\"\"\n",
    "        # logging losses - for each target\n",
    "        if prediction_kwargs is None:\n",
    "            prediction_kwargs = {}\n",
    "        y_hat_point = self.to_prediction(out, **prediction_kwargs)\n",
    "        if isinstance(self.loss, MultiLoss):\n",
    "            y_hat_point_detached = [p.detach() for p in y_hat_point]\n",
    "        else:\n",
    "            y_hat_point_detached = [y_hat_point.detach()]\n",
    "\n",
    "        for metric in self.logging_metrics:\n",
    "            for idx, y_point, y_part, encoder_target in zip(\n",
    "                list(range(len(y_hat_point_detached))),\n",
    "                y_hat_point_detached,\n",
    "                to_list(y[0]),\n",
    "                to_list(x[\"encoder_target\"]),\n",
    "            ):\n",
    "                y_true = (y_part, y[1])\n",
    "                if isinstance(metric, MASE):\n",
    "                    loss_value = metric(\n",
    "                        y_point, y_true, encoder_target=encoder_target, encoder_lengths=x[\"encoder_lengths\"]\n",
    "                    )\n",
    "                else:\n",
    "                    loss_value = metric(y_point, y_true)\n",
    "                if len(y_hat_point_detached) > 1:\n",
    "                    target_tag = self.target_names[idx] + \" \"\n",
    "                else:\n",
    "                    target_tag = \"\"\n",
    "                self.log(\n",
    "                    f\"{target_tag}{self.current_stage}_{metric.name}\",\n",
    "                    loss_value,\n",
    "                    on_step=self.training,\n",
    "                    on_epoch=True,\n",
    "                    batch_size=len(x[\"decoder_target\"]),\n",
    "                )\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, x: Dict[str, Union[torch.Tensor, List[torch.Tensor]]]\n",
    "    ) -> Dict[str, Union[torch.Tensor, List[torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Network forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, Union[torch.Tensor, List[torch.Tensor]]]): network input (x as returned by the dataloader).\n",
    "                See :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.to_dataloader` method that\n",
    "                returns a tuple of ``x`` and ``y``. This function expects ``x``.\n",
    "\n",
    "        Returns:\n",
    "            NamedTuple[Union[torch.Tensor, List[torch.Tensor]]]: network outputs / dictionary of tensors or list\n",
    "                of tensors. Create it using the\n",
    "                :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.to_network_output` method.\n",
    "                The minimal required entries in the dictionary are (and shapes in brackets):\n",
    "\n",
    "                * ``prediction`` (batch_size x n_decoder_time_steps x n_outputs or list thereof with each\n",
    "                  entry for a different target): re-scaled predictions that can be fed to metric. List of tensors\n",
    "                  if multiple targets are predicted at the same time.\n",
    "\n",
    "                Before passing outputting the predictions, you want to rescale them into real space.\n",
    "                By default, you can use the\n",
    "                :py:meth:`~pytorch_forecasting.models.base_model.BaseModel.transform_output`\n",
    "                method to achieve this.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            .. code-block:: python\n",
    "\n",
    "                def forward(self, x:\n",
    "                    # x is a batch generated based on the TimeSeriesDataset, here we just use the\n",
    "                    # continuous variables for the encoder\n",
    "                    network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "                    prediction = self.linear(network_input)  #\n",
    "\n",
    "                    # rescale predictions into target space\n",
    "                    prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "                    # We need to return a dictionary that at least contains the prediction\n",
    "                    # The parameter can be directly forwarded from the input.\n",
    "                    # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "                    return self.to_network_output(prediction=prediction)\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Run at epoch end for training or validation. Can be overriden in models.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    @property\n",
    "    def log_interval(self) -> float:\n",
    "        \"\"\"\n",
    "        Log interval depending if training or validating\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            return self.hparams.log_interval\n",
    "        elif self.predicting:\n",
    "            return -1\n",
    "        else:\n",
    "            return self.hparams.log_val_interval\n",
    "\n",
    "    def log_prediction(\n",
    "        self, x: Dict[str, torch.Tensor], out: Dict[str, torch.Tensor], batch_idx: int, **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Log metrics every training/validation step.\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): x as passed to the network by the dataloader\n",
    "            out (Dict[str, torch.Tensor]): output of the network\n",
    "            batch_idx (int): current batch index\n",
    "            **kwargs: paramters to pass to ``plot_prediction``\n",
    "        \"\"\"\n",
    "        # log single prediction figure\n",
    "        if (batch_idx % self.log_interval == 0 or self.log_interval < 1.0) and self.log_interval > 0:\n",
    "            if self.log_interval < 1.0:  # log multiple steps\n",
    "                log_indices = torch.arange(\n",
    "                    0, len(x[\"encoder_lengths\"]), max(1, round(self.log_interval * len(x[\"encoder_lengths\"])))\n",
    "                )\n",
    "            else:\n",
    "                log_indices = [0]\n",
    "            for idx in log_indices:\n",
    "                fig = self.plot_prediction(x, out, idx=idx, add_loss_to_title=True, **kwargs)\n",
    "                tag = f\"{self.current_stage} prediction\"\n",
    "                if self.training:\n",
    "                    tag += f\" of item {idx} in global batch {self.global_step}\"\n",
    "                else:\n",
    "                    tag += f\" of item {idx} in batch {batch_idx}\"\n",
    "                if isinstance(fig, (list, tuple)):\n",
    "                    for idx, f in enumerate(fig):\n",
    "                        self.logger.experiment.add_figure(\n",
    "                            f\"{self.target_names[idx]} {tag}\",\n",
    "                            f,\n",
    "                            global_step=self.global_step,\n",
    "                        )\n",
    "                else:\n",
    "                    self.logger.experiment.add_figure(\n",
    "                        tag,\n",
    "                        fig,\n",
    "                        global_step=self.global_step,\n",
    "                    )\n",
    "\n",
    "\n",
    "    def plot_prediction(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        idx: int = 0,\n",
    "        add_loss_to_title: Union[Metric, torch.Tensor, bool] = False,\n",
    "        show_future_observed: bool = True,\n",
    "        ax=None,\n",
    "        quantiles_kwargs: Dict[str, Any] = {},\n",
    "        prediction_kwargs: Dict[str, Any] = {},\n",
    "    ) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot prediction of prediction vs actuals\n",
    "\n",
    "        Args:\n",
    "            x: network input\n",
    "            out: network output\n",
    "            idx: index of prediction to plot\n",
    "            add_loss_to_title: if to add loss to title or loss function to calculate. Can be either metrics,\n",
    "                bool indicating if to use loss metric or tensor which contains losses for all samples.\n",
    "                Calcualted losses are determined without weights. Default to False.\n",
    "            show_future_observed: if to show actuals for future. Defaults to True.\n",
    "            ax: matplotlib axes to plot on\n",
    "            quantiles_kwargs (Dict[str, Any]): parameters for ``to_quantiles()`` of the loss metric.\n",
    "            prediction_kwargs (Dict[str, Any]): parameters for ``to_prediction()`` of the loss metric.\n",
    "\n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "        # all true values for y of the first sample in batch\n",
    "        encoder_targets = to_list(x[\"encoder_target\"])\n",
    "        decoder_targets = to_list(x[\"decoder_target\"])\n",
    "\n",
    "        y_raws = to_list(out[\"prediction\"])  # raw predictions - used for calculating loss\n",
    "        y_hats = to_list(self.to_prediction(out, **prediction_kwargs))\n",
    "        y_quantiles = to_list(self.to_quantiles(out, **quantiles_kwargs))\n",
    "\n",
    "        # for each target, plot\n",
    "        figs = []\n",
    "        for y_raw, y_hat, y_quantile, encoder_target, decoder_target in zip(\n",
    "            y_raws, y_hats, y_quantiles, encoder_targets, decoder_targets\n",
    "        ):\n",
    "            y_all = torch.cat([encoder_target[idx], decoder_target[idx]])\n",
    "            max_encoder_length = x[\"encoder_lengths\"].max()\n",
    "            y = torch.cat(\n",
    "                (\n",
    "                    y_all[: x[\"encoder_lengths\"][idx]],\n",
    "                    y_all[max_encoder_length : (max_encoder_length + x[\"decoder_lengths\"][idx])],\n",
    "                ),\n",
    "            )\n",
    "            # move predictions to cpu\n",
    "            y_hat = y_hat.detach().cpu()[idx, : x[\"decoder_lengths\"][idx]]\n",
    "            y_quantile = y_quantile.detach().cpu()[idx, : x[\"decoder_lengths\"][idx]]\n",
    "            y_raw = y_raw.detach().cpu()[idx, : x[\"decoder_lengths\"][idx]]\n",
    "\n",
    "            # move to cpu\n",
    "            y = y.detach().cpu()\n",
    "            # create figure\n",
    "            if ax is None:\n",
    "                fig, ax = plt.subplots()\n",
    "            else:\n",
    "                fig = ax.get_figure()\n",
    "            n_pred = y_hat.shape[0]\n",
    "            x_obs = np.arange(-(y.shape[0] - n_pred), 0)\n",
    "            x_pred = np.arange(n_pred)\n",
    "            prop_cycle = iter(plt.rcParams[\"axes.prop_cycle\"])\n",
    "            obs_color = next(prop_cycle)[\"color\"]\n",
    "            pred_color = next(prop_cycle)[\"color\"]\n",
    "            # plot observed history\n",
    "            if len(x_obs) > 0:\n",
    "                if len(x_obs) > 1:\n",
    "                    plotter = ax.plot\n",
    "                else:\n",
    "                    plotter = ax.scatter\n",
    "                plotter(x_obs, y[:-n_pred], label=\"observed\", c=obs_color)\n",
    "            if len(x_pred) > 1:\n",
    "                plotter = ax.plot\n",
    "            else:\n",
    "                plotter = ax.scatter\n",
    "\n",
    "            # plot observed prediction\n",
    "            if show_future_observed:\n",
    "                plotter(x_pred, y[-n_pred:], label=None, c=obs_color)\n",
    "\n",
    "            # plot prediction\n",
    "            plotter(x_pred, y_hat, label=\"predicted\", c=pred_color)\n",
    "\n",
    "            # plot predicted quantiles\n",
    "            plotter(x_pred, y_quantile[:, y_quantile.shape[1] // 2], c=pred_color, alpha=0.15)\n",
    "            for i in range(y_quantile.shape[1] // 2):\n",
    "                if len(x_pred) > 1:\n",
    "                    ax.fill_between(x_pred, y_quantile[:, i], y_quantile[:, -i - 1], alpha=0.15, fc=pred_color)\n",
    "                else:\n",
    "                    quantiles = torch.tensor([[y_quantile[0, i]], [y_quantile[0, -i - 1]]])\n",
    "                    ax.errorbar(\n",
    "                        x_pred,\n",
    "                        y[[-n_pred]],\n",
    "                        yerr=quantiles - y[-n_pred],\n",
    "                        c=pred_color,\n",
    "                        capsize=1.0,\n",
    "                    )\n",
    "\n",
    "            if add_loss_to_title is not False:\n",
    "                if isinstance(add_loss_to_title, bool):\n",
    "                    loss = self.loss\n",
    "                elif isinstance(add_loss_to_title, torch.Tensor):\n",
    "                    loss = add_loss_to_title.detach()[idx].item()\n",
    "                elif isinstance(add_loss_to_title, Metric):\n",
    "                    loss = add_loss_to_title\n",
    "                else:\n",
    "                    raise ValueError(f\"add_loss_to_title '{add_loss_to_title}'' is unkown\")\n",
    "                if isinstance(loss, MASE):\n",
    "                    loss_value = loss(y_raw[None], (y[-n_pred:][None], None), y[:n_pred][None])\n",
    "                elif isinstance(loss, Metric):\n",
    "                    try:\n",
    "                        loss_value = loss(y_raw[None], (y[-n_pred:][None], None))\n",
    "                    except Exception:\n",
    "                        loss_value = \"-\"\n",
    "                else:\n",
    "                    loss_value = loss\n",
    "                ax.set_title(f\"Loss {loss_value}\")\n",
    "            ax.set_xlabel(\"Time index\")\n",
    "            fig.legend()\n",
    "            figs.append(fig)\n",
    "\n",
    "        # return multiple of target is a list, otherwise return single figure\n",
    "        if isinstance(x[\"encoder_target\"], (tuple, list)):\n",
    "            return figs\n",
    "        else:\n",
    "            return fig\n",
    "\n",
    "\n",
    "    def log_gradient_flow(self, named_parameters: Dict[str, torch.Tensor]) -> None:\n",
    "        \"\"\"\n",
    "        log distribution of gradients to identify exploding / vanishing gradients\n",
    "        \"\"\"\n",
    "        ave_grads = []\n",
    "        layers = []\n",
    "        for name, p in named_parameters:\n",
    "            if p.grad is not None and p.requires_grad and \"bias\" not in name:\n",
    "                layers.append(name)\n",
    "                ave_grads.append(p.grad.abs().cpu().mean())\n",
    "                self.logger.experiment.add_histogram(tag=name, values=p.grad, global_step=self.global_step)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(ave_grads)\n",
    "        ax.set_xlabel(\"Layers\")\n",
    "        ax.set_ylabel(\"Average gradient\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_title(\"Gradient flow\")\n",
    "        self.logger.experiment.add_figure(\"Gradient flow\", fig, global_step=self.global_step)\n",
    "\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        \"\"\"\n",
    "        Log gradient flow for debugging.\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.hparams.log_interval > 0\n",
    "            and self.global_step % self.hparams.log_interval == 0\n",
    "            and self.hparams.log_gradient_flow\n",
    "        ):\n",
    "            self.log_gradient_flow(self.named_parameters())\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers.\n",
    "\n",
    "        Uses single Ranger optimizer. Depending if learning rate is a list or a single float, implement dynamic\n",
    "        learning rate scheduler or deterministic version\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List]: first entry is list of optimizers and second is list of schedulers\n",
    "        \"\"\"\n",
    "        # either set a schedule of lrs or find it dynamically\n",
    "        if self.hparams.optimizer_params is None:\n",
    "            optimizer_params = {}\n",
    "        else:\n",
    "            optimizer_params = self.hparams.optimizer_params\n",
    "        # set optimizer\n",
    "        lrs = self.hparams.learning_rate\n",
    "        if isinstance(lrs, (list, tuple)):\n",
    "            lr = lrs[0]\n",
    "        else:\n",
    "            lr = lrs\n",
    "        if callable(self.optimizer):\n",
    "            try:\n",
    "                optimizer = self.optimizer(\n",
    "                    self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "                )\n",
    "            except TypeError:  # in case there is no weight decay\n",
    "                optimizer = self.optimizer(self.parameters(), lr=lr, **optimizer_params)\n",
    "        elif self.hparams.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "            )\n",
    "        elif self.hparams.optimizer == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "            )\n",
    "        elif self.hparams.optimizer == \"ranger\":\n",
    "            if any([isinstance(c, LearningRateFinder) for c in self.trainer.callbacks]):\n",
    "                # if finding learning rate, switch off warm up and cool down\n",
    "                optimizer_params.setdefault(\"num_warm_up_iterations\", 0)\n",
    "                optimizer_params.setdefault(\"num_warm_down_iterations\", 0)\n",
    "                optimizer_params.setdefault(\"lookahead_merge_time\", 1e6)\n",
    "                optimizer_params.setdefault(\"num_iterations\", 100)\n",
    "            elif self.trainer.limit_train_batches is not None:\n",
    "                # if finding limiting train batches, set iterations to it\n",
    "                optimizer_params.setdefault(\n",
    "                    \"num_iterations\", min(self.trainer.num_training_batches, self.trainer.limit_train_batches)\n",
    "                )\n",
    "            else:\n",
    "                # if finding not limiting train batches, set iterations to dataloader length\n",
    "                optimizer_params.setdefault(\"num_iterations\", self.trainer.num_training_batches)\n",
    "            optimizer = Ranger21(self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params)\n",
    "        elif self.hparams.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "            )\n",
    "        elif hasattr(torch.optim, self.hparams.optimizer):\n",
    "            try:\n",
    "                optimizer = getattr(torch.optim, self.hparams.optimizer)(\n",
    "                    self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "                )\n",
    "            except TypeError:  # in case there is no weight decay\n",
    "                optimizer = getattr(torch.optim, self.hparams.optimizer)(self.parameters(), lr=lr, **optimizer_params)\n",
    "        elif hasattr(pytorch_optimizer, self.hparams.optimizer):\n",
    "            try:\n",
    "                optimizer = getattr(pytorch_optimizer, self.hparams.optimizer)(\n",
    "                    self.parameters(), lr=lr, weight_decay=self.hparams.weight_decay, **optimizer_params\n",
    "                )\n",
    "            except TypeError:  # in case there is no weight decay\n",
    "                optimizer = getattr(pytorch_optimizer, self.hparams.optimizer)(\n",
    "                    self.parameters(), lr=lr, **optimizer_params\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer of self.hparams.optimizer={self.hparams.optimizer} unknown\")\n",
    "\n",
    "        # set scheduler\n",
    "        if isinstance(lrs, (list, tuple)):  # change for each epoch\n",
    "            # normalize lrs\n",
    "            lrs = np.array(lrs) / lrs[0]\n",
    "            scheduler_config = {\n",
    "                \"scheduler\": LambdaLR(optimizer, lambda epoch: lrs[min(epoch, len(lrs) - 1)]),\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"strict\": False,\n",
    "            }\n",
    "        elif self.hparams.reduce_on_plateau_patience is None:\n",
    "            scheduler_config = {}\n",
    "        else:  # find schedule based on validation loss\n",
    "            scheduler_config = {\n",
    "                \"scheduler\": ReduceLROnPlateau(\n",
    "                    optimizer,\n",
    "                    mode=\"min\",\n",
    "                    factor=1.0 / self.hparams.reduce_on_plateau_reduction,\n",
    "                    patience=self.hparams.reduce_on_plateau_patience,\n",
    "                    cooldown=self.hparams.reduce_on_plateau_patience,\n",
    "                    min_lr=self.hparams.reduce_on_plateau_min_lr,\n",
    "                ),\n",
    "                \"monitor\": \"val_loss\",  # Default: val_loss\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"strict\": False,\n",
    "            }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs) -> LightningModule:\n",
    "        \"\"\"\n",
    "        Create model from dataset, i.e. save dataset parameters in model\n",
    "\n",
    "        This function should be called as ``super().from_dataset()`` in a derived models that implement it\n",
    "\n",
    "        Args:\n",
    "            dataset (TimeSeriesDataSet): timeseries dataset\n",
    "\n",
    "        Returns:\n",
    "            BaseModel: Model that can be trained\n",
    "        \"\"\"\n",
    "        if \"output_transformer\" not in kwargs:\n",
    "            kwargs[\"output_transformer\"] = dataset.target_normalizer\n",
    "        if \"dataset_parameters\" not in kwargs:\n",
    "            kwargs[\"dataset_parameters\"] = dataset.get_parameters()\n",
    "        net = cls(**kwargs)\n",
    "        if dataset.multi_target:\n",
    "            assert isinstance(\n",
    "                net.loss, MultiLoss\n",
    "            ), f\"multiple targets require loss to be MultiLoss but found {net.loss}\"\n",
    "        else:\n",
    "            assert not isinstance(net.loss, MultiLoss), \"MultiLoss not compatible with single target\"\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n",
    "        checkpoint[\"dataset_parameters\"] = getattr(\n",
    "            self, \"dataset_parameters\", None\n",
    "        )  # add dataset parameters for making fast predictions\n",
    "        # hyper parameters are passed as arguments directly and not as single dictionary\n",
    "        checkpoint[\"hparams_name\"] = \"kwargs\"\n",
    "        # save specials\n",
    "        checkpoint[self.CHECKPOINT_HYPER_PARAMS_SPECIAL_KEY] = {k: getattr(self, k) for k in self.hparams_special}\n",
    "        # add special hparams them back to save the hparams correctly for checkpoint\n",
    "        checkpoint[self.CHECKPOINT_HYPER_PARAMS_KEY].update(checkpoint[self.CHECKPOINT_HYPER_PARAMS_SPECIAL_KEY])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def target_names(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List of targets that are predicted.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: list of target names\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"dataset_parameters\") and self.dataset_parameters is not None:\n",
    "            return to_list(self.dataset_parameters[\"target\"])\n",
    "        else:\n",
    "            return [f\"Target {idx + 1}\" for idx in range(self.n_targets)]\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n",
    "        self.dataset_parameters = checkpoint.get(\"dataset_parameters\", None)\n",
    "        # load specials\n",
    "        for k, v in checkpoint[self.CHECKPOINT_HYPER_PARAMS_SPECIAL_KEY].items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "\n",
    "    def to_prediction(self, out: Dict[str, Any], use_metric: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Convert output to prediction using the loss metric.\n",
    "\n",
    "        Args:\n",
    "            out (Dict[str, Any]): output of network where \"prediction\" has been\n",
    "                transformed with :py:meth:`~transform_output`\n",
    "            use_metric (bool): if to use metric to convert for conversion, if False,\n",
    "                simply take the average over ``out[\"prediction\"]``\n",
    "            **kwargs: arguments to metric ``to_quantiles`` method\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: predictions of shape batch_size x timesteps\n",
    "        \"\"\"\n",
    "        if not use_metric:\n",
    "            # if samples were already drawn directly take mean\n",
    "            # todo: support classification\n",
    "            if isinstance(self.loss, MultiLoss):\n",
    "                out = [Metric.to_prediction(loss, out[\"prediction\"][idx]) for idx, loss in enumerate(self.loss)]\n",
    "            else:\n",
    "                out = Metric.to_prediction(self.loss, out[\"prediction\"])\n",
    "        else:\n",
    "            try:\n",
    "                out = self.loss.to_prediction(out[\"prediction\"], **kwargs)\n",
    "            except TypeError:  # in case passed kwargs do not exist\n",
    "                out = self.loss.to_prediction(out[\"prediction\"])\n",
    "        return out\n",
    "\n",
    "\n",
    "    def to_quantiles(self, out: Dict[str, Any], use_metric: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Convert output to quantiles using the loss metric.\n",
    "\n",
    "        Args:\n",
    "            out (Dict[str, Any]): output of network where \"prediction\" has been\n",
    "                transformed with :py:meth:`~transform_output`\n",
    "            use_metric (bool): if to use metric to convert for conversion, if False,\n",
    "                simply take the quantiles over ``out[\"prediction\"]``\n",
    "            **kwargs: arguments to metric ``to_quantiles`` method\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: quantiles of shape batch_size x timesteps x n_quantiles\n",
    "        \"\"\"\n",
    "        # if samples are output directly take quantiles\n",
    "        if not use_metric:\n",
    "            # todo: support classification\n",
    "            if isinstance(self.loss, MultiLoss):\n",
    "                out = [\n",
    "                    Metric.to_quantiles(loss, out[\"prediction\"][idx], quantiles=kwargs.get(\"quantiles\", loss.quantiles))\n",
    "                    for idx, loss in enumerate(self.loss)\n",
    "                ]\n",
    "            else:\n",
    "                out = Metric.to_quantiles(\n",
    "                    self.loss, out[\"prediction\"], quantiles=kwargs.get(\"quantiles\", self.loss.quantiles)\n",
    "                )\n",
    "        else:\n",
    "            try:\n",
    "                out = self.loss.to_quantiles(out[\"prediction\"], **kwargs)\n",
    "            except TypeError:  # in case passed kwargs do not exist\n",
    "                out = self.loss.to_quantiles(out[\"prediction\"])\n",
    "        return out\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        data: Union[DataLoader, pd.DataFrame, TimeSeriesDataSet],\n",
    "        mode: Union[str, Tuple[str, str]] = \"prediction\",\n",
    "        return_index: bool = False,\n",
    "        return_decoder_lengths: bool = False,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        fast_dev_run: bool = False,\n",
    "        return_x: bool = False,\n",
    "        return_y: bool = False,\n",
    "        mode_kwargs: Dict[str, Any] = None,\n",
    "        trainer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        write_interval: Literal[\"batch\", \"epoch\", \"batch_and_epoch\"] = \"batch\",\n",
    "        output_dir: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> Prediction:\n",
    "        \"\"\"\n",
    "        Run inference / prediction.\n",
    "\n",
    "        Args:\n",
    "            dataloader: dataloader, dataframe or dataset\n",
    "            mode: one of \"prediction\", \"quantiles\", or \"raw\", or tuple ``(\"raw\", output_name)`` where output_name is\n",
    "                a name in the dictionary returned by ``forward()``\n",
    "            return_index: if to return the prediction index (in the same order as the output, i.e. the row of the\n",
    "                dataframe corresponds to the first dimension of the output and the given time index is the time index\n",
    "                of the first prediction)\n",
    "            return_decoder_lengths: if to return decoder_lengths (in the same order as the output\n",
    "            batch_size: batch size for dataloader - only used if data is not a dataloader is passed\n",
    "            num_workers: number of workers for dataloader - only used if data is not a dataloader is passed\n",
    "            fast_dev_run: if to only return results of first batch\n",
    "            return_x: if to return network inputs (in the same order as prediction output)\n",
    "            return_y: if to return network targets (in the same order as prediction output)\n",
    "            mode_kwargs (Dict[str, Any]): keyword arguments for ``to_prediction()`` or ``to_quantiles()``\n",
    "                for modes \"prediction\" and \"quantiles\"\n",
    "            trainer_kwargs (Dict[str, Any], optional): keyword arguments for the trainer\n",
    "            write_interval: interval to write predictions to disk\n",
    "            output_dir: directory to write predictions to. Defaults to None. If set function will return empty list\n",
    "            **kwargs: additional arguments to network's forward method\n",
    "\n",
    "        Returns:\n",
    "            Prediction: if one of the ```return`` arguments is present,\n",
    "                prediction tuple with fields ``prediction``, ``x``, ``y``, ``index`` and ``decoder_lengths``\n",
    "        \"\"\"\n",
    "        # convert to dataloader\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data = TimeSeriesDataSet.from_parameters(self.dataset_parameters, data, predict=True)\n",
    "        if isinstance(data, TimeSeriesDataSet):\n",
    "            dataloader = data.to_dataloader(batch_size=batch_size, train=False, num_workers=num_workers)\n",
    "        else:\n",
    "            dataloader = data\n",
    "\n",
    "        # mode kwargs default to None\n",
    "        if mode_kwargs is None:\n",
    "            mode_kwargs = {}\n",
    "\n",
    "        # ensure passed dataloader is correct\n",
    "        assert isinstance(dataloader.dataset, TimeSeriesDataSet), \"dataset behind dataloader mut be TimeSeriesDataSet\"\n",
    "\n",
    "        predict_callback = PredictCallback(\n",
    "            mode=mode,\n",
    "            return_index=return_index,\n",
    "            return_decoder_lengths=return_decoder_lengths,\n",
    "            write_interval=write_interval,\n",
    "            return_x=return_x,\n",
    "            mode_kwargs=mode_kwargs,\n",
    "            output_dir=output_dir,\n",
    "            predict_kwargs=kwargs,\n",
    "            return_y=return_y,\n",
    "        )\n",
    "        if trainer_kwargs is None:\n",
    "            trainer_kwargs = {}\n",
    "        trainer_kwargs.setdefault(\"callbacks\", trainer_kwargs.get(\"callbacks\", []) + [predict_callback])\n",
    "        trainer_kwargs.setdefault(\"enable_progress_bar\", False)\n",
    "        trainer_kwargs.setdefault(\"inference_mode\", False)\n",
    "        assert (\n",
    "            \"fast_dev_run\" not in trainer_kwargs\n",
    "        ), \"fast_dev_run should be passed as argument to predict and not in trainer_kwargs\"\n",
    "        log_level_lighting = logging.getLogger(\"lightning\").getEffectiveLevel()\n",
    "        log_level_pytorch_lightning = logging.getLogger(\"pytorch_lightning\").getEffectiveLevel()\n",
    "        logging.getLogger(\"lightning\").setLevel(logging.WARNING)\n",
    "        logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
    "        trainer = Trainer(fast_dev_run=fast_dev_run, **trainer_kwargs)\n",
    "        trainer.predict(self, dataloader)\n",
    "        logging.getLogger(\"lightning\").setLevel(log_level_lighting)\n",
    "        logging.getLogger(\"pytorch_lightning\").setLevel(log_level_pytorch_lightning)\n",
    "\n",
    "        return predict_callback.result\n",
    "\n",
    "\n",
    "    def predict_dependency(\n",
    "        self,\n",
    "        data: Union[DataLoader, pd.DataFrame, TimeSeriesDataSet],\n",
    "        variable: str,\n",
    "        values: Iterable,\n",
    "        mode: str = \"dataframe\",\n",
    "        target=\"decoder\",\n",
    "        show_progress_bar: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> Union[np.ndarray, torch.Tensor, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Predict partial dependency.\n",
    "\n",
    "        Args:\n",
    "            data (Union[DataLoader, pd.DataFrame, TimeSeriesDataSet]): data\n",
    "            variable (str): variable which to modify\n",
    "            values (Iterable): array of values to probe\n",
    "            mode (str, optional): Output mode. Defaults to \"dataframe\". Either\n",
    "\n",
    "                * \"series\": values are average prediction and index are probed values\n",
    "                * \"dataframe\": columns are as obtained by the `dataset.x_to_index()` method,\n",
    "                    prediction (which is the mean prediction over the time horizon),\n",
    "                    normalized_prediction (which are predictions devided by the prediction for the first probed value)\n",
    "                    the variable name for the probed values\n",
    "                * \"raw\": outputs a tensor of shape len(values) x prediction_shape\n",
    "\n",
    "            target: Defines which values are overwritten for making a prediction.\n",
    "                Same as in :py:meth:`~pytorch_forecasting.data.timeseries.TimeSeriesDataSet.set_overwrite_values`.\n",
    "                Defaults to \"decoder\".\n",
    "            show_progress_bar: if to show progress bar. Defaults to False.\n",
    "            **kwargs: additional kwargs to :py:meth:`~predict` method\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, torch.Tensor, pd.Series, pd.DataFrame]: output\n",
    "        \"\"\"\n",
    "        values = np.asarray(values)\n",
    "        if isinstance(data, pd.DataFrame):  # convert to dataframe\n",
    "            data = TimeSeriesDataSet.from_parameters(self.dataset_parameters, data, predict=True)\n",
    "        elif isinstance(data, DataLoader):\n",
    "            data = data.dataset\n",
    "\n",
    "        results = []\n",
    "        progress_bar = tqdm(desc=\"Predict\", unit=\" batches\", total=len(values), disable=not show_progress_bar)\n",
    "        for idx, value in enumerate(values):\n",
    "            # set values\n",
    "            data.set_overwrite_values(variable=variable, values=value, target=target)\n",
    "            # predict\n",
    "            pred_kwargs = deepcopy(kwargs)\n",
    "            pred_kwargs.setdefault(\"mode\", \"prediction\")\n",
    "\n",
    "            if idx == 0 and mode == \"dataframe\":  # need index for returning as dataframe\n",
    "                res = self.predict(data, return_index=True, **pred_kwargs)\n",
    "                results.append(res.output)\n",
    "            else:\n",
    "                results.append(self.predict(data, **pred_kwargs))\n",
    "            # increment progress\n",
    "            progress_bar.update()\n",
    "\n",
    "        data.reset_overwrite_values()  # reset overwrite values to avoid side-effect\n",
    "\n",
    "        # results to one tensor\n",
    "        results = torch.stack(results, dim=0)\n",
    "\n",
    "        # convert results to requested output format\n",
    "        if mode == \"series\":\n",
    "            results = results[:, ~torch.isnan(results[0])].mean(1)  # average samples and prediction horizon\n",
    "            results = pd.Series(results.cpu().numpy(), index=values)\n",
    "\n",
    "        elif mode == \"dataframe\":\n",
    "            # take mean over time\n",
    "            is_nan = torch.isnan(results)\n",
    "            results[is_nan] = 0\n",
    "            results = results.sum(-1) / (~is_nan).float().sum(-1)\n",
    "\n",
    "            # create dataframe\n",
    "            dependencies = (\n",
    "                res.index.iloc[np.tile(np.arange(len(res.index)), len(values))]\n",
    "                .reset_index(drop=True)\n",
    "                .assign(prediction=results.flatten().cpu().numpy())\n",
    "            )\n",
    "            dependencies[variable] = values.repeat(len(data))\n",
    "            first_prediction = dependencies.groupby(data.group_ids, observed=True).prediction.transform(\"first\")\n",
    "            dependencies[\"normalized_prediction\"] = dependencies[\"prediction\"] / first_prediction\n",
    "            dependencies[\"id\"] = dependencies.groupby(data.group_ids, observed=True).ngroup()\n",
    "            results = dependencies\n",
    "\n",
    "        elif mode == \"raw\":\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is unknown - see documentation for available modes\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "class BaseModelWithCovariates(BaseModel):\n",
    "    @property\n",
    "    def target_positions(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Positions of target variable(s) in covariates.\n",
    "\n",
    "        Returns:\n",
    "            torch.LongTensor: tensor of positions.\n",
    "        \"\"\"\n",
    "        # todo: expand for categorical targets\n",
    "        if \"target\" in self.hparams:\n",
    "            target = self.hparams.target\n",
    "        else:\n",
    "            target = self.dataset_parameters[\"target\"]\n",
    "        return torch.tensor(\n",
    "            [self.hparams.x_reals.index(name) for name in to_list(target)],\n",
    "            device=self.device,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def reals(self) -> List[str]:\n",
    "        \"\"\"List of all continuous variables in model\"\"\"\n",
    "        return list(\n",
    "            dict.fromkeys(\n",
    "                self.hparams.static_reals\n",
    "                + self.hparams.time_varying_reals_encoder\n",
    "                + self.hparams.time_varying_reals_decoder\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def categoricals(self) -> List[str]:\n",
    "        \"\"\"List of all categorical variables in model\"\"\"\n",
    "        return list(\n",
    "            dict.fromkeys(\n",
    "                self.hparams.static_categoricals\n",
    "                + self.hparams.time_varying_categoricals_encoder\n",
    "                + self.hparams.time_varying_categoricals_decoder\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def static_variables(self) -> List[str]:\n",
    "        \"\"\"List of all static variables in model\"\"\"\n",
    "        return self.hparams.static_categoricals + self.hparams.static_reals\n",
    "\n",
    "    @property\n",
    "    def encoder_variables(self) -> List[str]:\n",
    "        \"\"\"List of all encoder variables in model (excluding static variables)\"\"\"\n",
    "        return self.hparams.time_varying_categoricals_encoder + self.hparams.time_varying_reals_encoder\n",
    "\n",
    "    @property\n",
    "    def decoder_variables(self) -> List[str]:\n",
    "        \"\"\"List of all decoder variables in model (excluding static variables)\"\"\"\n",
    "        return self.hparams.time_varying_categoricals_decoder + self.hparams.time_varying_reals_decoder\n",
    "\n",
    "    @property\n",
    "    def categorical_groups_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"Mapping of categorical variables to categorical groups\"\"\"\n",
    "        groups = {}\n",
    "        for group_name, sublist in self.hparams.categorical_groups.items():\n",
    "            groups.update({name: group_name for name in sublist})\n",
    "        return groups\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: TimeSeriesDataSet,\n",
    "        allowed_encoder_known_variable_names: List[str] = None,\n",
    "        **kwargs,\n",
    "    ) -> LightningModule:\n",
    "        \"\"\"\n",
    "        Create model from dataset and set parameters related to covariates.\n",
    "\n",
    "        Args:\n",
    "            dataset: timeseries dataset\n",
    "            allowed_encoder_known_variable_names: List of known variables that are allowed in encoder, defaults to all\n",
    "            **kwargs: additional arguments such as hyperparameters for model (see ``__init__()``)\n",
    "\n",
    "        Returns:\n",
    "            LightningModule\n",
    "        \"\"\"\n",
    "        # assert fixed encoder and decoder length for the moment\n",
    "        if allowed_encoder_known_variable_names is None:\n",
    "            allowed_encoder_known_variable_names = (\n",
    "                dataset.time_varying_known_categoricals + dataset.time_varying_known_reals\n",
    "            )\n",
    "\n",
    "        # embeddings\n",
    "        embedding_labels = {\n",
    "            name: encoder.classes_\n",
    "            for name, encoder in dataset.categorical_encoders.items()\n",
    "            if name in dataset.categoricals\n",
    "        }\n",
    "        embedding_paddings = dataset.dropout_categoricals\n",
    "        # determine embedding sizes based on heuristic\n",
    "        embedding_sizes = {\n",
    "            name: (len(encoder.classes_), get_embedding_size(len(encoder.classes_)))\n",
    "            for name, encoder in dataset.categorical_encoders.items()\n",
    "            if name in dataset.categoricals\n",
    "        }\n",
    "        embedding_sizes.update(kwargs.get(\"embedding_sizes\", {}))\n",
    "        kwargs.setdefault(\"embedding_sizes\", embedding_sizes)\n",
    "\n",
    "        new_kwargs = dict(\n",
    "            static_categoricals=dataset.static_categoricals,\n",
    "            time_varying_categoricals_encoder=[\n",
    "                name for name in dataset.time_varying_known_categoricals if name in allowed_encoder_known_variable_names\n",
    "            ]\n",
    "            + dataset.time_varying_unknown_categoricals,\n",
    "            time_varying_categoricals_decoder=dataset.time_varying_known_categoricals,\n",
    "            static_reals=dataset.static_reals,\n",
    "            time_varying_reals_encoder=[\n",
    "                name for name in dataset.time_varying_known_reals if name in allowed_encoder_known_variable_names\n",
    "            ]\n",
    "            + dataset.time_varying_unknown_reals,\n",
    "            time_varying_reals_decoder=dataset.time_varying_known_reals,\n",
    "            x_reals=dataset.reals,\n",
    "            x_categoricals=dataset.flat_categoricals,\n",
    "            embedding_labels=embedding_labels,\n",
    "            embedding_paddings=embedding_paddings,\n",
    "            categorical_groups=dataset.variable_groups,\n",
    "        )\n",
    "        new_kwargs.update(kwargs)\n",
    "        return super().from_dataset(dataset, **new_kwargs)\n",
    "\n",
    "\n",
    "    def extract_features(\n",
    "        self,\n",
    "        x,\n",
    "        embeddings: MultiEmbedding = None,\n",
    "        period: str = \"all\",\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract features\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): input from the dataloader\n",
    "            embeddings (MultiEmbedding): embeddings for categorical variables\n",
    "            period (str, optional): One of \"encoder\", \"decoder\" or \"all\". Defaults to \"all\".\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: tensor with selected variables\n",
    "        \"\"\"\n",
    "        # select period\n",
    "        if period == \"encoder\":\n",
    "            x_cat = x[\"encoder_cat\"]\n",
    "            x_cont = x[\"encoder_cont\"]\n",
    "        elif period == \"decoder\":\n",
    "            x_cat = x[\"decoder_cat\"]\n",
    "            x_cont = x[\"decoder_cont\"]\n",
    "        elif period == \"all\":\n",
    "            x_cat = torch.cat([x[\"encoder_cat\"], x[\"decoder_cat\"]], dim=1)  # concatenate in time dimension\n",
    "            x_cont = torch.cat([x[\"encoder_cont\"], x[\"decoder_cont\"]], dim=1)  # concatenate in time dimension\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown type: {type}\")\n",
    "\n",
    "        # create dictionary of encoded vectors\n",
    "        input_vectors = embeddings(x_cat)\n",
    "        input_vectors.update(\n",
    "            {\n",
    "                name: x_cont[..., idx].unsqueeze(-1)\n",
    "                for idx, name in enumerate(self.hparams.x_reals)\n",
    "                if name in self.reals\n",
    "            }\n",
    "        )\n",
    "        return input_vectors\n",
    "\n",
    "\n",
    "    def calculate_prediction_actual_by_variable(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        y_pred: torch.Tensor,\n",
    "        normalize: bool = True,\n",
    "        bins: int = 95,\n",
    "        std: float = 2.0,\n",
    "        log_scale: bool = None,\n",
    "    ) -> Dict[str, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Calculate predictions and actuals by variable averaged by ``bins`` bins spanning from ``-std`` to ``+std``\n",
    "\n",
    "        Args:\n",
    "            x: input as ``forward()``\n",
    "            y_pred: predictions obtained by ``self(x, **kwargs)``\n",
    "            normalize: if to return normalized averages, i.e. mean or sum of ``y``\n",
    "            bins: number of bins to calculate\n",
    "            std: number of standard deviations for standard scaled continuous variables\n",
    "            log_scale (str, optional): if to plot in log space. If None, determined based on skew of values.\n",
    "                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dictionary that can be used to plot averages with :py:meth:`~plot_prediction_actual_by_variable`\n",
    "        \"\"\"\n",
    "        support = {}  # histogram\n",
    "        # averages\n",
    "        averages_actual = {}\n",
    "        averages_prediction = {}\n",
    "\n",
    "        # mask values and transform to log space\n",
    "        max_encoder_length = x[\"decoder_lengths\"].max()\n",
    "        mask = create_mask(max_encoder_length, x[\"decoder_lengths\"], inverse=True)\n",
    "        # select valid y values\n",
    "        y_flat = x[\"decoder_target\"][mask]\n",
    "        y_pred_flat = y_pred[mask]\n",
    "\n",
    "        # determine in which average in log-space to transform data\n",
    "        if log_scale is None:\n",
    "            skew = torch.mean(((y_flat - torch.mean(y_flat)) / torch.std(y_flat)) ** 3)\n",
    "            log_scale = skew > 1.6\n",
    "\n",
    "        if log_scale:\n",
    "            y_flat = torch.log(y_flat + 1e-8)\n",
    "            y_pred_flat = torch.log(y_pred_flat + 1e-8)\n",
    "\n",
    "        # real bins\n",
    "        positive_bins = (bins - 1) // 2\n",
    "\n",
    "        # if to normalize\n",
    "        if normalize:\n",
    "            reduction = \"mean\"\n",
    "        else:\n",
    "            reduction = \"sum\"\n",
    "        # continuous variables\n",
    "        reals = x[\"decoder_cont\"]\n",
    "        for idx, name in enumerate(self.hparams.x_reals):\n",
    "            averages_actual[name], support[name] = groupby_apply(\n",
    "                (reals[..., idx][mask] * positive_bins / std).round().clamp(-positive_bins, positive_bins).long()\n",
    "                + positive_bins,\n",
    "                y_flat,\n",
    "                bins=bins,\n",
    "                reduction=reduction,\n",
    "                return_histogram=True,\n",
    "            )\n",
    "            averages_prediction[name], _ = groupby_apply(\n",
    "                (reals[..., idx][mask] * positive_bins / std).round().clamp(-positive_bins, positive_bins).long()\n",
    "                + positive_bins,\n",
    "                y_pred_flat,\n",
    "                bins=bins,\n",
    "                reduction=reduction,\n",
    "                return_histogram=True,\n",
    "            )\n",
    "\n",
    "        # categorical_variables\n",
    "        cats = x[\"decoder_cat\"]\n",
    "        for idx, name in enumerate(self.hparams.x_categoricals):  # todo: make it work for grouped categoricals\n",
    "            reduction = \"sum\"\n",
    "            name = self.categorical_groups_mapping.get(name, name)\n",
    "            averages_actual_cat, support_cat = groupby_apply(\n",
    "                cats[..., idx][mask],\n",
    "                y_flat,\n",
    "                bins=self.hparams.embedding_sizes[name][0],\n",
    "                reduction=reduction,\n",
    "                return_histogram=True,\n",
    "            )\n",
    "            averages_prediction_cat, _ = groupby_apply(\n",
    "                cats[..., idx][mask],\n",
    "                y_pred_flat,\n",
    "                bins=self.hparams.embedding_sizes[name][0],\n",
    "                reduction=reduction,\n",
    "                return_histogram=True,\n",
    "            )\n",
    "\n",
    "            # add either to existing calculations or\n",
    "            if name in averages_actual:\n",
    "                averages_actual[name] += averages_actual_cat\n",
    "                support[name] += support_cat\n",
    "                averages_prediction[name] += averages_prediction_cat\n",
    "            else:\n",
    "                averages_actual[name] = averages_actual_cat\n",
    "                support[name] = support_cat\n",
    "                averages_prediction[name] = averages_prediction_cat\n",
    "\n",
    "        if normalize:  # run reduction for categoricals\n",
    "            for name in self.hparams.embedding_sizes.keys():\n",
    "                averages_actual[name] /= support[name].clamp(min=1)\n",
    "                averages_prediction[name] /= support[name].clamp(min=1)\n",
    "\n",
    "        if log_scale:\n",
    "            for name in support.keys():\n",
    "                averages_actual[name] = torch.exp(averages_actual[name])\n",
    "                averages_prediction[name] = torch.exp(averages_prediction[name])\n",
    "\n",
    "        return {\n",
    "            \"support\": support,\n",
    "            \"average\": {\"actual\": averages_actual, \"prediction\": averages_prediction},\n",
    "            \"std\": std,\n",
    "        }\n",
    "\n",
    "\n",
    "    def plot_prediction_actual_by_variable(\n",
    "        self, data: Dict[str, Dict[str, torch.Tensor]], name: str = None, ax=None, log_scale: bool = None\n",
    "    ) -> Union[Dict[str, plt.Figure], plt.Figure]:\n",
    "        if name is None:  # run recursion for figures\n",
    "            figs = {name: self.plot_prediction_actual_by_variable(data, name) for name in data[\"support\"].keys()}\n",
    "            return figs\n",
    "        else:\n",
    "            # create figure\n",
    "            kwargs = {}\n",
    "            # adjust figure size for figures with many labels\n",
    "            if self.hparams.embedding_sizes.get(name, [1e9])[0] > 10:\n",
    "                kwargs = dict(figsize=(10, 5))\n",
    "            if ax is None:\n",
    "                fig, ax = plt.subplots(**kwargs)\n",
    "            else:\n",
    "                fig = ax.get_figure()\n",
    "            ax.set_title(f\"{name} averages\")\n",
    "            ax.set_xlabel(name)\n",
    "            ax.set_ylabel(\"Prediction\")\n",
    "\n",
    "            ax2 = ax.twinx()  # second axis for histogram\n",
    "            ax2.set_ylabel(\"Frequency\")\n",
    "\n",
    "            # get values for average plot and histogram\n",
    "            values_actual = data[\"average\"][\"actual\"][name].detach().cpu().numpy()\n",
    "            values_prediction = data[\"average\"][\"prediction\"][name].detach().cpu().numpy()\n",
    "            bins = values_actual.size\n",
    "            support = data[\"support\"][name].detach().cpu().numpy()\n",
    "\n",
    "            # only display values where samples were observed\n",
    "            support_non_zero = support > 0\n",
    "            support = support[support_non_zero]\n",
    "            values_actual = values_actual[support_non_zero]\n",
    "            values_prediction = values_prediction[support_non_zero]\n",
    "\n",
    "            # determine if to display results in log space\n",
    "            if log_scale is None:\n",
    "                log_scale = scipy.stats.skew(values_actual) > 1.6\n",
    "\n",
    "            if log_scale:\n",
    "                ax.set_yscale(\"log\")\n",
    "\n",
    "            # plot averages\n",
    "            if name in self.hparams.x_reals:\n",
    "                # create x\n",
    "                if name in to_list(self.dataset_parameters[\"target\"]):\n",
    "                    if isinstance(self.output_transformer, MultiNormalizer):\n",
    "                        scaler = self.output_transformer.normalizers[self.dataset_parameters[\"target\"].index(name)]\n",
    "                    else:\n",
    "                        scaler = self.output_transformer\n",
    "                else:\n",
    "                    scaler = self.dataset_parameters[\"scalers\"][name]\n",
    "                x = np.linspace(-data[\"std\"], data[\"std\"], bins)\n",
    "                # reversing normalization for group normalizer is not possible without sample level information\n",
    "                if not isinstance(scaler, (GroupNormalizer, EncoderNormalizer)):\n",
    "                    x = scaler.inverse_transform(x.reshape(-1, 1)).reshape(-1)\n",
    "                    ax.set_xlabel(f\"Normalized {name}\")\n",
    "\n",
    "                if len(x) > 0:\n",
    "                    x_step = x[1] - x[0]\n",
    "                else:\n",
    "                    x_step = 1\n",
    "                x = x[support_non_zero]\n",
    "                ax.plot(x, values_actual, label=\"Actual\")\n",
    "                ax.plot(x, values_prediction, label=\"Prediction\")\n",
    "\n",
    "            elif name in self.hparams.embedding_labels:\n",
    "                # sort values from lowest to highest\n",
    "                sorting = values_actual.argsort()\n",
    "                labels = np.asarray(list(self.hparams.embedding_labels[name].keys()))[support_non_zero][sorting]\n",
    "                values_actual = values_actual[sorting]\n",
    "                values_prediction = values_prediction[sorting]\n",
    "                support = support[sorting]\n",
    "                # cut entries if there are too many categories to fit nicely on the plot\n",
    "                maxsize = 50\n",
    "                if values_actual.size > maxsize:\n",
    "                    values_actual = np.concatenate([values_actual[: maxsize // 2], values_actual[-maxsize // 2 :]])\n",
    "                    values_prediction = np.concatenate(\n",
    "                        [values_prediction[: maxsize // 2], values_prediction[-maxsize // 2 :]]\n",
    "                    )\n",
    "                    labels = np.concatenate([labels[: maxsize // 2], labels[-maxsize // 2 :]])\n",
    "                    support = np.concatenate([support[: maxsize // 2], support[-maxsize // 2 :]])\n",
    "                # plot for each category\n",
    "                x = np.arange(values_actual.size)\n",
    "                x_step = 1\n",
    "                ax.scatter(x, values_actual, label=\"Actual\")\n",
    "                ax.scatter(x, values_prediction, label=\"Prediction\")\n",
    "                # set labels at x axis\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels(labels, rotation=90)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown name {name}\")\n",
    "            # plot support histogram\n",
    "            if len(support) > 1 and np.median(support) < support.max() / 10:\n",
    "                ax2.set_yscale(\"log\")\n",
    "            ax2.bar(x, support, width=x_step, linewidth=0, alpha=0.2, color=\"k\")\n",
    "            # adjust layout and legend\n",
    "            fig.tight_layout()\n",
    "            fig.legend()\n",
    "            return fig\n",
    "\n",
    "\n",
    "\n",
    "class AutoRegressiveBaseModel(BaseModel):\n",
    "    \"\"\"\n",
    "    Model with additional methods for autoregressive models.\n",
    "\n",
    "    Adds in particular the :py:meth:`~decode_autoregressive` method for making auto-regressive predictions.\n",
    "\n",
    "    Assumes the following hyperparameters:\n",
    "\n",
    "    Args:\n",
    "        target (str): name of target variable\n",
    "        target_lags (Dict[str, Dict[str, int]]): dictionary of target names mapped each to a dictionary of corresponding\n",
    "            lagged variables and their lags.\n",
    "            Lags can be useful to indicate seasonality to the models. If you know the seasonalit(ies) of your data,\n",
    "            add at least the target variables with the corresponding lags to improve performance.\n",
    "            Defaults to no lags, i.e. an empty dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: TimeSeriesDataSet,\n",
    "        **kwargs,\n",
    "    ) -> LightningModule:\n",
    "        \"\"\"\n",
    "        Create model from dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: timeseries dataset\n",
    "            **kwargs: additional arguments such as hyperparameters for model (see ``__init__()``)\n",
    "\n",
    "        Returns:\n",
    "            LightningModule\n",
    "        \"\"\"\n",
    "        kwargs.setdefault(\"target\", dataset.target)\n",
    "        # check that lags for targets are the same\n",
    "        lags = {name: lag for name, lag in dataset.lags.items() if name in dataset.target_names}  # filter for targets\n",
    "        target0 = dataset.target_names[0]\n",
    "        lag = set(lags.get(target0, []))\n",
    "        for target in dataset.target_names:\n",
    "            assert lag == set(lags.get(target, [])), f\"all target lags in dataset must be the same but found {lags}\"\n",
    "\n",
    "        kwargs.setdefault(\"target_lags\", {name: dataset._get_lagged_names(name) for name in lags})\n",
    "        return super().from_dataset(dataset, **kwargs)\n",
    "\n",
    "\n",
    "    def output_to_prediction(\n",
    "        self,\n",
    "        normalized_prediction_parameters: torch.Tensor,\n",
    "        target_scale: Union[List[torch.Tensor], torch.Tensor],\n",
    "        n_samples: int = 1,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Union[List[torch.Tensor], torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convert network output to rescaled and normalized prediction.\n",
    "\n",
    "        Function is typically not called directly but via :py:meth:`~decode_autoregressive`.\n",
    "\n",
    "        Args:\n",
    "            normalized_prediction_parameters (torch.Tensor): network prediction output\n",
    "            target_scale (Union[List[torch.Tensor], torch.Tensor]): target scale to rescale network output\n",
    "            n_samples (int, optional): Number of samples to draw independently. Defaults to 1.\n",
    "            **kwargs: extra arguments for dictionary passed to :py:meth:`~transform_output` method.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Union[List[torch.Tensor], torch.Tensor], torch.Tensor]: tuple of rescaled prediction and\n",
    "                normalized prediction (e.g. for input into next auto-regressive step)\n",
    "        \"\"\"\n",
    "        single_prediction = to_list(normalized_prediction_parameters)[0].ndim == 2\n",
    "        if single_prediction:  # add time dimension as it is expected\n",
    "            normalized_prediction_parameters = apply_to_list(normalized_prediction_parameters, lambda x: x.unsqueeze(1))\n",
    "        # transform into real space\n",
    "        prediction_parameters = self.transform_output(\n",
    "            prediction=normalized_prediction_parameters, target_scale=target_scale, **kwargs\n",
    "        )\n",
    "        # todo: handle classification\n",
    "        # sample value(s) from distribution and  select first sample\n",
    "        if isinstance(self.loss, DistributionLoss) or (\n",
    "            isinstance(self.loss, MultiLoss) and isinstance(self.loss[0], DistributionLoss)\n",
    "        ):\n",
    "            # todo: handle mixed losses\n",
    "            if n_samples > 1:\n",
    "                prediction_parameters = apply_to_list(\n",
    "                    prediction_parameters, lambda x: x.reshape(int(x.size(0) / n_samples), n_samples, -1)\n",
    "                )\n",
    "                prediction = self.loss.sample(prediction_parameters, 1)\n",
    "                prediction = apply_to_list(prediction, lambda x: x.reshape(x.size(0) * n_samples, 1, -1))\n",
    "            else:\n",
    "                prediction = self.loss.sample(normalized_prediction_parameters, 1)\n",
    "\n",
    "        else:\n",
    "            prediction = prediction_parameters\n",
    "        # normalize prediction prediction\n",
    "        normalized_prediction = self.output_transformer.transform(prediction, target_scale=target_scale)\n",
    "        if isinstance(normalized_prediction, list):\n",
    "            input_target = torch.cat(normalized_prediction, dim=-1)\n",
    "        else:\n",
    "            input_target = normalized_prediction  # set next input target to normalized prediction\n",
    "\n",
    "        # remove time dimension\n",
    "        if single_prediction:\n",
    "            prediction = apply_to_list(prediction, lambda x: x.squeeze(1))\n",
    "            input_target = input_target.squeeze(1)\n",
    "        return prediction, input_target\n",
    "\n",
    "\n",
    "    def decode_autoregressive(\n",
    "        self,\n",
    "        decode_one: Callable,\n",
    "        first_target: Union[List[torch.Tensor], torch.Tensor],\n",
    "        first_hidden_state: Any,\n",
    "        target_scale: Union[List[torch.Tensor], torch.Tensor],\n",
    "        n_decoder_steps: int,\n",
    "        n_samples: int = 1,\n",
    "        **kwargs,\n",
    "    ) -> Union[List[torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Make predictions in auto-regressive manner.\n",
    "\n",
    "        Supports only continuous targets.\n",
    "\n",
    "        Args:\n",
    "            decode_one (Callable): function that takes at least the following arguments:\n",
    "\n",
    "                * ``idx`` (int): index of decoding step (from 0 to n_decoder_steps-1)\n",
    "                * ``lagged_targets`` (List[torch.Tensor]): list of normalized targets.\n",
    "                  List is ``idx + 1`` elements long with the most recent entry at the end, i.e.\n",
    "                  ``previous_target = lagged_targets[-1]`` and in general ``lagged_targets[-lag]``.\n",
    "                * ``hidden_state`` (Any): Current hidden state required for prediction.\n",
    "                  Keys are variable names. Only lags that are greater than ``idx`` are included.\n",
    "                * additional arguments are not dynamic but can be passed via the ``**kwargs`` argument\n",
    "\n",
    "                And returns tuple of (not rescaled) network prediction output and hidden state for next\n",
    "                auto-regressive step.\n",
    "\n",
    "            first_target (Union[List[torch.Tensor], torch.Tensor]): first target value to use for decoding\n",
    "            first_hidden_state (Any): first hidden state used for decoding\n",
    "            target_scale (Union[List[torch.Tensor], torch.Tensor]): target scale as in ``x``\n",
    "            n_decoder_steps (int): number of decoding/prediction steps\n",
    "            n_samples (int): number of independent samples to draw from the distribution -\n",
    "                only relevant for multivariate models. Defaults to 1.\n",
    "            **kwargs: additional arguments that are passed to the decode_one function.\n",
    "\n",
    "        Returns:\n",
    "            Union[List[torch.Tensor], torch.Tensor]: re-scaled prediction\n",
    "\n",
    "        Example:\n",
    "\n",
    "            LSTM/GRU decoder\n",
    "\n",
    "            .. code-block:: python\n",
    "\n",
    "                def decode(self, x, hidden_state):\n",
    "                    # create input vector\n",
    "                    input_vector = x[\"decoder_cont\"].clone()\n",
    "                    input_vector[..., self.target_positions] = torch.roll(\n",
    "                        input_vector[..., self.target_positions],\n",
    "                        shifts=1,\n",
    "                        dims=1,\n",
    "                    )\n",
    "                    # but this time fill in missing target from encoder_cont at the first time step instead of\n",
    "                    # throwing it away\n",
    "                    last_encoder_target = x[\"encoder_cont\"][\n",
    "                        torch.arange(x[\"encoder_cont\"].size(0), device=x[\"encoder_cont\"].device),\n",
    "                        x[\"encoder_lengths\"] - 1,\n",
    "                        self.target_positions.unsqueeze(-1)\n",
    "                    ].T.contiguous()\n",
    "                    input_vector[:, 0, self.target_positions] = last_encoder_target\n",
    "\n",
    "                    if self.training:  # training mode\n",
    "                        decoder_output, _ = self.rnn(\n",
    "                            x,\n",
    "                            hidden_state,\n",
    "                            lengths=x[\"decoder_lengths\"],\n",
    "                            enforce_sorted=False,\n",
    "                        )\n",
    "\n",
    "                        # from hidden state size to outputs\n",
    "                        if isinstance(self.hparams.target, str):  # single target\n",
    "                            output = self.distribution_projector(decoder_output)\n",
    "                        else:\n",
    "                            output = [projector(decoder_output) for projector in self.distribution_projector]\n",
    "\n",
    "                        # predictions are not yet rescaled -> so rescale now\n",
    "                        return self.transform_output(output, target_scale=target_scale)\n",
    "\n",
    "                    else:  # prediction mode\n",
    "                        target_pos = self.target_positions\n",
    "\n",
    "                        def decode_one(idx, lagged_targets, hidden_state):\n",
    "                            x = input_vector[:, [idx]]\n",
    "                            x[:, 0, target_pos] = lagged_targets[-1]  # overwrite at target positions\n",
    "\n",
    "                            # overwrite at lagged targets positions\n",
    "                            for lag, lag_positions in lagged_target_positions.items():\n",
    "                                if idx > lag:  # only overwrite if target has been generated\n",
    "                                    x[:, 0, lag_positions] = lagged_targets[-lag]\n",
    "\n",
    "                            decoder_output, hidden_state = self.rnn(x, hidden_state)\n",
    "                            decoder_output = decoder_output[:, 0]  # take first timestep\n",
    "                            # from hidden state size to outputs\n",
    "                            if isinstance(self.hparams.target, str):  # single target\n",
    "                                output = self.distribution_projector(decoder_output)\n",
    "                            else:\n",
    "                                output = [projector(decoder_output) for projector in self.distribution_projector]\n",
    "                            return output, hidden_state\n",
    "\n",
    "                        # make predictions which are fed into next step\n",
    "                        output = self.decode_autoregressive(\n",
    "                            decode_one,\n",
    "                            first_target=input_vector[:, 0, target_pos],\n",
    "                            first_hidden_state=hidden_state,\n",
    "                            target_scale=x[\"target_scale\"],\n",
    "                            n_decoder_steps=input_vector.size(1),\n",
    "                        )\n",
    "\n",
    "                        # predictions are already rescaled\n",
    "                        return output\n",
    "\n",
    "        \"\"\"\n",
    "        # make predictions which are fed into next step\n",
    "        output = []\n",
    "        current_target = first_target\n",
    "        current_hidden_state = first_hidden_state\n",
    "\n",
    "        normalized_output = [first_target]\n",
    "\n",
    "        for idx in range(n_decoder_steps):\n",
    "            # get lagged targets\n",
    "            current_target, current_hidden_state = decode_one(\n",
    "                idx, lagged_targets=normalized_output, hidden_state=current_hidden_state, **kwargs\n",
    "            )\n",
    "\n",
    "            # get prediction and its normalized version for the next step\n",
    "            prediction, current_target = self.output_to_prediction(\n",
    "                current_target, target_scale=target_scale, n_samples=n_samples\n",
    "            )\n",
    "            # save normalized output for lagged targets\n",
    "            normalized_output.append(current_target)\n",
    "            # set output to unnormalized samples, append each target as n_batch_samples x n_random_samples\n",
    "\n",
    "            output.append(prediction)\n",
    "        if isinstance(self.hparams.target, str):\n",
    "            output = torch.stack(output, dim=1)\n",
    "        else:\n",
    "            # for multi-targets\n",
    "            output = [torch.stack([out[idx] for out in output], dim=1) for idx in range(len(self.target_positions))]\n",
    "        return output\n",
    "\n",
    "\n",
    "    @property\n",
    "    def target_positions(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Positions of target variable(s) in covariates.\n",
    "\n",
    "        Returns:\n",
    "            torch.LongTensor: tensor of positions.\n",
    "        \"\"\"\n",
    "        # todo: expand for categorical targets\n",
    "        return torch.tensor(\n",
    "            [0],\n",
    "            device=self.device,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "    def plot_prediction(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        idx: int = 0,\n",
    "        add_loss_to_title: Union[Metric, torch.Tensor, bool] = False,\n",
    "        show_future_observed: bool = True,\n",
    "        ax=None,\n",
    "        quantiles_kwargs: Dict[str, Any] = {},\n",
    "        prediction_kwargs: Dict[str, Any] = {},\n",
    "    ) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot prediction of prediction vs actuals\n",
    "\n",
    "        Args:\n",
    "            x: network input\n",
    "            out: network output\n",
    "            idx: index of prediction to plot\n",
    "            add_loss_to_title: if to add loss to title or loss function to calculate. Can be either metrics,\n",
    "                bool indicating if to use loss metric or tensor which contains losses for all samples.\n",
    "                Calcualted losses are determined without weights. Default to False.\n",
    "            show_future_observed: if to show actuals for future. Defaults to True.\n",
    "            ax: matplotlib axes to plot on\n",
    "            quantiles_kwargs (Dict[str, Any]): parameters for ``to_quantiles()`` of the loss metric.\n",
    "            prediction_kwargs (Dict[str, Any]): parameters for ``to_prediction()`` of the loss metric.\n",
    "\n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "\n",
    "        # get predictions\n",
    "        if isinstance(self.loss, DistributionLoss):\n",
    "            prediction_kwargs.setdefault(\"use_metric\", False)\n",
    "            quantiles_kwargs.setdefault(\"use_metric\", False)\n",
    "\n",
    "        return super().plot_prediction(\n",
    "            x=x,\n",
    "            out=out,\n",
    "            idx=idx,\n",
    "            add_loss_to_title=add_loss_to_title,\n",
    "            show_future_observed=show_future_observed,\n",
    "            ax=ax,\n",
    "            quantiles_kwargs=quantiles_kwargs,\n",
    "            prediction_kwargs=prediction_kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "    @property\n",
    "    def lagged_target_positions(self) -> Dict[int, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Positions of lagged target variable(s) in covariates.\n",
    "\n",
    "        Returns:\n",
    "            Dict[int, torch.LongTensor]: dictionary mapping integer lags to tensor of variable positions.\n",
    "        \"\"\"\n",
    "        raise Exception(\n",
    "            \"lagged targets can only be used with class inheriting \"\n",
    "            \"from AutoRegressiveBaseModelWithCovariates but not from AutoRegressiveBaseModel\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class AutoRegressiveBaseModelWithCovariates(BaseModelWithCovariates, AutoRegressiveBaseModel):\n",
    "    \"\"\"\n",
    "    Model with additional methods for autoregressive models with covariates.\n",
    "\n",
    "    Assumes the following hyperparameters:\n",
    "\n",
    "    Args:\n",
    "        target (str): name of target variable\n",
    "        target_lags (Dict[str, Dict[str, int]]): dictionary of target names mapped each to a dictionary of corresponding\n",
    "            lagged variables and their lags.\n",
    "            Lags can be useful to indicate seasonality to the models. If you know the seasonalit(ies) of your data,\n",
    "            add at least the target variables with the corresponding lags to improve performance.\n",
    "            Defaults to no lags, i.e. an empty dictionary.\n",
    "        static_categoricals (List[str]): names of static categorical variables\n",
    "        static_reals (List[str]): names of static continuous variables\n",
    "        time_varying_categoricals_encoder (List[str]): names of categorical variables for encoder\n",
    "        time_varying_categoricals_decoder (List[str]): names of categorical variables for decoder\n",
    "        time_varying_reals_encoder (List[str]): names of continuous variables for encoder\n",
    "        time_varying_reals_decoder (List[str]): names of continuous variables for decoder\n",
    "        x_reals (List[str]): order of continuous variables in tensor passed to forward function\n",
    "        x_categoricals (List[str]): order of categorical variables in tensor passed to forward function\n",
    "        embedding_sizes (Dict[str, Tuple[int, int]]): dictionary mapping categorical variables to tuple of integers\n",
    "            where the first integer denotes the number of categorical classes and the second the embedding size\n",
    "        embedding_labels (Dict[str, List[str]]): dictionary mapping (string) indices to list of categorical labels\n",
    "        embedding_paddings (List[str]): names of categorical variables for which label 0 is always mapped to an\n",
    "             embedding vector filled with zeros\n",
    "        categorical_groups (Dict[str, List[str]]): dictionary of categorical variables that are grouped together and\n",
    "            can also take multiple values simultaneously (e.g. holiday during octoberfest). They should be implemented\n",
    "            as bag of embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def lagged_target_positions(self) -> Dict[int, torch.LongTensor]:\n",
    "        \"\"\"\n",
    "        Positions of lagged target variable(s) in covariates.\n",
    "\n",
    "        Returns:\n",
    "            Dict[int, torch.LongTensor]: dictionary mapping integer lags to tensor of variable positions.\n",
    "        \"\"\"\n",
    "        # todo: expand for categorical targets\n",
    "        if len(self.hparams.target_lags) == 0:\n",
    "            return {}\n",
    "        else:\n",
    "            # extract lags which are the same across all targets\n",
    "            lags = list(next(iter(self.hparams.target_lags.values())).values())\n",
    "            lag_names = {l: [] for l in lags}\n",
    "            for targeti_lags in self.hparams.target_lags.values():\n",
    "                for name, l in targeti_lags.items():\n",
    "                    lag_names[l].append(name)\n",
    "\n",
    "            lag_pos = {\n",
    "                lag: torch.tensor(\n",
    "                    [self.hparams.x_reals.index(name) for name in to_list(names)],\n",
    "                    device=self.device,\n",
    "                    dtype=torch.long,\n",
    "                )\n",
    "                for lag, names in lag_names.items()\n",
    "            }\n",
    "            return lag_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import lightning as pl\n",
    "class ImgTransformer(pl.LightningModule):\n",
    "    def __init__(self, imgpath_encoder, hidden_size):\n",
    "        super().__init__()\n",
    "        self.imgpath_encoder = imgpath_encoder\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) # Transform image based on ImageNet standard\n",
    "\n",
    "        self.swin_transformer = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\") # Get pre-trained SwinTransformer\n",
    "        self.linear = torch.nn.Linear(self.swin_transformer.config.hidden_size*49, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-1).type(torch.int)\n",
    "        path_li = self.imgpath_encoder.inverse_transform(x.cpu().ravel())\n",
    "        img_li = []\n",
    "        for path in path_li:\n",
    "            img = self.transform(Image.open(path).convert(\"RGB\"))\n",
    "            img_li.append(img)\n",
    "        img_tensor = torch.stack(img_li, dim=0)\n",
    "        swin_res = self.swin_transformer(img_tensor.to(device)).last_hidden_state.reshape(x.shape[0], -1)\n",
    "        res = torch.nn.ReLU()(self.linear(swin_res))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The temporal fusion transformer is a powerful predictive model for forecasting timeseries\n",
    "\"\"\"\n",
    "from copy import copy\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Metric as LightningMetric\n",
    "\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE, MAPE, MASE, RMSE, SMAPE, MultiHorizonMetric, MultiLoss, QuantileLoss\n",
    "# from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "from pytorch_forecasting.models.nn import LSTM, MultiEmbedding\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import (\n",
    "    AddNorm,\n",
    "    GateAddNorm,\n",
    "    GatedLinearUnit,\n",
    "    GatedResidualNetwork,\n",
    "    InterpretableMultiHeadAttention,\n",
    "    VariableSelectionNetwork,\n",
    ")\n",
    "from pytorch_forecasting.utils import create_mask, detach, integer_histogram, masked_op, padded_stack, to_list\n",
    "\n",
    "\n",
    "class TemporalFusionTransformer(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 16,\n",
    "        lstm_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        output_size: Union[int, List[int]] = 7,\n",
    "        loss: MultiHorizonMetric = None,\n",
    "        attention_head_size: int = 4,\n",
    "        max_encoder_length: int = 10,\n",
    "        static_categoricals: List[str] = [],\n",
    "        static_reals: List[str] = [],\n",
    "        time_varying_categoricals_encoder: List[str] = [],\n",
    "        time_varying_categoricals_decoder: List[str] = [],\n",
    "        categorical_groups: Dict[str, List[str]] = {},\n",
    "        time_varying_reals_encoder: List[str] = [],\n",
    "        time_varying_reals_decoder: List[str] = [],\n",
    "        x_reals: List[str] = [],\n",
    "        x_categoricals: List[str] = [],\n",
    "        hidden_continuous_size: int = 8,\n",
    "        hidden_continuous_sizes: Dict[str, int] = {},\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]] = {},\n",
    "        embedding_paddings: List[str] = [],\n",
    "        embedding_labels: Dict[str, np.ndarray] = {},\n",
    "        learning_rate: float = 1e-3,\n",
    "        log_interval: Union[int, float] = -1,\n",
    "        log_val_interval: Union[int, float] = None,\n",
    "        log_gradient_flow: bool = False,\n",
    "        reduce_on_plateau_patience: int = 1000,\n",
    "        monotone_constaints: Dict[str, int] = {},\n",
    "        share_single_variable_networks: bool = False,\n",
    "        causal_attention: bool = True,\n",
    "        logging_metrics: nn.ModuleList = None,\n",
    "        imgpath_encoder = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if logging_metrics is None:\n",
    "            logging_metrics = nn.ModuleList([SMAPE(), MAE(), RMSE(), MAPE()])\n",
    "        if loss is None:\n",
    "            loss = QuantileLoss()\n",
    "        self.save_hyperparameters()\n",
    "        # store loss function separately as it is a module\n",
    "        assert isinstance(loss, LightningMetric), \"Loss has to be a PyTorch Lightning `Metric`\"\n",
    "        super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
    "\n",
    "        # processing inputs\n",
    "        # embeddings\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes=self.hparams.embedding_sizes,\n",
    "            categorical_groups=self.hparams.categorical_groups,\n",
    "            embedding_paddings=self.hparams.embedding_paddings,\n",
    "            x_categoricals=self.hparams.x_categoricals,\n",
    "            max_embedding_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # continuous variable processing\n",
    "        self.prescalers = nn.ModuleDict(\n",
    "            {\n",
    "                name: nn.Linear(1, self.hparams.hidden_continuous_sizes.get(name, self.hparams.hidden_continuous_size)) if name != \"img_path\"\n",
    "                else ImgTransformer(imgpath_encoder, self.hparams.hidden_continuous_sizes.get(name, self.hparams.hidden_continuous_size))\n",
    "                for name in self.reals\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        # variable selection\n",
    "        # variable selection for static variables\n",
    "        static_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name] for name in self.hparams.static_categoricals\n",
    "        }\n",
    "        static_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(name, self.hparams.hidden_continuous_size)\n",
    "                for name in self.hparams.static_reals\n",
    "            }\n",
    "        )\n",
    "        self.static_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=static_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={name: True for name in self.hparams.static_categoricals},\n",
    "            dropout=self.hparams.dropout,\n",
    "            prescalers=self.prescalers,\n",
    "        )\n",
    "\n",
    "        # variable selection for encoder and decoder\n",
    "        encoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name] for name in self.hparams.time_varying_categoricals_encoder\n",
    "        }\n",
    "        encoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(name, self.hparams.hidden_continuous_size)\n",
    "                for name in self.hparams.time_varying_reals_encoder\n",
    "            }\n",
    "        )\n",
    "\n",
    "        decoder_input_sizes = {\n",
    "            name: self.input_embeddings.output_size[name] for name in self.hparams.time_varying_categoricals_decoder\n",
    "        }\n",
    "        decoder_input_sizes.update(\n",
    "            {\n",
    "                name: self.hparams.hidden_continuous_sizes.get(name, self.hparams.hidden_continuous_size)\n",
    "                for name in self.hparams.time_varying_reals_decoder\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # create single variable grns that are shared across decoder and encoder\n",
    "        if self.hparams.share_single_variable_networks:\n",
    "            self.shared_single_variable_grns = nn.ModuleDict()\n",
    "            for name, input_size in encoder_input_sizes.items():\n",
    "                self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                    input_size,\n",
    "                    min(input_size, self.hparams.hidden_size),\n",
    "                    self.hparams.hidden_size,\n",
    "                    self.hparams.dropout,\n",
    "                )\n",
    "            for name, input_size in decoder_input_sizes.items():\n",
    "                if name not in self.shared_single_variable_grns:\n",
    "                    self.shared_single_variable_grns[name] = GatedResidualNetwork(\n",
    "                        input_size,\n",
    "                        min(input_size, self.hparams.hidden_size),\n",
    "                        self.hparams.hidden_size,\n",
    "                        self.hparams.dropout,\n",
    "                    )\n",
    "\n",
    "        self.encoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=encoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={name: True for name in self.hparams.time_varying_categoricals_encoder},\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns={}\n",
    "            if not self.hparams.share_single_variable_networks\n",
    "            else self.shared_single_variable_grns,\n",
    "        )\n",
    "\n",
    "        self.decoder_variable_selection = VariableSelectionNetwork(\n",
    "            input_sizes=decoder_input_sizes,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            input_embedding_flags={name: True for name in self.hparams.time_varying_categoricals_decoder},\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "            prescalers=self.prescalers,\n",
    "            single_variable_grns={}\n",
    "            if not self.hparams.share_single_variable_networks\n",
    "            else self.shared_single_variable_grns,\n",
    "        )\n",
    "\n",
    "        # static encoders\n",
    "        # for variable selection\n",
    "        self.static_context_variable_selection = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # for hidden state of the lstm\n",
    "        self.static_context_initial_hidden_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # for cell state of the lstm\n",
    "        self.static_context_initial_cell_lstm = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        # for post lstm static enrichment\n",
    "        self.static_context_enrichment = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size, self.hparams.hidden_size, self.hparams.hidden_size, self.hparams.dropout\n",
    "        )\n",
    "\n",
    "        # lstm encoder (history) and decoder (future) for local processing\n",
    "        self.lstm_encoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.lstm_decoder = LSTM(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            num_layers=self.hparams.lstm_layers,\n",
    "            dropout=self.hparams.dropout if self.hparams.lstm_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # skip connection for lstm\n",
    "        self.post_lstm_gate_encoder = GatedLinearUnit(self.hparams.hidden_size, dropout=self.hparams.dropout)\n",
    "        self.post_lstm_gate_decoder = self.post_lstm_gate_encoder\n",
    "        # self.post_lstm_gate_decoder = GatedLinearUnit(self.hparams.hidden_size, dropout=self.hparams.dropout)\n",
    "        self.post_lstm_add_norm_encoder = AddNorm(self.hparams.hidden_size, trainable_add=False)\n",
    "        # self.post_lstm_add_norm_decoder = AddNorm(self.hparams.hidden_size, trainable_add=True)\n",
    "        self.post_lstm_add_norm_decoder = self.post_lstm_add_norm_encoder\n",
    "\n",
    "        # static enrichment and processing past LSTM\n",
    "        self.static_enrichment = GatedResidualNetwork(\n",
    "            input_size=self.hparams.hidden_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            output_size=self.hparams.hidden_size,\n",
    "            dropout=self.hparams.dropout,\n",
    "            context_size=self.hparams.hidden_size,\n",
    "        )\n",
    "\n",
    "        # attention for long-range processing\n",
    "        self.multihead_attn = InterpretableMultiHeadAttention(\n",
    "            d_model=self.hparams.hidden_size, n_head=self.hparams.attention_head_size, dropout=self.hparams.dropout\n",
    "        )\n",
    "        self.post_attn_gate_norm = GateAddNorm(\n",
    "            self.hparams.hidden_size, dropout=self.hparams.dropout, trainable_add=False\n",
    "        )\n",
    "        self.pos_wise_ff = GatedResidualNetwork(\n",
    "            self.hparams.hidden_size, self.hparams.hidden_size, self.hparams.hidden_size, dropout=self.hparams.dropout\n",
    "        )\n",
    "\n",
    "        # output processing -> no dropout at this late stage\n",
    "        self.pre_output_gate_norm = GateAddNorm(self.hparams.hidden_size, dropout=None, trainable_add=False)\n",
    "\n",
    "        if self.n_targets > 1:  # if to run with multiple targets\n",
    "            self.output_layer = nn.ModuleList(\n",
    "                [nn.Linear(self.hparams.hidden_size, output_size) for output_size in self.hparams.output_size]\n",
    "            )\n",
    "        else:\n",
    "            self.output_layer = nn.Linear(self.hparams.hidden_size, self.hparams.output_size)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataset(\n",
    "        cls,\n",
    "        dataset: TimeSeriesDataSet,\n",
    "        allowed_encoder_known_variable_names: List[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create model from dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: timeseries dataset\n",
    "            allowed_encoder_known_variable_names: List of known variables that are allowed in encoder, defaults to all\n",
    "            **kwargs: additional arguments such as hyperparameters for model (see ``__init__()``)\n",
    "\n",
    "        Returns:\n",
    "            TemporalFusionTransformer\n",
    "        \"\"\"\n",
    "        # add maximum encoder length\n",
    "        # update defaults\n",
    "        new_kwargs = copy(kwargs)\n",
    "        new_kwargs[\"max_encoder_length\"] = dataset.max_encoder_length\n",
    "        new_kwargs.update(cls.deduce_default_output_parameters(dataset, kwargs, QuantileLoss()))\n",
    "\n",
    "        # create class and return\n",
    "        return super().from_dataset(\n",
    "            dataset, allowed_encoder_known_variable_names=allowed_encoder_known_variable_names, **new_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    def expand_static_context(self, context, timesteps):\n",
    "        \"\"\"\n",
    "        add time dimension to static context\n",
    "        \"\"\"\n",
    "        return context[:, None].expand(-1, timesteps, -1)\n",
    "\n",
    "\n",
    "    def get_attention_mask(self, encoder_lengths: torch.LongTensor, decoder_lengths: torch.LongTensor):\n",
    "        \"\"\"\n",
    "        Returns causal mask to apply for self-attention layer.\n",
    "        \"\"\"\n",
    "        decoder_length = decoder_lengths.max()\n",
    "        if self.hparams.causal_attention:\n",
    "            # indices to which is attended\n",
    "            attend_step = torch.arange(decoder_length, device=self.device)\n",
    "            # indices for which is predicted\n",
    "            predict_step = torch.arange(0, decoder_length, device=self.device)[:, None]\n",
    "            # do not attend to steps to self or after prediction\n",
    "            decoder_mask = (attend_step >= predict_step).unsqueeze(0).expand(encoder_lengths.size(0), -1, -1)\n",
    "        else:\n",
    "            # there is value in attending to future forecasts if they are made with knowledge currently\n",
    "            #   available\n",
    "            #   one possibility is here to use a second attention layer for future attention (assuming different effects\n",
    "            #   matter in the future than the past)\n",
    "            #   or alternatively using the same layer but allowing forward attention - i.e. only\n",
    "            #   masking out non-available data and self\n",
    "            decoder_mask = create_mask(decoder_length, decoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "        # do not attend to steps where data is padded\n",
    "        encoder_mask = create_mask(encoder_lengths.max(), encoder_lengths).unsqueeze(1).expand(-1, decoder_length, -1)\n",
    "        # combine masks along attended time - first encoder and then decoder\n",
    "        mask = torch.cat(\n",
    "            (\n",
    "                encoder_mask,\n",
    "                decoder_mask,\n",
    "            ),\n",
    "            dim=2,\n",
    "        )\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        input dimensions: n_samples x time x variables\n",
    "        \"\"\"\n",
    "        encoder_lengths = x[\"encoder_lengths\"]\n",
    "        decoder_lengths = x[\"decoder_lengths\"]\n",
    "        x_cat = torch.cat([x[\"encoder_cat\"], x[\"decoder_cat\"]], dim=1)  # concatenate in time dimension\n",
    "        x_cont = torch.cat([x[\"encoder_cont\"], x[\"decoder_cont\"]], dim=1)  # concatenate in time dimension\n",
    "        timesteps = x_cont.size(1)  # encode + decode length\n",
    "        max_encoder_length = int(encoder_lengths.max())\n",
    "        input_vectors = self.input_embeddings(x_cat)\n",
    "        input_vectors.update(\n",
    "            {\n",
    "                name: x_cont[..., idx].unsqueeze(-1)\n",
    "                for idx, name in enumerate(self.hparams.x_reals)\n",
    "                if name in self.reals\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Embedding and variable selection\n",
    "        if len(self.static_variables) > 0:\n",
    "            # static embeddings will be constant over entire batch\n",
    "            static_embedding = {name: input_vectors[name][:, 0] for name in self.static_variables}\n",
    "            static_embedding, static_variable_selection = self.static_variable_selection(static_embedding)\n",
    "        else:\n",
    "            static_embedding = torch.zeros(\n",
    "                (x_cont.size(0), self.hparams.hidden_size), dtype=self.dtype, device=self.device\n",
    "            )\n",
    "            static_variable_selection = torch.zeros((x_cont.size(0), 0), dtype=self.dtype, device=self.device)\n",
    "\n",
    "        static_context_variable_selection = self.expand_static_context(\n",
    "            self.static_context_variable_selection(static_embedding), timesteps\n",
    "        )\n",
    "        embeddings_varying_encoder = {\n",
    "            name: input_vectors[name][:, :max_encoder_length] for name in self.encoder_variables\n",
    "        }\n",
    "        embeddings_varying_encoder, encoder_sparse_weights = self.encoder_variable_selection(\n",
    "            embeddings_varying_encoder,\n",
    "            static_context_variable_selection[:, :max_encoder_length],\n",
    "        )\n",
    "\n",
    "        embeddings_varying_decoder = {\n",
    "            name: input_vectors[name][:, max_encoder_length:] for name in self.decoder_variables  # select decoder\n",
    "        }\n",
    "        embeddings_varying_decoder, decoder_sparse_weights = self.decoder_variable_selection(\n",
    "            embeddings_varying_decoder,\n",
    "            static_context_variable_selection[:, max_encoder_length:],\n",
    "        )\n",
    "\n",
    "        # LSTM\n",
    "        # calculate initial state\n",
    "        input_hidden = self.static_context_initial_hidden_lstm(static_embedding).expand(\n",
    "            self.hparams.lstm_layers, -1, -1\n",
    "        )\n",
    "        input_cell = self.static_context_initial_cell_lstm(static_embedding).expand(self.hparams.lstm_layers, -1, -1)\n",
    "\n",
    "        # run local encoder\n",
    "        encoder_output, (hidden, cell) = self.lstm_encoder(\n",
    "            embeddings_varying_encoder, (input_hidden, input_cell), lengths=encoder_lengths, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # run local decoder\n",
    "        decoder_output, _ = self.lstm_decoder(\n",
    "            embeddings_varying_decoder,\n",
    "            (hidden, cell),\n",
    "            lengths=decoder_lengths,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "\n",
    "        # skip connection over lstm\n",
    "        lstm_output_encoder = self.post_lstm_gate_encoder(encoder_output)\n",
    "        lstm_output_encoder = self.post_lstm_add_norm_encoder(lstm_output_encoder, embeddings_varying_encoder)\n",
    "\n",
    "        lstm_output_decoder = self.post_lstm_gate_decoder(decoder_output)\n",
    "        lstm_output_decoder = self.post_lstm_add_norm_decoder(lstm_output_decoder, embeddings_varying_decoder)\n",
    "\n",
    "        lstm_output = torch.cat([lstm_output_encoder, lstm_output_decoder], dim=1)\n",
    "\n",
    "        # static enrichment\n",
    "        static_context_enrichment = self.static_context_enrichment(static_embedding)\n",
    "        attn_input = self.static_enrichment(\n",
    "            lstm_output, self.expand_static_context(static_context_enrichment, timesteps)\n",
    "        )\n",
    "\n",
    "        # Attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(\n",
    "            q=attn_input[:, max_encoder_length:],  # query only for predictions\n",
    "            k=attn_input,\n",
    "            v=attn_input,\n",
    "            mask=self.get_attention_mask(encoder_lengths=encoder_lengths, decoder_lengths=decoder_lengths),\n",
    "        )\n",
    "\n",
    "        # skip connection over attention\n",
    "        attn_output = self.post_attn_gate_norm(attn_output, attn_input[:, max_encoder_length:])\n",
    "\n",
    "        output = self.pos_wise_ff(attn_output)\n",
    "\n",
    "        # skip connection over temporal fusion decoder (not LSTM decoder despite the LSTM output contains\n",
    "        # a skip from the variable selection network)\n",
    "        output = self.pre_output_gate_norm(output, lstm_output[:, max_encoder_length:])\n",
    "        if self.n_targets > 1:  # if to use multi-target architecture\n",
    "            output = [output_layer(output) for output_layer in self.output_layer]\n",
    "        else:\n",
    "            output = self.output_layer(output)\n",
    "\n",
    "        return self.to_network_output(\n",
    "            prediction=self.transform_output(output, target_scale=x[\"target_scale\"]),\n",
    "            encoder_attention=attn_output_weights[..., :max_encoder_length],\n",
    "            decoder_attention=attn_output_weights[..., max_encoder_length:],\n",
    "            static_variables=static_variable_selection,\n",
    "            encoder_variables=encoder_sparse_weights,\n",
    "            decoder_variables=decoder_sparse_weights,\n",
    "            decoder_lengths=decoder_lengths,\n",
    "            encoder_lengths=encoder_lengths,\n",
    "        )\n",
    "\n",
    "\n",
    "    def on_fit_end(self):\n",
    "        if self.log_interval > 0:\n",
    "            self.log_embeddings()\n",
    "\n",
    "\n",
    "    def create_log(self, x, y, out, batch_idx, **kwargs):\n",
    "        log = super().create_log(x, y, out, batch_idx, **kwargs)\n",
    "        if self.log_interval > 0:\n",
    "            log[\"interpretation\"] = self._log_interpretation(out)\n",
    "        return log\n",
    "\n",
    "\n",
    "    def _log_interpretation(self, out):\n",
    "        # calculate interpretations etc for latter logging\n",
    "        interpretation = self.interpret_output(\n",
    "            detach(out),\n",
    "            reduction=\"sum\",\n",
    "            attention_prediction_horizon=0,  # attention only for first prediction horizon\n",
    "        )\n",
    "        return interpretation\n",
    "\n",
    "    def on_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        run at epoch end for training or validation\n",
    "        \"\"\"\n",
    "        if self.log_interval > 0 and not self.training:\n",
    "            self.log_interpretation(outputs)\n",
    "\n",
    "\n",
    "    def interpret_output(\n",
    "        self,\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        reduction: str = \"none\",\n",
    "        attention_prediction_horizon: int = 0,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        interpret output of model\n",
    "\n",
    "        Args:\n",
    "            out: output as produced by ``forward()``\n",
    "            reduction: \"none\" for no averaging over batches, \"sum\" for summing attentions, \"mean\" for\n",
    "                normalizing by encode lengths\n",
    "            attention_prediction_horizon: which prediction horizon to use for attention\n",
    "\n",
    "        Returns:\n",
    "            interpretations that can be plotted with ``plot_interpretation()``\n",
    "        \"\"\"\n",
    "        # take attention and concatenate if a list to proper attention object\n",
    "        batch_size = len(out[\"decoder_attention\"])\n",
    "        if isinstance(out[\"decoder_attention\"], (list, tuple)):\n",
    "            # start with decoder attention\n",
    "            # assume issue is in last dimension, we need to find max\n",
    "            max_last_dimension = max(x.size(-1) for x in out[\"decoder_attention\"])\n",
    "            first_elm = out[\"decoder_attention\"][0]\n",
    "            # create new attention tensor into which we will scatter\n",
    "            decoder_attention = torch.full(\n",
    "                (batch_size, *first_elm.shape[:-1], max_last_dimension),\n",
    "                float(\"nan\"),\n",
    "                dtype=first_elm.dtype,\n",
    "                device=first_elm.device,\n",
    "            )\n",
    "            # scatter into tensor\n",
    "            for idx, x in enumerate(out[\"decoder_attention\"]):\n",
    "                decoder_length = out[\"decoder_lengths\"][idx]\n",
    "                decoder_attention[idx, :, :, :decoder_length] = x[..., :decoder_length]\n",
    "        else:\n",
    "            decoder_attention = out[\"decoder_attention\"].clone()\n",
    "            decoder_mask = create_mask(out[\"decoder_attention\"].size(1), out[\"decoder_lengths\"])\n",
    "            decoder_attention[decoder_mask[..., None, None].expand_as(decoder_attention)] = float(\"nan\")\n",
    "\n",
    "        if isinstance(out[\"encoder_attention\"], (list, tuple)):\n",
    "            # same game for encoder attention\n",
    "            # create new attention tensor into which we will scatter\n",
    "            first_elm = out[\"encoder_attention\"][0]\n",
    "            encoder_attention = torch.full(\n",
    "                (batch_size, *first_elm.shape[:-1], self.hparams.max_encoder_length),\n",
    "                float(\"nan\"),\n",
    "                dtype=first_elm.dtype,\n",
    "                device=first_elm.device,\n",
    "            )\n",
    "            # scatter into tensor\n",
    "            for idx, x in enumerate(out[\"encoder_attention\"]):\n",
    "                encoder_length = out[\"encoder_lengths\"][idx]\n",
    "                encoder_attention[idx, :, :, self.hparams.max_encoder_length - encoder_length :] = x[\n",
    "                    ..., :encoder_length\n",
    "                ]\n",
    "        else:\n",
    "            # roll encoder attention (so start last encoder value is on the right)\n",
    "            encoder_attention = out[\"encoder_attention\"].clone()\n",
    "            shifts = encoder_attention.size(3) - out[\"encoder_lengths\"]\n",
    "            new_index = (\n",
    "                torch.arange(encoder_attention.size(3), device=encoder_attention.device)[None, None, None].expand_as(\n",
    "                    encoder_attention\n",
    "                )\n",
    "                - shifts[:, None, None, None]\n",
    "            ) % encoder_attention.size(3)\n",
    "            encoder_attention = torch.gather(encoder_attention, dim=3, index=new_index)\n",
    "            # expand encoder_attentiont to full size\n",
    "            if encoder_attention.size(-1) < self.hparams.max_encoder_length:\n",
    "                encoder_attention = torch.concat(\n",
    "                    [\n",
    "                        torch.full(\n",
    "                            (\n",
    "                                *encoder_attention.shape[:-1],\n",
    "                                self.hparams.max_encoder_length - out[\"encoder_lengths\"].max(),\n",
    "                            ),\n",
    "                            float(\"nan\"),\n",
    "                            dtype=encoder_attention.dtype,\n",
    "                            device=encoder_attention.device,\n",
    "                        ),\n",
    "                        encoder_attention,\n",
    "                    ],\n",
    "                    dim=-1,\n",
    "                )\n",
    "\n",
    "        # combine attention vector\n",
    "        attention = torch.concat([encoder_attention, decoder_attention], dim=-1)\n",
    "        attention[attention < 1e-5] = float(\"nan\")\n",
    "\n",
    "        # histogram of decode and encode lengths\n",
    "        encoder_length_histogram = integer_histogram(out[\"encoder_lengths\"], min=0, max=self.hparams.max_encoder_length)\n",
    "        decoder_length_histogram = integer_histogram(\n",
    "            out[\"decoder_lengths\"], min=1, max=out[\"decoder_variables\"].size(1)\n",
    "        )\n",
    "\n",
    "        # mask where decoder and encoder where not applied when averaging variable selection weights\n",
    "        encoder_variables = out[\"encoder_variables\"].squeeze(-2).clone()\n",
    "        encode_mask = create_mask(encoder_variables.size(1), out[\"encoder_lengths\"])\n",
    "        encoder_variables = encoder_variables.masked_fill(encode_mask.unsqueeze(-1), 0.0).sum(dim=1)\n",
    "        encoder_variables /= (\n",
    "            out[\"encoder_lengths\"]\n",
    "            .where(out[\"encoder_lengths\"] > 0, torch.ones_like(out[\"encoder_lengths\"]))\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        decoder_variables = out[\"decoder_variables\"].squeeze(-2).clone()\n",
    "        decode_mask = create_mask(decoder_variables.size(1), out[\"decoder_lengths\"])\n",
    "        decoder_variables = decoder_variables.masked_fill(decode_mask.unsqueeze(-1), 0.0).sum(dim=1)\n",
    "        decoder_variables /= out[\"decoder_lengths\"].unsqueeze(-1)\n",
    "\n",
    "        # static variables need no masking\n",
    "        static_variables = out[\"static_variables\"].squeeze(1)\n",
    "        # attention is batch x time x heads x time_to_attend\n",
    "        # average over heads + only keep prediction attention and attention on observed timesteps\n",
    "        attention = masked_op(\n",
    "            attention[\n",
    "                :, attention_prediction_horizon, :, : self.hparams.max_encoder_length + attention_prediction_horizon\n",
    "            ],\n",
    "            op=\"mean\",\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        if reduction != \"none\":  # if to average over batches\n",
    "            static_variables = static_variables.sum(dim=0)\n",
    "            encoder_variables = encoder_variables.sum(dim=0)\n",
    "            decoder_variables = decoder_variables.sum(dim=0)\n",
    "\n",
    "            attention = masked_op(attention, dim=0, op=reduction)\n",
    "        else:\n",
    "            attention = attention / masked_op(attention, dim=1, op=\"sum\").unsqueeze(-1)  # renormalize\n",
    "\n",
    "        interpretation = dict(\n",
    "            attention=attention.masked_fill(torch.isnan(attention), 0.0),\n",
    "            static_variables=static_variables,\n",
    "            encoder_variables=encoder_variables,\n",
    "            decoder_variables=decoder_variables,\n",
    "            encoder_length_histogram=encoder_length_histogram,\n",
    "            decoder_length_histogram=decoder_length_histogram,\n",
    "        )\n",
    "        return interpretation\n",
    "\n",
    "\n",
    "    def plot_prediction(\n",
    "        self,\n",
    "        x: Dict[str, torch.Tensor],\n",
    "        out: Dict[str, torch.Tensor],\n",
    "        idx: int,\n",
    "        plot_attention: bool = True,\n",
    "        add_loss_to_title: bool = False,\n",
    "        show_future_observed: bool = True,\n",
    "        ax=None,\n",
    "        **kwargs,\n",
    "    ) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Plot actuals vs prediction and attention\n",
    "\n",
    "        Args:\n",
    "            x (Dict[str, torch.Tensor]): network input\n",
    "            out (Dict[str, torch.Tensor]): network output\n",
    "            idx (int): sample index\n",
    "            plot_attention: if to plot attention on secondary axis\n",
    "            add_loss_to_title: if to add loss to title. Default to False.\n",
    "            show_future_observed: if to show actuals for future. Defaults to True.\n",
    "            ax: matplotlib axes to plot on\n",
    "\n",
    "        Returns:\n",
    "            plt.Figure: matplotlib figure\n",
    "        \"\"\"\n",
    "\n",
    "        # plot prediction as normal\n",
    "        fig = super().plot_prediction(\n",
    "            x,\n",
    "            out,\n",
    "            idx=idx,\n",
    "            add_loss_to_title=add_loss_to_title,\n",
    "            show_future_observed=show_future_observed,\n",
    "            ax=ax,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # add attention on secondary axis\n",
    "        if plot_attention:\n",
    "            interpretation = self.interpret_output(out.iget(slice(idx, idx + 1)))\n",
    "            for f in to_list(fig):\n",
    "                ax = f.axes[0]\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.set_ylabel(\"Attention\")\n",
    "                encoder_length = x[\"encoder_lengths\"][0]\n",
    "                ax2.plot(\n",
    "                    torch.arange(-encoder_length, 0),\n",
    "                    interpretation[\"attention\"][0, -encoder_length:].detach().cpu(),\n",
    "                    alpha=0.2,\n",
    "                    color=\"k\",\n",
    "                )\n",
    "                f.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "    def plot_interpretation(self, interpretation: Dict[str, torch.Tensor]) -> Dict[str, plt.Figure]:\n",
    "        \"\"\"\n",
    "        Make figures that interpret model.\n",
    "\n",
    "        * Attention\n",
    "        * Variable selection weights / importances\n",
    "\n",
    "        Args:\n",
    "            interpretation: as obtained from ``interpret_output()``\n",
    "\n",
    "        Returns:\n",
    "            dictionary of matplotlib figures\n",
    "        \"\"\"\n",
    "        figs = {}\n",
    "\n",
    "        # attention\n",
    "        fig, ax = plt.subplots()\n",
    "        attention = interpretation[\"attention\"].detach().cpu()\n",
    "        attention = attention / attention.sum(-1).unsqueeze(-1)\n",
    "        ax.plot(\n",
    "            np.arange(-self.hparams.max_encoder_length, attention.size(0) - self.hparams.max_encoder_length), attention\n",
    "        )\n",
    "        ax.set_xlabel(\"Time index\")\n",
    "        ax.set_ylabel(\"Attention\")\n",
    "        ax.set_title(\"Attention\")\n",
    "        figs[\"attention\"] = fig\n",
    "\n",
    "        # variable selection\n",
    "        def make_selection_plot(title, values, labels):\n",
    "            fig, ax = plt.subplots(figsize=(7, len(values) * 0.25 + 2))\n",
    "            order = np.argsort(values)\n",
    "            values = values / values.sum(-1).unsqueeze(-1)\n",
    "            ax.barh(np.arange(len(values)), values[order] * 100, tick_label=np.asarray(labels)[order])\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel(\"Importance in %\")\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "\n",
    "        figs[\"static_variables\"] = make_selection_plot(\n",
    "            \"Static variables importance\", interpretation[\"static_variables\"].detach().cpu(), self.static_variables\n",
    "        )\n",
    "        figs[\"encoder_variables\"] = make_selection_plot(\n",
    "            \"Encoder variables importance\", interpretation[\"encoder_variables\"].detach().cpu(), self.encoder_variables\n",
    "        )\n",
    "        figs[\"decoder_variables\"] = make_selection_plot(\n",
    "            \"Decoder variables importance\", interpretation[\"decoder_variables\"].detach().cpu(), self.decoder_variables\n",
    "        )\n",
    "\n",
    "        return figs\n",
    "\n",
    "\n",
    "    def log_interpretation(self, outputs):\n",
    "        \"\"\"\n",
    "        Log interpretation metrics to tensorboard.\n",
    "        \"\"\"\n",
    "        # extract interpretations\n",
    "        interpretation = {\n",
    "            # use padded_stack because decoder length histogram can be of different length\n",
    "            name: padded_stack([x[\"interpretation\"][name].detach() for x in outputs], side=\"right\", value=0).sum(0)\n",
    "            for name in outputs[0][\"interpretation\"].keys()\n",
    "        }\n",
    "        # normalize attention with length histogram squared to account for: 1. zeros in attention and\n",
    "        # 2. higher attention due to less values\n",
    "        attention_occurances = interpretation[\"encoder_length_histogram\"][1:].flip(0).float().cumsum(0)\n",
    "        attention_occurances = attention_occurances / attention_occurances.max()\n",
    "        attention_occurances = torch.cat(\n",
    "            [\n",
    "                attention_occurances,\n",
    "                torch.ones(\n",
    "                    interpretation[\"attention\"].size(0) - attention_occurances.size(0),\n",
    "                    dtype=attention_occurances.dtype,\n",
    "                    device=attention_occurances.device,\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        interpretation[\"attention\"] = interpretation[\"attention\"] / attention_occurances.pow(2).clamp(1.0)\n",
    "        interpretation[\"attention\"] = interpretation[\"attention\"] / interpretation[\"attention\"].sum()\n",
    "\n",
    "        figs = self.plot_interpretation(interpretation)  # make interpretation figures\n",
    "        label = self.current_stage\n",
    "        # log to tensorboard\n",
    "        for name, fig in figs.items():\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{label.capitalize()} {name} importance\", fig, global_step=self.global_step\n",
    "            )\n",
    "\n",
    "        # log lengths of encoder/decoder\n",
    "        for type in [\"encoder\", \"decoder\"]:\n",
    "            fig, ax = plt.subplots()\n",
    "            lengths = (\n",
    "                padded_stack([out[\"interpretation\"][f\"{type}_length_histogram\"] for out in outputs])\n",
    "                .sum(0)\n",
    "                .detach()\n",
    "                .cpu()\n",
    "            )\n",
    "            if type == \"decoder\":\n",
    "                start = 1\n",
    "            else:\n",
    "                start = 0\n",
    "            ax.plot(torch.arange(start, start + len(lengths)), lengths)\n",
    "            ax.set_xlabel(f\"{type.capitalize()} length\")\n",
    "            ax.set_ylabel(\"Number of samples\")\n",
    "            ax.set_title(f\"{type.capitalize()} length distribution in {label} epoch\")\n",
    "\n",
    "            self.logger.experiment.add_figure(\n",
    "                f\"{label.capitalize()} {type} length distribution\", fig, global_step=self.global_step\n",
    "            )\n",
    "\n",
    "\n",
    "    def log_embeddings(self):\n",
    "        \"\"\"\n",
    "        Log embeddings to tensorboard\n",
    "        \"\"\"\n",
    "        for name, emb in self.input_embeddings.items():\n",
    "            labels = self.hparams.embedding_labels[name]\n",
    "            self.logger.experiment.add_embedding(\n",
    "                emb.weight.data.detach().cpu(), metadata=labels, tag=name, global_step=self.global_step\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:197: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Create network from TimeSeriesDataSet\n",
    "model = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=128,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.3,\n",
    "    hidden_continuous_size=128,\n",
    "    output_size=8,  # number of quantiles\n",
    "    imgpath_encoder = encoder,\n",
    "    loss=pf.metrics.QuantileLoss(),\n",
    "    log_interval=10,  # logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = pf.metrics.quantile.QuantileLoss()\n",
    "\n",
    "def train(epoch, val_mean_loss):\n",
    "    model.train()\n",
    "    total_len = len(train_dataloader)\n",
    "    total_loss = 0\n",
    "    for n, data in enumerate(train_dataloader):\n",
    "        x, y = data\n",
    "        x = {key:val.to(device) for key, val in x.items()}\n",
    "        \n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x).prediction\n",
    "        loss = loss_fn(pred, y[0].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss / (n+1)\n",
    "        print(f\"\\r {epoch}:{n}/{total_len} mean_train_loss: {mean_loss} -- mean_val_loss: {val_mean_loss}\", end=\"\")\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    total_len = len(val_dataloader)\n",
    "    total_loss = 0\n",
    "    for n, data in enumerate(val_dataloader):\n",
    "        x, y = data\n",
    "        x = {key:val.to(device) for key, val in x.items()}\n",
    "        \n",
    "        # Pred\n",
    "        with torch.no_grad():\n",
    "            pred = model(x).prediction\n",
    "            loss = loss_fn(pred, y[0].to(device))\n",
    "\n",
    "            # Report\n",
    "            total_loss += loss.item()\n",
    "            mean_loss = total_loss / (n+1)\n",
    "    return mean_loss\n",
    " \n",
    "def plot(train_loss_li, val_loss_li):\n",
    "    # Plot loss\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss_li, label=\"train\")\n",
    "    plt.plot(val_loss_li, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "\n",
    "epoch = 15\n",
    "train_loss_li, val_loss_li = [], []\n",
    "val_mean_loss = 0\n",
    "for e in range(epoch):\n",
    "    train_loss = train(e, val_mean_loss) # Train\n",
    "    val_mean_loss = val_loss = val()\n",
    "\n",
    "    train_loss_li.append(train_loss)\n",
    "    val_loss_li.append(val_loss)\n",
    "    plot(train_loss_li, val_loss_li)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 9, 11])\n",
      "torch.Size([64, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for data in val_dataloader:\n",
    "    x, y = data\n",
    "    x = {k:v.to(device) for k, v in x.items()}\n",
    "    pred = model(x)\n",
    "    print(x[\"encoder_cont\"].shape)\n",
    "    print(y[0].shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(23,5))\n",
    "    model.plot_prediction(x, # network input\n",
    "                        pred, # network output\n",
    "                        idx=0,\n",
    "                        add_loss_to_title=True,\n",
    "                        ax=ax)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalFusionTransformer(\n",
       "  \t\"attention_head_size\":               4\n",
       "  \t\"categorical_groups\":                {}\n",
       "  \t\"causal_attention\":                  True\n",
       "  \t\"dataset_parameters\":                {'time_idx': 'time_idx', 'target': 'sales', 'group_ids': ['img_path'], 'weight': None, 'max_encoder_length': 9, 'min_encoder_length': 4, 'min_prediction_idx': 0, 'min_prediction_length': 3, 'max_prediction_length': 3, 'static_categoricals': ['season', 'category', 'color', 'fabric', 'extra'], 'static_reals': ['img_path', 'release_day', 'release_week', 'release_month', 'release_year', 'encoder_length', 'sales_center', 'sales_scale'], 'time_varying_known_categoricals': [], 'time_varying_known_reals': ['time_idx', 'relative_time_idx'], 'time_varying_unknown_categoricals': [], 'time_varying_unknown_reals': ['sales'], 'variable_groups': {}, 'constant_fill_strategy': {}, 'allow_missing_timesteps': False, 'lags': {}, 'add_relative_time_idx': True, 'add_target_scales': True, 'add_encoder_length': True, 'target_normalizer': TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs={}), 'categorical_encoders': {'__group_id__img_path': NaNLabelEncoder(add_nan=False, warn=True), 'season': NaNLabelEncoder(add_nan=False, warn=True), 'category': NaNLabelEncoder(add_nan=False, warn=True), 'color': NaNLabelEncoder(add_nan=False, warn=True), 'fabric': NaNLabelEncoder(add_nan=False, warn=True), 'extra': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'img_path': None, 'release_day': StandardScaler(), 'release_week': StandardScaler(), 'release_month': StandardScaler(), 'release_year': StandardScaler(), 'encoder_length': StandardScaler(), 'sales_center': StandardScaler(), 'sales_scale': StandardScaler(), 'time_idx': StandardScaler(), 'relative_time_idx': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
       "  \t\"dropout\":                           0.3\n",
       "  \t\"embedding_labels\":                  {'season': {'AW17': 0, 'AW18': 1, 'AW19': 2, 'SS17': 3, 'SS18': 4, 'SS19': 5}, 'category': {'capris': 0, 'culottes': 1, 'doll dress': 2, 'drop sleeve': 3, 'gitana skirt': 4, 'kimono dress': 5, 'long cardigan': 6, 'long coat': 7, 'long dress': 8, 'long duster': 9, 'long sleeve': 10, 'maxi': 11, 'medium cardigan': 12, 'medium coat': 13, 'miniskirt': 14, 'patterned': 15, 'printed': 16, 'sheath dress': 17, 'shirt dress': 18, 'short cardigan': 19, 'short coat': 20, 'short sleeves': 21, 'shorts': 22, 'sleeveless': 23, 'solid colours': 24, 'tracksuit': 25, 'trapeze dress': 26}, 'color': {'black': 0, 'blue': 1, 'brown': 2, 'green': 3, 'grey': 4, 'orange': 5, 'red': 6, 'violet': 7, 'white': 8, 'yellow': 9}, 'fabric': {'acrylic': 0, 'angora': 1, 'bengaline': 2, 'cady': 3, 'chambree': 4, 'chanel': 5, 'chine crepe': 6, 'cloth': 7, 'cotton': 8, 'crepe': 9, 'dainetto': 10, 'dark jeans': 11, 'devore': 12, 'embossed': 13, 'faux leather': 14, 'flamed': 15, 'fluid': 16, 'fluid polyviscous': 17, 'foam rubber': 18, 'frise': 19, 'fur': 20, 'georgette': 21, 'heavy jeans': 22, 'hron': 23, 'ity': 24, 'jacquard': 25, 'lace': 26, 'light jeans': 27, 'linen': 28, 'lurex': 29, 'macrame': 30, 'marocain': 31, 'matte jersey': 32, 'milano stitch': 33, 'mohair': 34, 'muslin cotton or silk': 35, 'mutton': 36, 'nice': 37, 'nylon': 38, 'ottoman': 39, 'paillettes': 40, 'piquet': 41, 'plisse': 42, 'plumetis': 43, 'plush': 44, 'polyviscous': 45, 'satin cotton': 46, 'scottish': 47, 'scuba crepe': 48, 'shiny jersey': 49, 'silky satin': 50, 'tactel': 51, 'technical': 52, 'tencel': 53, 'tulle': 54, 'velvet': 55, 'viscose twill': 56, 'webbing': 57}, 'extra': {'belted': 0, 'bow': 1, 'button': 2, 'collar': 3, 'cowl neck': 4, 'draped': 5, 'drawstring': 6, 'dropped': 7, 'flat': 8, 'hem': 9, 'long sleeve': 10, 'neckline': 11, 'pocket': 12, 'racerback': 13, 'scoop': 14, 'shoulder': 15, 'sleeve': 16, 'sleeveless': 17, 'strap': 18, 'strapless': 19, 'v-neck': 20, 'zip': 21, 'zipper': 22}}\n",
       "  \t\"embedding_paddings\":                []\n",
       "  \t\"embedding_sizes\":                   {'season': (6, 4), 'category': (27, 10), 'color': (10, 6), 'fabric': (58, 16), 'extra': (23, 9)}\n",
       "  \t\"hidden_continuous_size\":            128\n",
       "  \t\"hidden_continuous_sizes\":           {}\n",
       "  \t\"hidden_size\":                       128\n",
       "  \t\"imgpath_encoder\":                   LabelEncoder()\n",
       "  \t\"learning_rate\":                     0.03\n",
       "  \t\"log_gradient_flow\":                 False\n",
       "  \t\"log_interval\":                      10\n",
       "  \t\"log_val_interval\":                  10\n",
       "  \t\"logging_metrics\":                   ModuleList(\n",
       "  \t  (0): SMAPE()\n",
       "  \t  (1): MAE()\n",
       "  \t  (2): RMSE()\n",
       "  \t  (3): MAPE()\n",
       "  \t)\n",
       "  \t\"loss\":                              QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98])\n",
       "  \t\"lstm_layers\":                       1\n",
       "  \t\"max_encoder_length\":                9\n",
       "  \t\"monotone_constaints\":               {}\n",
       "  \t\"optimizer\":                         Ranger\n",
       "  \t\"optimizer_params\":                  None\n",
       "  \t\"output_size\":                       8\n",
       "  \t\"output_transformer\":                TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs={})\n",
       "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "  \t\"reduce_on_plateau_patience\":        4\n",
       "  \t\"reduce_on_plateau_reduction\":       2.0\n",
       "  \t\"share_single_variable_networks\":    False\n",
       "  \t\"static_categoricals\":               ['season', 'category', 'color', 'fabric', 'extra']\n",
       "  \t\"static_reals\":                      ['img_path', 'release_day', 'release_week', 'release_month', 'release_year', 'encoder_length', 'sales_center', 'sales_scale']\n",
       "  \t\"time_varying_categoricals_decoder\": []\n",
       "  \t\"time_varying_categoricals_encoder\": []\n",
       "  \t\"time_varying_reals_decoder\":        ['time_idx', 'relative_time_idx']\n",
       "  \t\"time_varying_reals_encoder\":        ['time_idx', 'relative_time_idx', 'sales']\n",
       "  \t\"weight_decay\":                      0.0\n",
       "  \t\"x_categoricals\":                    ['season', 'category', 'color', 'fabric', 'extra']\n",
       "  \t\"x_reals\":                           ['img_path', 'release_day', 'release_week', 'release_month', 'release_year', 'encoder_length', 'sales_center', 'sales_scale', 'time_idx', 'relative_time_idx', 'sales']\n",
       "  (loss): QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98])\n",
       "  (logging_metrics): ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "  )\n",
       "  (input_embeddings): MultiEmbedding(\n",
       "    (embeddings): ModuleDict(\n",
       "      (season): Embedding(6, 4)\n",
       "      (category): Embedding(27, 10)\n",
       "      (color): Embedding(10, 6)\n",
       "      (fabric): Embedding(58, 16)\n",
       "      (extra): Embedding(23, 9)\n",
       "    )\n",
       "  )\n",
       "  (prescalers): ModuleDict(\n",
       "    (img_path): ImgTransformer(\n",
       "      (swin_transformer): SwinModel(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "      )\n",
       "      (linear): Linear(in_features=37632, out_features=128, bias=True)\n",
       "    )\n",
       "    (release_day): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (release_week): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (release_month): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (release_year): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (encoder_length): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (sales_center): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (sales_scale): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (relative_time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (sales): Linear(in_features=1, out_features=128, bias=True)\n",
       "  )\n",
       "  (static_variable_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (resample_norm): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (fc1): Linear(in_features=1069, out_features=13, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=13, out_features=13, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (fc): Linear(in_features=13, out_features=26, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((13,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (single_variable_grns): ModuleDict(\n",
       "      (season): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (category): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (color): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (fabric): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (extra): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (img_path): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (release_day): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (release_week): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (release_month): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (release_year): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder_length): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sales_center): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sales_scale): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prescalers): ModuleDict(\n",
       "      (img_path): ImgTransformer(\n",
       "        (swin_transformer): SwinModel(\n",
       "          (embeddings): SwinEmbeddings(\n",
       "            (patch_embeddings): SwinPatchEmbeddings(\n",
       "              (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "            )\n",
       "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (encoder): SwinEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.1)\n",
       "                    (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (1): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.1)\n",
       "                    (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (2): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-5): 6 x SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.1)\n",
       "                    (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (downsample): SwinPatchMerging(\n",
       "                  (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (3): SwinStage(\n",
       "                (blocks): ModuleList(\n",
       "                  (0-1): 2 x SwinLayer(\n",
       "                    (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (attention): SwinAttention(\n",
       "                      (self): SwinSelfAttention(\n",
       "                        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                      (output): SwinSelfOutput(\n",
       "                        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                        (dropout): Dropout(p=0.0, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): SwinDropPath(p=0.1)\n",
       "                    (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                    (intermediate): SwinIntermediate(\n",
       "                      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): SwinOutput(\n",
       "                      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "        )\n",
       "        (linear): Linear(in_features=37632, out_features=128, bias=True)\n",
       "      )\n",
       "      (release_day): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (release_week): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (release_month): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (release_year): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (encoder_length): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (sales_center): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (sales_scale): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (encoder_variable_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (resample_norm): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (fc1): Linear(in_features=384, out_features=3, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (context): Linear(in_features=128, out_features=3, bias=False)\n",
       "      (fc2): Linear(in_features=3, out_features=3, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (fc): Linear(in_features=3, out_features=6, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (single_variable_grns): ModuleDict(\n",
       "      (time_idx): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_time_idx): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (sales): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prescalers): ModuleDict(\n",
       "      (time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (relative_time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (sales): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (decoder_variable_selection): VariableSelectionNetwork(\n",
       "    (flattened_grn): GatedResidualNetwork(\n",
       "      (resample_norm): ResampleNorm(\n",
       "        (resample): TimeDistributedInterpolation()\n",
       "        (gate): Sigmoid()\n",
       "        (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (fc1): Linear(in_features=256, out_features=2, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (context): Linear(in_features=128, out_features=2, bias=False)\n",
       "      (fc2): Linear(in_features=2, out_features=2, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (fc): Linear(in_features=2, out_features=4, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (single_variable_grns): ModuleDict(\n",
       "      (time_idx): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_time_idx): GatedResidualNetwork(\n",
       "        (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prescalers): ModuleDict(\n",
       "      (time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "      (relative_time_idx): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (static_context_variable_selection): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_initial_hidden_lstm): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_initial_cell_lstm): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_enrichment): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm_encoder): LSTM(128, 128, batch_first=True)\n",
       "  (lstm_decoder): LSTM(128, 128, batch_first=True)\n",
       "  (post_lstm_gate_encoder): GatedLinearUnit(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (post_lstm_gate_decoder): GatedLinearUnit(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (post_lstm_add_norm_encoder): AddNorm(\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (post_lstm_add_norm_decoder): AddNorm(\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (static_enrichment): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (context): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (multihead_attn): InterpretableMultiHeadAttention(\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (v_layer): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (q_layers): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (k_layers): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (softmax): Softmax(dim=2)\n",
       "    )\n",
       "    (w_h): Linear(in_features=32, out_features=128, bias=False)\n",
       "  )\n",
       "  (post_attn_gate_norm): GateAddNorm(\n",
       "    (glu): GatedLinearUnit(\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (add_norm): AddNorm(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.3, inplace=False)\n",
       "        (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_output_gate_norm): GateAddNorm(\n",
       "    (glu): GatedLinearUnit(\n",
       "      (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (add_norm): AddNorm(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
