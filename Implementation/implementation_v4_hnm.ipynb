{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = \"\"\"\n",
    "Desciption\n",
    "- Visuelle products\n",
    "- Not use of pytorch_forecasting\n",
    "- year, month, day, dayofweek as positional embedding feature\n",
    "- Extract attention from MY transformer but not from image model's\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd; pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import SwinModel, SwinConfig, ViTModel, ViTConfig, Mask2FormerModel, Mask2FormerConfig\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n",
    "import pytorch_forecasting as pf\n",
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f34ca0e2730>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling parameter\n",
    "n_smaples = None\n",
    "\n",
    "# Data parameter\n",
    "random_state = 0\n",
    "encoder_len = 365\n",
    "pred_len = 30\n",
    "batch_size = 128\n",
    "valid_start_date = \"2020-01-22\"\n",
    "\n",
    "# Model hyperparameter\n",
    "d_model = 128; d_model = 256; d_model = 512\n",
    "nhead = 4; nhead = 8\n",
    "d_ff = 256; d_ff = 512; d_ff = 1024\n",
    "dropout = 0.3; dropout = 0.3\n",
    "num_layers = 4; num_layers = 6\n",
    "\n",
    "# Seed set\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read transaction\n",
    "df_trans = pd.read_csv(\"../HnM/transactions_train.csv\", parse_dates=[\"t_dat\"], dtype={\"article_id\":str})\n",
    "df_train_raw = df_trans[df_trans[\"t_dat\"] < valid_start_date]\n",
    "df_valid_raw = df_trans[df_trans[\"t_dat\"] >= valid_start_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, is_train=True):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Make sales\n",
    "    data = data.groupby([\"article_id\", \"t_dat\"], as_index=False).agg(sales=(\"customer_id\",\"size\"), price=(\"price\",\"mean\"))\n",
    "    \n",
    "    # Expand date\n",
    "    def func(x):\n",
    "        date_min = x[\"t_dat\"].min()\n",
    "        date_max = x[\"t_dat\"].max()\n",
    "\n",
    "        date_ref = pd.DataFrame(pd.date_range(date_min, date_max, freq=\"d\"), columns=[\"t_dat\"])\n",
    "        x = pd.merge(date_ref, x, on=\"t_dat\", how=\"left\")\n",
    "        return x\n",
    "\n",
    "    data = data.groupby(\"article_id\", as_index=False).apply(lambda x: func(x)).reset_index(drop=True)\n",
    "    data[\"sales\"] = data[\"sales\"].fillna(0)\n",
    "    data[\"price\"] = data[\"price\"].fillna(method=\"ffill\")\n",
    "    data[\"article_id\"] = data[\"article_id\"].fillna(method=\"ffill\")\n",
    "    data[\"price\"] = data[\"price\"].fillna(method=\"ffill\")\n",
    "\n",
    "    # Generate temporal information\n",
    "    data[\"year\"] = data[\"t_dat\"].dt.year\n",
    "    data[\"month\"] = data[\"t_dat\"].dt.month\n",
    "    data[\"week\"] = data[\"t_dat\"].dt.isocalendar().week\n",
    "    data.loc[(data[\"month\"] == 12) & (data[\"week\"] < 10), \"year\"] = data[\"year\"] + 1\n",
    "    data.loc[(data[\"month\"] == 1) & (data[\"week\"] > 10), \"year\"] = data[\"year\"] - 1\n",
    "    data[\"dayofweek\"] = data[\"t_dat\"].dt.dayofweek\n",
    "    data[\"day\"] = data[\"t_dat\"].dt.day\n",
    "    data[\"month\"] -= 1\n",
    "    data[\"week\"] -= 1\n",
    "    data[\"day\"] -= 1\n",
    "    \n",
    "    # Aggregate with list\n",
    "    data = data.groupby(\"article_id\", as_index=False)[[\"sales\", \"price\", \"year\", \"month\", \"week\", \"day\", \"dayofweek\"]].agg(list)\n",
    "    data[\"size\"] = data[\"sales\"].str.len()\n",
    "\n",
    "    # Filter \n",
    "    if is_train:\n",
    "        data = data[data[\"size\"] > encoder_len + pred_len]\n",
    "    else: \n",
    "        data = data[data[\"size\"] > pred_len + 1]\n",
    "\n",
    "    return data\n",
    "\n",
    "df_train_prep = preprocess(df_train_raw)\n",
    "df_valid_prep = preprocess(df_valid_raw, is_train=False)\n",
    "\n",
    "df_train_prep.to_parquet(\"df_train_prep.parquet\")\n",
    "df_valid_prep.to_parquet(\"df_valid_prep.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales: torch.Size([128, 365, 1])\n",
      "year: torch.Size([128, 365, 1])\n",
      "month: torch.Size([128, 365])\n",
      "week: torch.Size([128, 365])\n",
      "day: torch.Size([128, 365])\n",
      "dayofweek: torch.Size([128, 365])\n",
      "____________________________________________________________________________________________________\n",
      "y: torch.Size([128, 30])\n",
      "year_y: torch.Size([128, 30, 1])\n",
      "month_y: torch.Size([128, 30])\n",
      "week_y: torch.Size([128, 30])\n",
      "day_y: torch.Size([128, 30])\n",
      "dayofweek_y: torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "df_train_prep = pd.read_parquet(\"df_train_prep.parquet\")\n",
    "df_valid_prep = pd.read_parquet(\"df_valid_prep.parquet\")\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.sales_tensor = self.get_windowed_tensor(data[\"sales\"].values)\n",
    "        self.year_tensor = self.get_windowed_tensor(data[\"year\"].values)\n",
    "        self.month_tensor = self.get_windowed_tensor(data[\"month\"].values)\n",
    "        self.week_tensor = self.get_windowed_tensor(data[\"week\"].values)\n",
    "        self.day_tensor = self.get_windowed_tensor(data[\"day\"].values)\n",
    "        self.dayofweek_tensor = self.get_windowed_tensor(data[\"dayofweek\"].values)\n",
    "    \n",
    "    def get_windowed_tensor(self, data):\n",
    "        windowed_tensor_li = []\n",
    "        for d in data:\n",
    "            d = d.astype(float)\n",
    "            d = torch.FloatTensor(d).unfold(0, encoder_len+pred_len, 1)\n",
    "            windowed_tensor_li.append(d)\n",
    "        windowed_tensor = torch.concat(windowed_tensor_li)\n",
    "        return windowed_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sales_tensor.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sales = np.log1p(self.sales_tensor[idx][:-pred_len]).unsqueeze(-1)\n",
    "        y = np.log1p(self.sales_tensor[idx][-pred_len:])\n",
    "        \n",
    "        year = self.year_tensor[idx][:-pred_len].unsqueeze(-1)\n",
    "        year_y = self.year_tensor[idx][-pred_len:].unsqueeze(-1)\n",
    "\n",
    "        month = self.month_tensor[idx][:-pred_len].type(torch.IntTensor)\n",
    "        month_y = self.month_tensor[idx][-pred_len:].type(torch.IntTensor)\n",
    "\n",
    "        week = self.week_tensor[idx][:-pred_len].type(torch.IntTensor)\n",
    "        week_y = self.week_tensor[idx][-pred_len:].type(torch.IntTensor)\n",
    "\n",
    "        day = self.day_tensor[idx][:-pred_len].type(torch.IntTensor)\n",
    "        day_y = self.day_tensor[idx][-pred_len:].type(torch.IntTensor)\n",
    "\n",
    "        dayofweek = self.dayofweek_tensor[idx][:-pred_len].type(torch.IntTensor)\n",
    "        dayofweek_y = self.dayofweek_tensor[idx][-pred_len:].type(torch.IntTensor)\n",
    "\n",
    "        \n",
    "        return sales, year, month, week, day, dayofweek, y, year_y, month_y, week_y, day_y, dayofweek_y\n",
    "\n",
    "train_dataset = TrainDataset(df_train_prep)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, prefetch_factor=8)\n",
    "for data in train_dataloader:\n",
    "    sales, year, month, week, day, dayofweek, y, year_y, month_y, week_y, day_y, dayofweek_y = data\n",
    "    print(\"sales:\", sales.shape)\n",
    "    print(\"year:\", year.shape)\n",
    "    print(\"month:\", month.shape)\n",
    "    print(\"week:\", week.shape)\n",
    "    print(\"day:\", day.shape)\n",
    "    print(\"dayofweek:\", dayofweek.shape)\n",
    "    print(\"_\"*100)\n",
    "    print(\"y:\", y.shape)\n",
    "    print(\"year_y:\", year_y.shape)\n",
    "    print(\"month_y:\", month_y.shape)\n",
    "    print(\"week_y:\", week_y.shape)\n",
    "    print(\"day_y:\", day_y.shape)\n",
    "    print(\"dayofweek_y:\", dayofweek_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Transformer(\n",
       "    (enc_positional_embedder): PositionalEmbedder(\n",
       "      (year_embedder): Linear(in_features=1, out_features=512, bias=True)\n",
       "      (month_embedder): Embedding(12, 512)\n",
       "      (week_embedder): Embedding(52, 512)\n",
       "      (day_embedder): Embedding(365, 512)\n",
       "      (dayofweek_embedder): Embedding(7, 512)\n",
       "      (layer_norm1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation1): ELU(alpha=1.0)\n",
       "      (linear3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (sales_embedder): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (encoder): Encoder()\n",
       "    (decoder): Decoder()\n",
       "    (dec_positional_embedder): PositionalEmbedder(\n",
       "      (year_embedder): Linear(in_features=1, out_features=512, bias=True)\n",
       "      (month_embedder): Embedding(12, 512)\n",
       "      (week_embedder): Embedding(52, 512)\n",
       "      (day_embedder): Embedding(365, 512)\n",
       "      (dayofweek_embedder): Embedding(7, 512)\n",
       "      (layer_norm1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation1): ELU(alpha=1.0)\n",
       "      (linear3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (ffn): FeedForward(\n",
       "      (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (activation): ELU(alpha=1.0)\n",
       "      (dropout1): Dropout(p=0.3, inplace=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=1, bias=True)\n",
       "      (dropout2): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class PositionalEmbedder(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # Raw embedder\n",
    "        self.year_embedder = torch.nn.Linear(1, d_model)\n",
    "        self.month_embedder = torch.nn.Embedding(num_embeddings=12, embedding_dim=d_model)\n",
    "        self.week_embedder = torch.nn.Embedding(num_embeddings=52, embedding_dim=d_model)\n",
    "        self.day_embedder = torch.nn.Embedding(num_embeddings=365, embedding_dim=d_model)\n",
    "        self.dayofweek_embedder = torch.nn.Embedding(num_embeddings=7, embedding_dim=d_model)\n",
    "        \n",
    "        # Concatenate\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(d_model*5)\n",
    "        self.linear1 = torch.nn.Linear(d_model*5, d_model)\n",
    "\n",
    "        # Feed forward concatenated positions\n",
    "        self.linear2 = torch.nn.Linear(d_model, d_model)\n",
    "        self.activation1 = torch.nn.ELU()\n",
    "        self.linear3 = torch.nn.Linear(d_model, d_model)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Residual\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, year, month, week, dayofweek, day):\n",
    "        year_embedding = self.year_embedder(year)\n",
    "        month_embedding = self.month_embedder(month)\n",
    "        week_embedding = self.week_embedder(week)\n",
    "        day_embedding = self.day_embedder(day)\n",
    "        dayofweek_embedding = self.day_embedder(dayofweek)\n",
    "\n",
    "        concat = self.linear1(self.layer_norm1(torch.concat([year_embedding, month_embedding, week_embedding, day_embedding, dayofweek_embedding], dim=-1)))\n",
    "        feed_forward = self.dropout(self.linear3(self.activation1(self.linear2(concat))))\n",
    "        residual = self.layer_norm2(concat + feed_forward)\n",
    "        return residual\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout, d_ff):\n",
    "        super().__init__()\n",
    "        # Self attention\n",
    "        self.mha = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add and norm\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed forward\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.activation = torch.nn.ELU()\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout3 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add and norm\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, positional_embedding):\n",
    "        # Self attention\n",
    "        self_attn = self.mha(\n",
    "                    query = x + positional_embedding,\n",
    "                    key = x + positional_embedding,\n",
    "                    value = x,\n",
    "                    need_weights = False\n",
    "                    )[0]\n",
    "        self_attn = self.dropout1(self_attn)\n",
    "\n",
    "        # Add and norm\n",
    "        add_norm1 = self.layer_norm1(x + self_attn)\n",
    "        \n",
    "        # Feed forward\n",
    "        feed_forward = self.linear2(self.dropout2(self.activation(self.linear1(add_norm1))))\n",
    "        feed_forward = self.dropout3(feed_forward)\n",
    "\n",
    "        # Add and norm\n",
    "        add_norm2 = self.layer_norm2(add_norm1 + feed_forward)\n",
    "\n",
    "        return add_norm2\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder_block, num_layers):\n",
    "        super().__init__()\n",
    "        # self.layers = [torch.nn.DataParallel(copy.deepcopy(encoder_block).to(device)) for _ in range(num_layers)]\n",
    "        self.layers = [torch.nn.DataParallel(copy.deepcopy(encoder_block).cuda()) for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, x, positional_embedding):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, positional_embedding)\n",
    "        return output\n",
    "\n",
    "class HistDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        # Self attention\n",
    "        self.self_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add and norm\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        # Cross attention\n",
    "        self.cross_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add and norm\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        # Feed forward\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.activation = torch.nn.ELU()\n",
    "        self.dropout3 = torch.nn.Dropout(dropout)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout4 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Add and norm\n",
    "        self.layer_norm3 = torch.nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, decoder_input, dec_positional_embedding, encoder_output, enc_positional_embedding):\n",
    "        # Self attention\n",
    "        self_attn = self.self_attn(\n",
    "                    query = decoder_input + dec_positional_embedding,\n",
    "                    key = decoder_input + dec_positional_embedding,\n",
    "                    value = decoder_input,\n",
    "                    need_weights = False\n",
    "                    )[0]\n",
    "        self_attn = self.dropout1(self_attn)\n",
    "        \n",
    "        # Add and norm\n",
    "        layer_norm1 = self.layer_norm1(self_attn + decoder_input)\n",
    "\n",
    "        # Cross attention\n",
    "        cross_attn, attn_weight = self.cross_attn(\n",
    "            query = layer_norm1 + dec_positional_embedding,\n",
    "            key = encoder_output + enc_positional_embedding,\n",
    "            value = encoder_output,\n",
    "            need_weights = True\n",
    "        )\n",
    "        cross_attn = self.dropout2(cross_attn)\n",
    "\n",
    "        # Add and norm\n",
    "        layer_norm2 = self.layer_norm2(cross_attn + layer_norm1)\n",
    "\n",
    "        # Feed forward\n",
    "        feed_forward = self.linear2(self.dropout3(self.activation(self.linear1(layer_norm2))))\n",
    "        feed_forward = self.dropout4(feed_forward)\n",
    "        \n",
    "        # Add and norm\n",
    "        layer_norm3 = self.layer_norm3(feed_forward + layer_norm2)\n",
    "\n",
    "        return layer_norm3, attn_weight\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, decoder_block, num_layers):\n",
    "        super().__init__()\n",
    "        # self.layers = [copy.deepcopy(decoder_block).to(device) for _ in range(num_layers)]\n",
    "        self.layers = [copy.deepcopy(decoder_block).cuda() for _ in range(num_layers)]\n",
    "    \n",
    "    def forward(self, dec_positional_embedding, encoder_output, enc_positional_embedding):\n",
    "        output = dec_positional_embedding\n",
    "        for layer in self.layers:\n",
    "            output, attn_weight = layer(output, dec_positional_embedding, encoder_output, enc_positional_embedding)\n",
    "        return output, attn_weight\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.activation = torch.nn.ELU()\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, 1)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.linear2(self.dropout1(self.activation(self.linear1(x)))))\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc_positional_embedder = PositionalEmbedder(d_model)\n",
    "        self.sales_embedder = torch.nn.Linear(1, d_model)\n",
    "        self.encoder = Encoder(EncoderLayer(d_model, nhead, dropout, d_ff), num_layers)\n",
    "        self.decoder = Decoder(HistDecoderLayer(d_model, nhead, dropout), num_layers)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_positional_embedder = PositionalEmbedder(d_model)\n",
    "\n",
    "        # Output\n",
    "        self.ffn = FeedForward()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, sales, year, month, week, day, dayofweek, year_y, month_y, week_y, day_y, dayofweek_y):\n",
    "        # Encoder\n",
    "        enc_positional_embedding = self.enc_positional_embedder(year, month, week, dayofweek, day)\n",
    "        sales = self.sales_embedder(sales)\n",
    "        encoder_output = self.encoder(sales, enc_positional_embedding)\n",
    "\n",
    "        # Decoder\n",
    "        dec_positional_embedding = self.dec_positional_embedder(year_y, month_y, week_y, dayofweek_y, day_y)\n",
    "        dec_output, atten_weight = self.decoder(dec_positional_embedding, encoder_output, enc_positional_embedding)\n",
    "\n",
    "        output = self.ffn(dec_output)\n",
    "        sigmoid_output = self.sigmoid(output)\n",
    "        \n",
    "        return sigmoid_output, output, atten_weight\n",
    "\n",
    "model = Transformer(d_model, nhead, dropout)\n",
    "model = torch.nn.DataParallel(model)\n",
    "# model.to(device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4675 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 199, in forward\n    dec_output, atten_weight = self.decoder(dec_positional_embedding, encoder_output, enc_positional_embedding)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 160, in forward\n    output, attn_weight = layer(output, dec_positional_embedding, encoder_output, enc_positional_embedding)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 119, in forward\n    self_attn = self.self_attn(\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/functional.py\", line 4787, in _in_projection_packed\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     train()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# sigmoid_pred, pred, atten_weight = model(sales.to(device), \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#                             year.to(device), \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#                             month.to(device), \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m#                             day_y.to(device), \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m#                             dayofweek_y.to(device))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m sigmoid_pred, pred, atten_weight \u001b[39m=\u001b[39m model(sales\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m                             year\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m                             month\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m                             week\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m                             day\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m                             dayofweek\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m                             year_y\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m                             month_y\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m                             week_y\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m                             day_y\u001b[39m.\u001b[39;49mcuda(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m                             dayofweek_y\u001b[39m.\u001b[39;49mcuda())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m sigmoid_pred \u001b[39m=\u001b[39m sigmoid_pred\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[39m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         output\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m     90\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 199, in forward\n    dec_output, atten_weight = self.decoder(dec_positional_embedding, encoder_output, enc_positional_embedding)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 160, in forward\n    output, attn_weight = layer(output, dec_positional_embedding, encoder_output, enc_positional_embedding)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_1473937/1340636551.py\", line 119, in forward\n    self_attn = self.self_attn(\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 1205, in forward\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/functional.py\", line 5224, in multi_head_attention_forward\n    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/functional.py\", line 4787, in _in_projection_packed\n    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "binary_loss_fn_ = torch.nn.BCELoss()\n",
    "mse_loss_fn_ = torch.nn.MSELoss()\n",
    "temp = None\n",
    "\n",
    "def binary_loss_fn(pred, y):\n",
    "    # y = torch.where(y==0, 0, 1).type(torch.FloatTensor).to(device)\n",
    "    y = torch.where(y==0, 0, 1).type(torch.FloatTensor).cuda()\n",
    "    loss = binary_loss_fn_(pred, y)\n",
    "    return loss\n",
    "\n",
    "def mse_loss_fn(pred, y):\n",
    "    # mask = torch.where(y==0, 0, 1).type(torch.FloatTensor).to(device)\n",
    "    mask = torch.where(y==0, 0, 1).type(torch.FloatTensor).cuda()\n",
    "    pred = pred * mask\n",
    "    # y = y.to(device) * mask\n",
    "    y = y.cuda() * mask\n",
    "    loss = mse_loss_fn_(pred, y)\n",
    "    return loss\n",
    "\n",
    "def plot_sample(y, pred, total_loss_li, binary_loss_li, mse_loss_li):\n",
    "    # Plot sample\n",
    "    clear_output(wait=True)\n",
    "    y_sample = y[-1]\n",
    "    pred_sample = pred[-1].detach().cpu()\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(total_loss_li, label=\"Total\", linewidth=3)\n",
    "    plt.twinx()\n",
    "    plt.plot(binary_loss_li, label=\"Binary\", color=\"orange\")\n",
    "    plt.plot(mse_loss_li, label=\"MSE\", color=\"green\")\n",
    "    \n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(np.expm1(y_sample), label=\"y\")\n",
    "    plt.plot(np.expm1(pred_sample), label=\"pred\")\n",
    "    plt.title(\"Sample\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss_li, binary_loss_li, mse_loss_li = [], [], []\n",
    "    mean_total_loss_li, mean_binary_loss_li, mean_mse_loss_li = [], [], []\n",
    "    pbar = tqdm(train_dataloader)\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        sales, year, month, week, day, dayofweek, y, year_y, month_y, week_y, day_y, dayofweek_y = data\n",
    "        \n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        # sigmoid_pred, pred, atten_weight = model(sales.to(device), \n",
    "        #                             year.to(device), \n",
    "        #                             month.to(device), \n",
    "        #                             week.to(device), \n",
    "        #                             day.to(device), \n",
    "        #                             dayofweek.to(device), \n",
    "        #                             year_y.to(device), \n",
    "        #                             month_y.to(device), \n",
    "        #                             week_y.to(device), \n",
    "        #                             day_y.to(device), \n",
    "        #                             dayofweek_y.to(device))\n",
    "        sigmoid_pred, pred, atten_weight = model(sales.cuda(), \n",
    "                                    year.cuda(), \n",
    "                                    month.cuda(), \n",
    "                                    week.cuda(), \n",
    "                                    day.cuda(), \n",
    "                                    dayofweek.cuda(), \n",
    "                                    year_y.cuda(), \n",
    "                                    month_y.cuda(), \n",
    "                                    week_y.cuda(), \n",
    "                                    day_y.cuda(), \n",
    "                                    dayofweek_y.cuda())\n",
    "        sigmoid_pred = sigmoid_pred.squeeze(-1)\n",
    "        pred = pred.squeeze(-1)\n",
    "\n",
    "        binary_loss = binary_loss_fn(sigmoid_pred, y)\n",
    "        mse_loss = mse_loss_fn(pred, y)\n",
    "        loss = binary_loss + mse_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        total_loss_li.append(loss.item())\n",
    "        binary_loss_li.append(binary_loss.item())\n",
    "        mse_loss_li.append(mse_loss.item())\n",
    "        pbar.set_description(f\"{np.round(sum(total_loss_li[-100:])/100, 5)} â†’ binary: {np.round(sum(binary_loss_li[-100:])/100, 5)}, mse: {np.round(sum(mse_loss_li[-100:])/100, 5)}\")\n",
    "        if n % 10 == 0:\n",
    "            mean_total_loss_li.append(sum(total_loss_li[-100:])/100)\n",
    "            mean_binary_loss_li.append(sum(binary_loss_li[-100:])/100)\n",
    "            mean_mse_loss_li.append(sum(mse_loss_li[-100:])/100)\n",
    "            \n",
    "            calc_pred = torch.where(sigmoid_pred>0.5, 1, 0) * pred\n",
    "            plot_sample(y, calc_pred, mean_total_loss_li[-10:], mean_binary_loss_li[-10:], mean_mse_loss_li[-10:])\n",
    "\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for data in train_dataloader:\n",
    "    sales, year, month, week, day, dayofweek, y, year_y, month_y, week_y, day_y, dayofweek_y = data\n",
    "    \n",
    "    # Train\n",
    "    with torch.no_grad():\n",
    "        sigmoid_pred, pred, atten_weight = model(sales.to(device), \n",
    "                                    year.to(device), \n",
    "                                    month.to(device), \n",
    "                                    week.to(device), \n",
    "                                    day.to(device), \n",
    "                                    dayofweek.to(device), \n",
    "                                    year_y.to(device), \n",
    "                                    month_y.to(device), \n",
    "                                    week_y.to(device), \n",
    "                                    day_y.to(device), \n",
    "                                    dayofweek_y.to(device))\n",
    "        sigmoid_pred = sigmoid_pred.squeeze(-1)\n",
    "        pred = pred.squeeze(-1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atten_weight' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m arr \u001b[39m=\u001b[39m atten_weight[\u001b[39m10\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/implementation_v4_hnm.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(arr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'atten_weight' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "arr = atten_weight[10].cpu().numpy()\n",
    "sns.heatmap(arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
