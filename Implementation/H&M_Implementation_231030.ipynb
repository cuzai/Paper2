{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd; pd.set_option(\"display.max_columns\", None)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import SwinModel\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General params\n",
    "random_state = 0\n",
    "\n",
    "# Dataset params\n",
    "is_data_exist = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_data_exist:\n",
    "    df_raw = pd.read_csv(\"../HnM/transactions_train.csv\", parse_dates=[\"t_dat\"], dtype={\"article_id\":str})\n",
    "    df_article = pd.read_csv(\"../HnM/articles.csv\", dtype={\"article_id\":str})\n",
    "    df_article = df_article[[\"article_id\"] + [col for col in df_article.columns if \"name\" in col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_data_exist:\n",
    "    df_prep = df_raw.copy()\n",
    "\n",
    "    # Make daily sales\n",
    "    df_prep1 = df_prep.copy()\n",
    "    df_prep1 = df_prep1.groupby([\"article_id\", \"t_dat\"], as_index=False).agg(sales=(\"customer_id\", \"count\"), price=(\"price\",\"mean\"))\n",
    "    df_prep1[\"avg\"] = df_prep1.groupby(\"article_id\")[\"sales\"].transform(\"mean\")\n",
    "\n",
    "    # Expand dates\n",
    "    def func(x):\n",
    "        article_id = x[\"article_id\"].iloc[0]\n",
    "\n",
    "        # Expand dates\n",
    "        min_date = x[\"t_dat\"].min()\n",
    "        max_date = x[\"t_dat\"].max()\n",
    "        date_ref = pd.DataFrame(pd.date_range(min_date, max_date, freq=\"d\"), columns=[\"t_dat\"])\n",
    "        x = pd.merge(x, date_ref, on=\"t_dat\", how=\"right\")\n",
    "\n",
    "        # Fill missing values\n",
    "        x[\"article_id\"] = x[\"article_id\"].fillna(article_id)\n",
    "        x[\"sales\"] = x[\"sales\"].fillna(0)\n",
    "        x[\"price\"] = x[\"price\"].fillna(method=\"ffill\")\n",
    "        return x\n",
    "    df_prep2 = df_prep1.copy()\n",
    "    df_prep2 = df_prep2.groupby(\"article_id\").apply(lambda x: func(x)).reset_index(drop=True)\n",
    "\n",
    "    # Make week column\n",
    "    df_prep3 = df_prep2.copy()\n",
    "    df_prep3[\"year\"] = df_prep3[\"t_dat\"].dt.year\n",
    "    df_prep3[\"month\"] = df_prep3[\"t_dat\"].dt.month.astype(str).str.zfill(2)\n",
    "    df_prep3[\"week\"] = df_prep3[\"t_dat\"].dt.isocalendar().week.astype(str).str.zfill(2)\n",
    "    df_prep3[\"year\"] = df_prep3.apply(lambda x: x[\"year\"]+1 if x[\"month\"]==\"12\" and x[\"week\"]==\"01\" \n",
    "                                               else (x[\"year\"]-1 if x[\"month\"]==\"01\" and x[\"week\"]!=\"01\" else x[\"year\"])\n",
    "                                                , axis=1).astype(str)\n",
    "    df_prep3[\"week_id\"] = (df_prep3[\"year\"] + df_prep3[\"week\"]).astype(int)\n",
    "\n",
    "    # Aggregate by week\n",
    "    df_prep4 = df_prep3.copy()\n",
    "    df_prep4 = df_prep4.groupby([\"article_id\", \"week_id\"], as_index=False).agg({\"sales\":\"sum\", \"price\":\"mean\", \"week\":\"max\"})\n",
    "    df_prep4 = df_prep4.sort_values([\"article_id\", \"week_id\"])\n",
    "    df_prep4[\"sales\"] = df_prep4[\"sales\"].apply(lambda x: 1e-5 if x == 0 else x)\n",
    "\n",
    "    # Group by article_id and make the data as lists\n",
    "    df_prep5 = df_prep4.copy()\n",
    "    df_sales = df_prep5.groupby(\"article_id\", as_index=False)[\"sales\"].apply(np.array)\n",
    "    df_price = df_prep5.groupby(\"article_id\", as_index=False)[\"price\"].apply(np.array)\n",
    "    df_week = df_prep5.groupby(\"article_id\", as_index=False)[\"week\"].apply(np.array)\n",
    "\n",
    "    df_prep5 = pd.merge(df_sales, df_price, on=\"article_id\")\n",
    "    df_prep5 = pd.merge(df_prep5, df_week, on=\"article_id\")\n",
    "\n",
    "    # Generate image path\n",
    "    df_prep6 = df_prep5.copy()\n",
    "    df_prep6[\"img_path\"] = df_prep6[\"article_id\"].apply(lambda x: f'../HnM/images/{x[:3]}/{x}.jpg') # Generate image path\n",
    "    df_prep6[\"is_valid\"] = df_prep6[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0) # Check whether the article has corresponding image file\n",
    "    df_prep6 = df_prep6[df_prep6[\"is_valid\"]==1].drop(\"is_valid\", axis=1)\n",
    "\n",
    "    # Join with article info\n",
    "    df_prep7 = df_prep6.copy()\n",
    "    df_prep7 = pd.merge(df_prep7, df_article, on=\"article_id\", how=\"left\")\n",
    "\n",
    "    # Calculate average\n",
    "    df_prep8 = df_prep7.copy()\n",
    "    df_prep8[\"avg_abv1\"] = df_prep8[\"sales\"].apply(lambda x: np.mean([i for i in x if i>1]))\n",
    "    df_prep8[\"avg\"] = df_prep8[\"sales\"].apply(lambda x: np.mean([i for i in x]))\n",
    "\n",
    "    df_prep8.to_parquet(\"./df_prep.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_prep.shape: (104106, 19)\n"
     ]
    }
   ],
   "source": [
    "df_read = pd.read_parquet(\"./df_prep.parquet\"); print(f\"df_prep.shape: {df_read.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_prep size>52 shape: (27791, 20)\n",
      "df_prep avg>50 shape: (7228, 20)\n",
      "df_train.shape: (5059, 20)\n",
      "df_valid.shape: (2169, 20)\n"
     ]
    }
   ],
   "source": [
    "df_prep = df_read.copy()\n",
    "# Filter\n",
    "df_prep[\"size\"] = df_prep[\"sales\"].str.len()\n",
    "df_prep = df_prep[df_prep[\"size\"] > 52]; print(f\"df_prep size>52 shape: {df_prep.shape}\")\n",
    "df_prep = df_prep[df_prep[\"avg\"] > 10]; print(f\"df_prep avg>50 shape: {df_prep.shape}\")\n",
    "\n",
    "# Split train valid\n",
    "df_train, df_valid = train_test_split(df_prep, test_size=0.3, random_state=random_state)\n",
    "print(f\"df_train.shape: {df_train.shape}\"); print(f\"df_valid.shape: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>price</th>\n",
       "      <th>week</th>\n",
       "      <th>img_path</th>\n",
       "      <th>prod_name</th>\n",
       "      <th>product_type_name</th>\n",
       "      <th>product_group_name</th>\n",
       "      <th>graphical_appearance_name</th>\n",
       "      <th>colour_group_name</th>\n",
       "      <th>perceived_colour_value_name</th>\n",
       "      <th>perceived_colour_master_name</th>\n",
       "      <th>department_name</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_group_name</th>\n",
       "      <th>section_name</th>\n",
       "      <th>garment_group_name</th>\n",
       "      <th>avg_abv1</th>\n",
       "      <th>avg</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>0417951001</td>\n",
       "      <td>[19.0, 14.0, 14.0, 11.0, 9.0, 40.0, 27.0, 42.0...</td>\n",
       "      <td>[0.013228046811945076, 0.013291796379568744, 0...</td>\n",
       "      <td>[02, 03, 04, 05, 38, 39, 40, 41, 42, 43, 44, 4...</td>\n",
       "      <td>../HnM/images/041/0417951001.jpg</td>\n",
       "      <td>Support 20 den 1p tights</td>\n",
       "      <td>Underwear Tights</td>\n",
       "      <td>Socks &amp; Tights</td>\n",
       "      <td>Solid</td>\n",
       "      <td>Beige</td>\n",
       "      <td>Medium Dusty</td>\n",
       "      <td>Beige</td>\n",
       "      <td>Tights basic</td>\n",
       "      <td>Lingeries/Tights</td>\n",
       "      <td>Ladieswear</td>\n",
       "      <td>Womens Nightwear, Socks &amp; Tigh</td>\n",
       "      <td>Socks and Tights</td>\n",
       "      <td>24.196262</td>\n",
       "      <td>24.196262</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                                              sales  \\\n",
       "2600  0417951001  [19.0, 14.0, 14.0, 11.0, 9.0, 40.0, 27.0, 42.0...   \n",
       "\n",
       "                                                  price  \\\n",
       "2600  [0.013228046811945076, 0.013291796379568744, 0...   \n",
       "\n",
       "                                                   week  \\\n",
       "2600  [02, 03, 04, 05, 38, 39, 40, 41, 42, 43, 44, 4...   \n",
       "\n",
       "                              img_path                 prod_name  \\\n",
       "2600  ../HnM/images/041/0417951001.jpg  Support 20 den 1p tights   \n",
       "\n",
       "     product_type_name product_group_name graphical_appearance_name  \\\n",
       "2600  Underwear Tights     Socks & Tights                     Solid   \n",
       "\n",
       "     colour_group_name perceived_colour_value_name  \\\n",
       "2600             Beige                Medium Dusty   \n",
       "\n",
       "     perceived_colour_master_name department_name        index_name  \\\n",
       "2600                        Beige    Tights basic  Lingeries/Tights   \n",
       "\n",
       "     index_group_name                    section_name garment_group_name  \\\n",
       "2600       Ladieswear  Womens Nightwear, Socks & Tigh   Socks and Tights   \n",
       "\n",
       "       avg_abv1        avg  size  \n",
       "2600  24.196262  24.196262   107  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1639165/3081782684.py:6: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  sales = torch.FloatTensor(self.data[\"sales\"].values[idx][:-12]).unsqueeze(-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sales = torch.FloatTensor(self.data[\"sales\"].values[idx][:-12]).unsqueeze(-1)\n",
    "        y = torch.FloatTensor(self.data[\"sales\"].values[idx][-12:]).unsqueeze(-1)\n",
    "\n",
    "        img_path = self.data[\"img_path\"].values[idx]\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) # Transform image based on ImageNet standard\n",
    "\n",
    "        img = transform(Image.open(img_path).convert(\"RGB\")) # Transform an image\n",
    "\n",
    "        return sales, img, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def func(data):\n",
    "    sales = [sales for sales, img, y in data]\n",
    "    img = [img for sales, img, y in data]\n",
    "    y = [y for sales, img, y in data]\n",
    "    \n",
    "    sales = torch.nn.utils.rnn.pad_sequence(sales, padding_value=0, batch_first=True)\n",
    "    img = torch.nn.utils.rnn.pad_sequence(img, padding_value=0, batch_first=True)\n",
    "    y = torch.nn.utils.rnn.pad_sequence(y, padding_value=0, batch_first=True)\n",
    "    return sales, img, y\n",
    "    \n",
    "train_dataset = Dataset(df_train)\n",
    "valid_dataset = Dataset(df_valid)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=lambda x: func(x))\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: func(x))\n",
    "next(iter(train_dataloader))[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return embedded * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # PE(pos, 2i) = sin(pos/10000^{2i/d_model}), \n",
    "    # PE(pos, 2i+1) = cos(pos/10000^{2i/d_model})\n",
    "    def __init__(self, max_len, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).reshape(-1,1).to(device)\n",
    "        i = torch.arange(d_model).to(device)//2\n",
    "        exp_term = 2*i/d_model\n",
    "        div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "        self.pos_encoded = position / div_term\n",
    "\n",
    "        self.pos_encoded[:, 0::2] = torch.sin(self.pos_encoded[:, 0::2])\n",
    "        self.pos_encoded[:, 1::2] = torch.cos(self.pos_encoded[:, 1::2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.pos_encoded[:x.shape[1], :]\n",
    "        return self.dropout(output)\n",
    "\n",
    "class Mask(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_padding_mask(self, arr):\n",
    "        res = torch.eq(arr, 0).type(torch.FloatTensor).to(device)\n",
    "        res = torch.where(res==1, -torch.inf, 0)\n",
    "        return res\n",
    "    \n",
    "    def get_lookahead_mask(self, arr):\n",
    "        seq_len = arr.shape[1]\n",
    "        mask = torch.triu(torch.ones((seq_len, seq_len))*-torch.inf, 1).to(device)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, arr):\n",
    "        padding_mask = self.get_padding_mask(arr)\n",
    "        lookahead_mask = self.get_lookahead_mask(arr)\n",
    "        return padding_mask, lookahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, d_model, input_size, output_size, max_seq_len, nhead, num_layers, d_ff=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.enc_mask = Mask()\n",
    "        self.enc_linear_embedding = torch.nn.Linear(input_size, d_model)\n",
    "        self.enc_pos_encoding = PositionalEncoding(max_seq_len, d_model, dropout)\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "\n",
    "        self.dec_mask = Mask()\n",
    "        self.swin_transformer = SwinModel.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\") # Get pre-trained SwinTransformer\n",
    "        self.swin_linear = torch.nn.Linear(self.swin_transformer.config.hidden_size, d_model)\n",
    "        self.decoder = torch.nn.TransformerDecoder(torch.nn.TransformerDecoderLayer(d_model, nhead, d_ff, dropout, batch_first=True), num_layers)\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(d_model*49, d_model)\n",
    "        self.linear2 = torch.nn.Linear(d_model, 11) # 11 is pred_length\n",
    "        \n",
    "    def forward(self, enc_sales, dec_input):\n",
    "        enc_padding_mask, _ = self.enc_mask(enc_sales.squeeze())\n",
    "        enc_output = torch.nn.ReLU()(self.enc_linear_embedding(enc_sales))\n",
    "\n",
    "        enc_output = self.enc_pos_encoding(enc_output)\n",
    "        enc_output = self.encoder(enc_output, src_key_padding_mask=enc_padding_mask)\n",
    "\n",
    "        dec_output = self.swin_transformer(dec_input).last_hidden_state\n",
    "        dec_output = torch.nn.ReLU()(self.swin_linear(dec_output))\n",
    "        dec_output = self.decoder(tgt=dec_output, memory=enc_output)\n",
    "\n",
    "        dec_output = torch.nn.Flatten()(dec_output)\n",
    "        dec_output = torch.nn.ReLU()(self.linear1(dec_output))\n",
    "        dec_output = torch.nn.ReLU()(self.linear2(dec_output))\n",
    "\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:20/158 mean_loss: 394.18178957984563"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/H&M_Implementation_231030.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m train_loss_li, val_loss_li \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(e) \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m val()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     train_loss_li\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/H&M_Implementation_231030.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m total_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m n, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     sales, img, y \u001b[39m=\u001b[39m data\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Train\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/Implementation/H&M_Implementation_231030.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mimg_path\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m ]) \u001b[39m# Transform image based on ImageNet standard\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m img \u001b[39m=\u001b[39m transform(Image\u001b[39m.\u001b[39;49mopen(img_path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m\"\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m\"\u001b[39;49m)) \u001b[39m# Transform an image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/Implementation/H%26M_Implementation_231030.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sales, img, y\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39;49mnormalize(tensor, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, inplace\u001b[39m=\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[39mif\u001b[39;00m std\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    927\u001b[0m     std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 928\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49msub_(mean)\u001b[39m.\u001b[39mdiv_(std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train valid compare\n",
    "model = Transformer(d_model=128, input_size=1, output_size=1, max_seq_len=150, nhead=4, num_layers=4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_len = len(train_dataloader)\n",
    "    total_loss = 0\n",
    "    for n, data in enumerate(train_dataloader):\n",
    "        sales, img, y = data\n",
    "\n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(sales.to(device), img.to(device))\n",
    "        loss = loss_fn(pred, y[:, 1:].squeeze().to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss / (n+1)\n",
    "        print(f\"\\r {epoch}:{n}/{total_len} mean_loss: {mean_loss}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    return mean_loss\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    total_len = len(valid_dataloader)\n",
    "    total_loss = 0\n",
    "    for n, data in enumerate(valid_dataloader):\n",
    "        sales, img, y = data\n",
    "        \n",
    "        # Pred\n",
    "        with torch.no_grad():\n",
    "            pred = model(sales.to(device), img.to(device))\n",
    "            loss = loss_fn(pred, y[:, 1:].squeeze().to(device))\n",
    "\n",
    "            # Report\n",
    "            total_loss += loss.item()\n",
    "            mean_loss = total_loss / (n+1)\n",
    "            return mean_loss\n",
    " \n",
    "def plot(train_loss_li, val_loss_li):\n",
    "    # Plot loss\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss_li, label=\"train\")\n",
    "    plt.plot(val_loss_li, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "\n",
    "epoch = 100\n",
    "train_loss_li, val_loss_li = [], []\n",
    "for e in range(epoch):\n",
    "    train_loss = train(e) # Train\n",
    "    val_loss = val()\n",
    "\n",
    "    train_loss_li.append(train_loss)\n",
    "    val_loss_li.append(val_loss)\n",
    "    plot(train_loss_li, val_loss_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
