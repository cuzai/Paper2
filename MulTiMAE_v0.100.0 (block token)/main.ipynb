{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7],\n",
       "        [1, 2, 3, 4, 5, 6, 7],\n",
       "        [0, 2, 3, 4, 5, 6, 7],\n",
       "        [0, 2, 3, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Reciporal gather\n",
    "arr = torch.arange(0, 8).unsqueeze(0).repeat(4, 1)\n",
    "idx = torch.tensor([0,0,1,1]).unsqueeze(-1)\n",
    "print(arr.shape)\n",
    "print(idx.shape)\n",
    "\n",
    "m = torch.ones(arr.shape).scatter(1, idx, 0)\n",
    "m = m.nonzero()[:, 1].reshape(-1, arr.size(1) - idx.size(1))\n",
    "\n",
    "torch.gather(arr, index=m, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from transformers import ViTModel, AutoImageProcessor, BertModel, AutoTokenizer\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "from architecture import Transformer\n",
    "from architecture_detail import *\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 10\n",
    "MAX_SEQ_LEN = 50\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"],\n",
    "    \"nlp\": [\"detail_desc\"]\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"],\n",
    "    \"nlp_cols\": [\"detail_desc\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 32\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "patch_size = 16\n",
    "\n",
    "d_model = {\"encoder\":256, \"decoder\":128}\n",
    "d_ff = {\"encoder\":256, \"decoder\":128}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"target\":0.25, \"temporal\":0.25, \"img\":0.25, \"nlp\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_train = df_train[~pd.isna(df_train[\"detail_desc\"])]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "df_valid = df_valid[~pd.isna(df_valid[\"detail_desc\"])]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2799.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 50, 1])\n",
      "day torch.Size([2, 50])\n",
      "dow torch.Size([2, 50])\n",
      "month torch.Size([2, 50])\n",
      "holiday torch.Size([2, 50])\n",
      "price torch.Size([2, 50, 1])\n",
      "target_fcst_mask torch.Size([2, 50, 1])\n",
      "temporal_padding_mask torch.Size([2, 50])\n",
      "img_path torch.Size([2, 3, 224, 224])\n",
      "detail_desc torch.Size([2, 9])\n",
      "detail_desc_remain_idx torch.Size([2, 2])\n",
      "detail_desc_masked_idx torch.Size([2, 6])\n",
      "detail_desc_revert_idx torch.Size([2, 8])\n",
      "detail_desc_remain_padding_mask torch.Size([2, 2])\n",
      "detail_desc_masked_padding_mask torch.Size([2, 6])\n",
      "detail_desc_revert_padding_mask torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n",
      "____________________________________________________________________________________________________\n",
      "tensor([[2, 4, 0, 3, 1],\n",
      "        [0, 2, 3, 4, 1],\n",
      "        [4, 2, 0, 1, 3],\n",
      "        [2, 1, 0, 4, 3],\n",
      "        [0, 4, 3, 2, 1]])\n",
      "____________________________________________________________________________________________________\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "arr = torch.arange(0, 5).unsqueeze(0).repeat(5, 1)\n",
    "print(arr); print(\"_\"*100)\n",
    "\n",
    "noise = torch.rand(5,5)\n",
    "shuffle_idx = torch.argsort(noise, dim=-1)\n",
    "revert_idx = torch.argsort(shuffle_idx, dim=-1)\n",
    "\n",
    "remain = torch.gather(arr, index=shuffle_idx, dim=-1)\n",
    "print(remain); print(\"_\"*100)\n",
    "\n",
    "revert = torch.gather(remain, index=revert_idx, dim=-1)\n",
    "print(revert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiheadBlockSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, -1, self.nhead, d_model//self.nhead).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.nhead, d_model//self.nhead).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.nhead, d_model//self.nhead).permute(0, 2, 1, 3)\n",
    "\n",
    "        temporal_attn_output, temporal_attn_weight = self.temporal_side_attention(Q, K, V, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask)\n",
    "        static_attn_output, static_attn_weight = self.static_side_attention(Q, K, V, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask)\n",
    "\n",
    "        # Concat all\n",
    "        temporal_attn_output = temporal_attn_output.view(temporal_attn_output.shape[0], -1, temporal_attn_output.shape[-1])\n",
    "        attn_output = torch.cat([temporal_attn_output, static_attn_output], dim=1)\n",
    "\n",
    "        return attn_output\n",
    "    \n",
    "    def temporal_side_attention(self, Q, K, V, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask):\n",
    "        # Obtain QKV\n",
    "        batch_size, seq_len, num_modality, _ = temporal_block_shape\n",
    "        temporal_unblock_idx = temporal_unblock_idx.unsqueeze(0).unsqueeze(1).unsqueeze(-1).repeat(Q.shape[0], Q.shape[1], 1, Q.shape[-1])\n",
    "        static_unblock_idx = static_unblock_idx.unsqueeze(0).unsqueeze(1).unsqueeze(-1).repeat(Q.shape[0], Q.shape[1], 1, Q.shape[-1])\n",
    "        \n",
    "        Qt = torch.gather(Q, index=temporal_unblock_idx, dim=-2).view(batch_size, Q.shape[1], seq_len, num_modality, Q.shape[-1])\n",
    "        Kt = torch.gather(K, index=temporal_unblock_idx, dim=-2).view(batch_size, K.shape[1], seq_len, num_modality, K.shape[-1])\n",
    "        Ks = torch.gather(K, index=static_unblock_idx, dim=-2).unsqueeze(-3)\n",
    "        Vt = torch.gather(V, index=temporal_unblock_idx, dim=-2).view(batch_size, Q.shape[1], seq_len, num_modality, Q.shape[-1])\n",
    "        Vs = torch.gather(V, index=static_unblock_idx, dim=-2).unsqueeze(-3)\n",
    "        \n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q @ K^t\n",
    "        QtKt = Qt @ Kt.permute(0,1,2,4,3)\n",
    "        QtKs = Qt @ Ks.permute(0,1,2,4,3)\n",
    "        QK = torch.cat([QtKt, QtKs], dim=-1)\n",
    "        logits = QK / math.sqrt(QK.shape[-1]//self.nhead)\n",
    "        \n",
    "        ### 2. Padding mask\n",
    "        static_unblock_padding_mask = static_unblock_padding_mask.unsqueeze(1).repeat(1, temporal_block_padding_mask.shape[1], 1)\n",
    "        key_padding_mask = torch.cat([temporal_block_padding_mask, static_unblock_padding_mask], dim=-1)\n",
    "        key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(-2).repeat(1, logits.shape[1], 1, logits.shape[-2], 1)\n",
    "        logits += key_padding_mask\n",
    "\n",
    "        ### 3. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        QtKt_attn_weight = attn_weight[:, :, :, :, :QtKt.shape[-1]]\n",
    "        QtKs_attn_weight = attn_weight[:, :, :, :, QtKt.shape[-1]:]\n",
    "\n",
    "        ### 4. Matmul V\n",
    "        QtKtVt = QtKt_attn_weight @ Vt\n",
    "        QtKsVs = QtKs_attn_weight @ Vs\n",
    "\n",
    "        temporal_attn_output = QtKtVt + QtKsVs\n",
    "        \n",
    "        ### 5. Concat heads\n",
    "        temporal_attn_output = temporal_attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, num_modality, -1)\n",
    "\n",
    "        return temporal_attn_output, attn_weight\n",
    "\n",
    "    def static_side_attention(self, Q, K, V, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask):\n",
    "        # Obtain QKV\n",
    "        batch_size, seq_len, num_modality, _ = temporal_block_shape\n",
    "        temporal_unblock_idx = temporal_unblock_idx.unsqueeze(0).unsqueeze(1).unsqueeze(-1).repeat(Q.shape[0], Q.shape[1], 1, Q.shape[-1])\n",
    "        static_unblock_idx = static_unblock_idx.unsqueeze(0).unsqueeze(1).unsqueeze(-1).repeat(Q.shape[0], Q.shape[1], 1, Q.shape[-1])\n",
    "\n",
    "        Qs = torch.gather(Q, index=static_unblock_idx, dim=-2)\n",
    "        \n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q @ K^t\n",
    "        QK = Qs @ K.permute(0,1,3,2)\n",
    "        logits = QK / math.sqrt(QK.shape[-1]//self.nhead)\n",
    "\n",
    "        ### 2. Padding mask\n",
    "        temporal_block_padding_mask = temporal_block_padding_mask.reshape(temporal_block_padding_mask.shape[0], -1)\n",
    "        key_padding_mask = torch.cat([temporal_block_padding_mask, static_unblock_padding_mask], dim=-1)\n",
    "        key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(-2).repeat(1, QK.shape[1], QK.shape[-2], 1)\n",
    "        logits += key_padding_mask\n",
    "\n",
    "        ### 3. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        ### 4. Matmul V\n",
    "        attn_output = attn_weight @ V\n",
    "        \n",
    "        ### 5. Concat heads\n",
    "        attn_output = attn_output.permute(0,2,1,3).reshape(batch_size, Qs.shape[-2], -1)\n",
    "\n",
    "        return attn_output, attn_weight\n",
    "\n",
    "\n",
    "class MultiheadBlockAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, padding_mask):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "        logits = QK / math.sqrt(d_model//self.nhead)\n",
    "        \n",
    "        ### #. Padding_mask\n",
    "        padding_mask = padding_mask.unsqueeze(1).unsqueeze(-2).repeat(1, logits.shape[1], 1, logits.shape[-2], 1)\n",
    "        logits += padding_mask\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn_weight @ V\n",
    "\n",
    "        ### 4. Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, -1, d_model)\n",
    "\n",
    "        return attn_output, attn_weight\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "        \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        embedding = self.embedding(val)\n",
    "        return embedding\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        embedding = self.img_model(val).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        return embedding\n",
    "\n",
    "class NlpEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.nlp_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)   \n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        # Make token_type_ids\n",
    "        token_type_ids = torch.zeros(val.shape).to(torch.int).to(device)\n",
    "\n",
    "        # Make attention mask\n",
    "        attention_mask = padding_mask_dict[f\"{key}_revert_padding_mask\"]\n",
    "        mask_for_global_token = torch.ones(attention_mask.shape[0], 1).to(device)\n",
    "        attention_mask = torch.cat([attention_mask, mask_for_global_token], dim=-1)\n",
    "\n",
    "        # Embed data\n",
    "        inputs = {\"input_ids\":val, \"token_type_ids\":token_type_ids, \"attention_mask\":attention_mask}\n",
    "        embedding = self.nlp_model(**inputs).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class TemporalRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, temporal_cols, device):\n",
    "        # Concat_data\n",
    "        concat_data_li, concat_padding_mask_li = [], []\n",
    "        for col in temporal_cols:\n",
    "            temporal_data = data_dict[col]\n",
    "            temporal_padding_mask = padding_mask_dict[\"temporal_padding_mask\"]\n",
    "            \n",
    "            concat_data_li.append(temporal_data)\n",
    "            concat_padding_mask_li.append(temporal_padding_mask)\n",
    "        \n",
    "        concat_data = torch.stack(concat_data_li, dim=-2) # Block shaped\n",
    "        concat_padding_mask = torch.stack(concat_padding_mask_li, dim=-1) # Block shaped with all 1s in terms of -1 dimension\n",
    "\n",
    "        # Split token and valid data\n",
    "        global_token_data = concat_data[:, :, :1, :]\n",
    "        global_token_padding_mask = concat_padding_mask[:, :, :1]\n",
    "\n",
    "        valid_data = concat_data[:, :, 1:, :]\n",
    "        valid_padding_mask = concat_padding_mask[:, :, 1:]\n",
    "\n",
    "        # Remain masking for valid data\n",
    "        num_modality = valid_data.shape[-2]\n",
    "        num_remain = int(num_modality * remain_rto)\n",
    "\n",
    "        noise = torch.rand(valid_data.shape[:-1]).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=-1)\n",
    "\n",
    "        remain_idx = shuffle_idx[:, :, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, :, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=-1)\n",
    "\n",
    "        # Apply mask\n",
    "        valid_remain_data = torch.gather(valid_data, index=remain_idx.unsqueeze(-1).repeat(1, 1, 1, valid_data.shape[-1]), dim=-2)\n",
    "        valid_remain_padding_mask = torch.gather(valid_padding_mask, index=remain_idx, dim=-1)\n",
    "\n",
    "        concat_remain_data = torch.cat([global_token_data, valid_remain_data], dim=-2)\n",
    "        concat_remain_padding_mask = torch.cat([global_token_padding_mask, valid_remain_padding_mask], dim=-1)\n",
    "        \n",
    "        # Obtain revert padding mask\n",
    "        revert_padding_mask = torch.ones(revert_idx.shape[0], revert_idx.shape[1], revert_idx.shape[2]+1).to(device)\n",
    "\n",
    "        # Finalize\n",
    "        result_dict = {\"temporal_remain\": concat_remain_data}\n",
    "        idx_dict.update({\"temporal_remain_idx\": remain_idx, \"temporal_masked_idx\":masked_idx, \"temporal_revert_idx\":revert_idx})\n",
    "        padding_mask_dict.update({\"temporal_remain_padding_mask\":concat_remain_padding_mask, \"temporal_revert_padding_mask\": revert_padding_mask})\n",
    "        return result_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class ImgRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, img_cols, device):\n",
    "        result_dict = {}\n",
    "        # Get indexs\n",
    "        for col in img_cols:\n",
    "            val = data_dict[col]\n",
    "            \n",
    "            # # Add global token\n",
    "            # global_token = self.global_token.unsqueeze(0).repeat(val.shape[0], 1, 1)\n",
    "            # val = torch.cat([global_token, val], dim=1)\n",
    "            \n",
    "            # Split global token\n",
    "            global_token = val[:, :1, :]\n",
    "            val = val[:, 1:, :]\n",
    "\n",
    "            # Apply remain\n",
    "            num_remain = int(val.shape[1] * remain_rto)\n",
    "            noise = torch.rand(val.shape[0], val.shape[1]).to(device)\n",
    "            shuffle_idx = torch.argsort(noise, dim=1)\n",
    "\n",
    "            remain_idx = shuffle_idx[:, :num_remain]\n",
    "            masked_idx = shuffle_idx[:, num_remain:]\n",
    "            revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "            remain_padding_mask = torch.ones(remain_idx.shape[0], remain_idx.shape[1]+1).to(device)\n",
    "            revert_padding_mask = torch.ones(revert_idx.shape[0], revert_idx.shape[1]+1).to(device)\n",
    "\n",
    "            # Apply mask\n",
    "            val = torch.gather(val, index=remain_idx.unsqueeze(-1).repeat(1, 1, val.shape[-1]), dim=1)\n",
    "            val = torch.cat([global_token, val], dim=1)\n",
    "\n",
    "            result_dict[col] = val\n",
    "            idx_dict.update({f\"{col}_remain_idx\":remain_idx, f\"{col}_masked_idx\":masked_idx, f\"{col}_revert_idx\":revert_idx})\n",
    "            padding_mask_dict.update({f\"{col}_remain_padding_mask\":remain_padding_mask, f\"{col}_revert_padding_mask\":revert_padding_mask})\n",
    "\n",
    "        return result_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class NlpRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, nlp_cols, device):\n",
    "        result_dict = {}\n",
    "        for col in nlp_cols:\n",
    "            val = data_dict[col]\n",
    "            \n",
    "            # Split global token\n",
    "            global_token = val[:, :1, :]\n",
    "            val = val[:, 1:, :]\n",
    "\n",
    "            # Apply remain mask\n",
    "            remain_idx = idx_dict[f\"{col}_remain_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            val = torch.gather(val, index=remain_idx, dim=1)\n",
    "            val = torch.cat([global_token, val], dim=1)\n",
    "            result_dict[col] = val\n",
    "\n",
    "            # Update padding_mask\n",
    "            padding_mask_for_global_token = torch.ones(val.shape[0], 1).to(device)\n",
    "\n",
    "            remain_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "            remain_padding_mask = torch.cat([padding_mask_for_global_token, remain_padding_mask], dim=-1)\n",
    "            padding_mask_dict[f\"{col}_remain_padding_mask\"] = remain_padding_mask\n",
    "\n",
    "            revert_padding_mask = padding_mask_dict[f\"{col}_revert_padding_mask\"]\n",
    "            revert_padding_mask = torch.cat([padding_mask_for_global_token, revert_padding_mask], dim=-1)\n",
    "            padding_mask_dict[f\"{col}_revert_padding_mask\"] = revert_padding_mask\n",
    "        return result_dict, padding_mask_dict\n",
    "\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadBlockSelfAttention(d_model, nhead, dropout)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src, temporal_block_shape, \n",
    "                        temporal_unblock_idx, static_unblock_idx,\n",
    "                        temporal_block_padding_mask, static_unblock_padding_mask):\n",
    "        x = src\n",
    "        attn_output = self._sa_block(self.norm1(x), temporal_block_shape,\n",
    "                                        temporal_unblock_idx, static_unblock_idx,\n",
    "                                        temporal_block_padding_mask, static_unblock_padding_mask)\n",
    "        x = x + attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "    def _sa_block(self, src, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask):\n",
    "        x = self.self_attn(src, src, src, temporal_block_shape, temporal_unblock_idx, static_unblock_idx, temporal_block_padding_mask, static_unblock_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "\n",
    "class TemporalRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, temporal_data, idx_dict, temporal_cols_g):\n",
    "        # Split global token and valid data\n",
    "        global_token_seq = temporal_data[:, :, :1, :]\n",
    "        valid_seq = temporal_data[:, :, 1:, :]\n",
    "\n",
    "        # Append mask_token\n",
    "        revert_idx = idx_dict[\"temporal_revert_idx\"]\n",
    "        mask_token = self.mask_token.unsqueeze(0).unsqueeze(1).repeat(valid_seq.shape[0], valid_seq.shape[1], revert_idx.shape[-1]-valid_seq.shape[-2], 1)\n",
    "        valid_seq = torch.cat([valid_seq, mask_token], dim=-2)\n",
    "        \n",
    "        # Apply revert\n",
    "        revert_idx = revert_idx.unsqueeze(-1).repeat(1, 1, 1, valid_seq.shape[-1])\n",
    "        valid_seq = torch.gather(valid_seq, index=revert_idx, dim=-2)\n",
    "\n",
    "        # Concat global token\n",
    "        temporal_revert_data = torch.cat([global_token_seq, valid_seq], dim=-2)\n",
    "        \n",
    "        # Split to dictionary\n",
    "        temporal_revert_dict = {}\n",
    "        for n, col in enumerate(temporal_cols_g):\n",
    "            temporal_revert_dict[col] = temporal_revert_data[:, :, n, :]\n",
    "        \n",
    "        return temporal_revert_dict\n",
    "\n",
    "class ImgRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, img_dict, idx_dict, img_cols):\n",
    "        img_revert_dict = {}\n",
    "        for col in img_cols:\n",
    "            img_data = img_dict[col]\n",
    "\n",
    "            # Split global token\n",
    "            global_token = img_data[:, :1, :]\n",
    "            valid_data = img_data[:, 1:, :]\n",
    "\n",
    "            # Append mask token\n",
    "            revert_idx = idx_dict[f\"{col}_revert_idx\"]\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(valid_data.shape[0], revert_idx.shape[1]-valid_data.shape[1], 1)\n",
    "            valid_data = torch.cat([valid_data, mask_token], dim=-2)\n",
    "\n",
    "            # Apply revert\n",
    "            revert_idx = revert_idx.unsqueeze(-1).repeat(1, 1, valid_data.shape[-1])\n",
    "            valid_data = torch.gather(valid_data, index=revert_idx, dim=-2)\n",
    "            \n",
    "            # Append global token\n",
    "            img_revert = torch.cat([global_token, valid_data], dim=1)\n",
    "            img_revert_dict[col] = img_revert\n",
    "\n",
    "        return img_revert_dict\n",
    "\n",
    "class NlpRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, nlp_dict, idx_dict, padding_mask_dict, nlp_cols):\n",
    "        nlp_revert_dict = {}\n",
    "        for col in nlp_cols:\n",
    "            nlp_data = nlp_dict[col]\n",
    "\n",
    "            # Replace padding mask to mask token\n",
    "            remain_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"].unsqueeze(-1).repeat(1, 1, nlp_data.shape[-1])\n",
    "            nlp_data = torch.where(remain_padding_mask==1, nlp_data, self.mask_token)\n",
    "\n",
    "            # Split global token\n",
    "            global_token = nlp_data[:, :1, :]\n",
    "            valid_data = nlp_data[:, 1:, :]\n",
    "\n",
    "            # Append mask token\n",
    "            revert_idx = idx_dict[f\"{col}_revert_idx\"]\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(valid_data.shape[0], revert_idx.shape[1]-valid_data.shape[1], 1)\n",
    "            valid_data = torch.cat([valid_data, mask_token], dim=-2)\n",
    "\n",
    "            # Apply revert\n",
    "            revert_idx = revert_idx.unsqueeze(-1).repeat(1, 1, valid_data.shape[-1])\n",
    "            valid_data = torch.gather(valid_data, index=revert_idx, dim=-2)\n",
    "            \n",
    "            # Append global token\n",
    "            nlp_revert = torch.cat([global_token, valid_data], dim=1)\n",
    "            nlp_revert_dict[col] = nlp_revert\n",
    "\n",
    "        return nlp_revert_dict\n",
    "\n",
    "\n",
    "class TemporalDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.cross_attn = MultiheadBlockAttention(d_model, nhead, dropout)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm3 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, cross_attn_padding_mask, self_attn_padding_mask):\n",
    "        x = tgt\n",
    "\n",
    "        self_attn_output, self_attn_weight = self._sa_block(self.norm2(x), self_attn_padding_mask)\n",
    "        x = x + self_attn_output\n",
    "\n",
    "        cross_attn_output, cross_attn_weight = self._ca_block(self.norm1(x.unsqueeze(-2)), memory, memory, cross_attn_padding_mask)\n",
    "        x = x + cross_attn_output.squeeze()\n",
    "\n",
    "        x = x + self._ff_block(self.norm3(x))\n",
    "        return x, self_attn_weight, cross_attn_weight\n",
    "\n",
    "    def _ca_block(self, query, key, value, padding_mask):\n",
    "        x, attn_weight = self.cross_attn(query, key, value, padding_mask)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "    def _sa_block(self, src, padding_mask):\n",
    "        padding_mask = padding_mask[:, :, 0].squeeze()\n",
    "        x, attn_weight = self.self_attn(src, src, src, key_padding_mask=padding_mask, average_attn_weights=False)\n",
    "        return self.dropout2(x), attn_weight\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "class NonTemporalDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.cross_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "        self.self_attn = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm3 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, memory, cross_attn_padding_mask, self_attn_padding_mask):\n",
    "        x = tgt\n",
    "\n",
    "        self_attn_output, self_attn_weight = self._sa_block(self.norm2(x), self_attn_padding_mask)\n",
    "        x = x + self_attn_output\n",
    "\n",
    "        cross_attn_output, cross_attn_weight = self._ca_block(self.norm1(x), memory, memory, cross_attn_padding_mask)\n",
    "        x = x + cross_attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm3(x))\n",
    "\n",
    "    \n",
    "        return x, self_attn_weight, cross_attn_weight\n",
    "    \n",
    "    def _ca_block(self, query, key, value, padding_mask):\n",
    "        x, attn_weight = self.cross_attn(query, key, value, key_padding_mask=padding_mask)\n",
    "        return self.dropout1(x), attn_weight\n",
    "    \n",
    "    def _sa_block(self, src, padding_mask):\n",
    "        x, attn_weight = self.self_attn(src, src, src, key_padding_mask=padding_mask)\n",
    "        return self.dropout2(x), attn_weight\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Class(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self):\n",
    "        return\n",
    "\n",
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"]:\n",
    "            self.embedding = TemporalEmbedding(col, data_info, label_encoder_dict, d_model)\n",
    "        elif col in data_info.modality_info[\"img\"]:\n",
    "            self.embedding = ImgEmbedding(d_model)\n",
    "        elif col in data_info.modality_info[\"nlp\"]:\n",
    "            self.embedding = NlpEmbedding(d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask, device):\n",
    "        return self.embedding(key, val, padding_mask, device)\n",
    "\n",
    "class GlobalToken(torch.nn.Module):\n",
    "    def __init__(self, data_info, d_model):\n",
    "        super().__init__()\n",
    "        self.data_info = data_info\n",
    "        self.global_token = torch.nn.Parameter(torch.rand(1, d_model))\n",
    "    \n",
    "    def forward(self, reference_dict):\n",
    "        reference = reference_dict[self.data_info.modality_info[\"target\"][0]]\n",
    "        batch_size, seq_len, d_model = reference.shape\n",
    "\n",
    "        global_token_seq = self.global_token.unsqueeze(0).repeat(batch_size, seq_len, 1)\n",
    "\n",
    "        return {\"global\":global_token_seq}\n",
    "\n",
    "class PosModEncoding(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc, modality_dict, modality_embedding, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_enc = pos_enc\n",
    "        self.modality = modality_dict[col]\n",
    "        self.modality_embedding = modality_embedding\n",
    "\n",
    "    def forward(self, key, val, device):\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Modality embedding\n",
    "        modality = torch.zeros(val.shape[1]).to(torch.int).to(device) + self.modality\n",
    "        modality = self.modality_embedding(modality)\n",
    "\n",
    "        return val + modality\n",
    "\n",
    "class Remain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temporal_remain = TemporalRemain()\n",
    "        self.img_remain = ImgRemain()\n",
    "        self.nlp_remain = NlpRemain()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, temporal_cols, img_cols, nlp_cols, device):\n",
    "        temporal_remain_dict, idx_dict, padding_mask_dict = self.temporal_remain(data_dict, idx_dict, padding_mask_dict, remain_rto[\"temporal\"], temporal_cols, device)\n",
    "        img_remain_dict, idx_dict, padding_mask_dict = self.img_remain(data_dict, idx_dict, padding_mask_dict, remain_rto[\"img\"], img_cols, device)\n",
    "        nlp_remain_dict, padding_mask_dict = self.nlp_remain(data_dict, idx_dict, padding_mask_dict, remain_rto[\"nlp\"], nlp_cols, device)\n",
    "\n",
    "        return temporal_remain_dict, img_remain_dict, nlp_remain_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation, num_layers, to_decoder_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = torch.nn.ModuleList([copy.deepcopy(EncoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "        self.to_decoder_dim = to_decoder_dim\n",
    "    \n",
    "    def forward(self, temporal_remain_dict, img_remain_dict, nlp_remain_dict, padding_mask_dict, img_cols, nlp_cols):\n",
    "        (src, temporal_block_remain_shape,\n",
    "            temporal_unblock_remain_idx, static_unblock_remain_idx,\n",
    "            temporal_block_remain_padding_mask, static_unblock_remain_padding_mask,\n",
    "            img_unblock_remain_idx_li, nlp_unblock_remain_idx_li) = self.get_src(temporal_remain_dict, img_remain_dict, nlp_remain_dict, padding_mask_dict, img_cols, nlp_cols)\n",
    "        \n",
    "        temporal_block_remain_padding_mask = torch.where(temporal_block_remain_padding_mask==1, 0, -torch.inf)\n",
    "        static_unblock_remain_padding_mask = torch.where(static_unblock_remain_padding_mask==1, 0, -torch.inf)\n",
    "        for mod in self.encoder_layers:\n",
    "            src = mod(src, temporal_block_remain_shape, \n",
    "                        temporal_unblock_remain_idx, static_unblock_remain_idx,\n",
    "                        temporal_block_remain_padding_mask, static_unblock_remain_padding_mask)\n",
    "        \n",
    "        encoding = self.to_decoder_dim(src)\n",
    "        temporal_dict, img_dict, nlp_dict = self.undo_src(encoding, temporal_block_remain_shape, temporal_unblock_remain_idx, static_unblock_remain_idx, img_unblock_remain_idx_li, nlp_unblock_remain_idx_li, img_cols, nlp_cols)\n",
    "        return temporal_dict, img_dict, nlp_dict\n",
    "    \n",
    "    def get_src(self, temporal_remain_dict, img_remain_dict, nlp_remain_dict, padding_mask_dict, img_cols, nlp_cols):\n",
    "        # Unblock temporal remain\n",
    "        temporal_block_remain = temporal_remain_dict[\"temporal_remain\"]\n",
    "        temporal_block_remain_shape = temporal_block_remain.shape\n",
    "        batch_size, seq_len, num_modality, d_model = temporal_block_remain_shape\n",
    "\n",
    "        temporal_unblock_remain = temporal_block_remain.view(batch_size, -1, d_model)\n",
    "        temporal_block_remain_padding_mask = padding_mask_dict[\"temporal_remain_padding_mask\"]\n",
    "        temporal_unblock_remain_idx = torch.arange(0, temporal_unblock_remain.shape[1]).to(device)\n",
    "\n",
    "        # Unblock img remain\n",
    "        img_unblock_remain_li, img_unblock_remain_padding_mask_li, img_unblock_remain_idx_li = [], [], []\n",
    "        unblock_idx = temporal_unblock_remain_idx[-1] + 1\n",
    "        for col in img_cols:\n",
    "            img_remain = img_remain_dict[col]\n",
    "            img_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "            length = img_remain.shape[1]\n",
    "            assert img_remain.shape[1] == img_padding_mask.shape[-1], f\"{img_remain.shape}, {img_padding_mask.shape}\"\n",
    "\n",
    "            img_unblock_remain_li.append(img_remain)\n",
    "            img_unblock_remain_padding_mask_li.append(img_padding_mask)\n",
    "            img_unblock_remain_idx_li.append(torch.arange(unblock_idx, unblock_idx+length).to(device))\n",
    "            unblock_idx += length\n",
    "        img_unblock_remain = torch.cat(img_unblock_remain_li, dim=1)\n",
    "        img_unblock_remain_padding_mask = torch.cat(img_unblock_remain_padding_mask_li, dim=1)\n",
    "        img_unblock_remain_idx = torch.cat(img_unblock_remain_idx_li)\n",
    "\n",
    "        # Unblock nlp remain\n",
    "        nlp_unblock_remain_li, nlp_unblock_remain_padding_mask_li, nlp_unblock_remain_idx_li = [], [], []\n",
    "        for col in nlp_cols:\n",
    "            nlp_remain = nlp_remain_dict[col]\n",
    "            nlp_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "            length = nlp_remain.shape[1]\n",
    "            assert nlp_remain.shape[1] == nlp_padding_mask.shape[-1], f\"{nlp_remain.shape}, {nlp_padding_mask.shape}\"\n",
    "\n",
    "            nlp_unblock_remain_li.append(nlp_remain)\n",
    "            nlp_unblock_remain_padding_mask_li.append(nlp_padding_mask)\n",
    "            nlp_unblock_remain_idx_li.append(torch.arange(unblock_idx, unblock_idx+length).to(device))\n",
    "            unblock_idx += length\n",
    "        nlp_unblock_remain = torch.cat(nlp_unblock_remain_li, dim=1)\n",
    "        nlp_unblock_remain_padding_mask = torch.cat(nlp_unblock_remain_padding_mask_li, dim=1)\n",
    "        nlp_unblock_remain_idx = torch.cat(nlp_unblock_remain_idx_li)\n",
    "\n",
    "        # Static\n",
    "        static_unblock_remain = torch.cat([img_unblock_remain, nlp_unblock_remain], dim=1)\n",
    "        static_unblock_remain_padding_mask = torch.cat([img_unblock_remain_padding_mask, nlp_unblock_remain_padding_mask], dim=1)\n",
    "        static_unblock_remain_idx = torch.cat([img_unblock_remain_idx, nlp_unblock_remain_idx])\n",
    "\n",
    "        # Src\n",
    "        src = torch.cat([temporal_unblock_remain, static_unblock_remain], dim=1)\n",
    "        \n",
    "        return (src, temporal_block_remain_shape,\n",
    "                temporal_unblock_remain_idx, static_unblock_remain_idx,\n",
    "                temporal_block_remain_padding_mask, static_unblock_remain_padding_mask,\n",
    "                img_unblock_remain_idx_li, nlp_unblock_remain_idx_li)\n",
    "        \n",
    "    def undo_src(self, data, temporal_block_remain_shape, temporal_unblock_remain_idx, static_unblock_remain_idx, img_unblock_remain_idx_li, nlp_unblock_remain_idx_li, img_cols, nlp_cols):\n",
    "        # Temporal\n",
    "        temporal_dict = {}\n",
    "        temporal_unblock_idx = temporal_unblock_remain_idx.unsqueeze(0).unsqueeze(-1).repeat(data.shape[0], 1, data.shape[-1])\n",
    "        temporal_encoding = torch.gather(data, index=temporal_unblock_idx, dim=1).view(temporal_block_remain_shape[0], temporal_block_remain_shape[1], temporal_block_remain_shape[2], -1)\n",
    "        temporal_dict[\"temporal\"] = temporal_encoding\n",
    "\n",
    "        # Img\n",
    "        img_dict = {}\n",
    "        for col, idx in zip(img_cols, img_unblock_remain_idx_li):\n",
    "            img_unblock_idx = idx.unsqueeze(0).unsqueeze(-1).repeat(data.shape[0], 1, data.shape[-1])\n",
    "            img_block_data = torch.gather(data, index=img_unblock_idx, dim=1)\n",
    "            img_dict[col] = img_block_data\n",
    "\n",
    "        # Nlp\n",
    "        nlp_dict = {}\n",
    "        for col, idx in zip(nlp_cols, nlp_unblock_remain_idx_li):\n",
    "            nlp_unblock_idx = idx.unsqueeze(0).unsqueeze(-1).repeat(data.shape[0], 1, data.shape[-1])\n",
    "            nlp_block_data = torch.gather(data, index=nlp_unblock_idx, dim=1)\n",
    "            nlp_dict[col] = nlp_block_data\n",
    "        \n",
    "        return temporal_dict, img_dict, nlp_dict\n",
    "\n",
    "class Revert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.temporal_revert = TemporalRevert(mask_token)\n",
    "        self.img_revert = ImgRevert(mask_token)\n",
    "        self.nlp_revert = NlpRevert(mask_token)\n",
    "    \n",
    "    def forward(self, temporal_dict, img_dict, nlp_dict, idx_dict, padding_mask_dict, temporal_cols_g, img_cols, nlp_cols):\n",
    "        temporal_revert_dict = self.temporal_revert(temporal_dict[\"temporal\"], idx_dict, temporal_cols_g)\n",
    "        img_revert_dict = self.img_revert(img_dict, idx_dict, img_cols)\n",
    "        nlp_revert_dict = self.nlp_revert(nlp_dict, idx_dict, padding_mask_dict, nlp_cols)\n",
    "\n",
    "        revert_dict = {}\n",
    "        revert_dict.update(temporal_revert_dict)\n",
    "        revert_dict.update(img_revert_dict)\n",
    "        revert_dict.update(nlp_revert_dict)\n",
    "        \n",
    "        return revert_dict\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.temporal_decoder_layers = torch.nn.ModuleList([copy.deepcopy(TemporalDecoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "        self.non_temporal_decoder_layers = torch.nn.ModuleList([copy.deepcopy(NonTemporalDecoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device):\n",
    "        # Temporal\n",
    "        if key in temporal_cols:\n",
    "            tgt = val\n",
    "            memory, cross_attn_padding_mask, self_attn_padding_mask = self.get_temporal_tgt_memory(key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\n",
    "            decoder_layers = self.temporal_decoder_layers\n",
    "        elif key in img_cols + nlp_cols:\n",
    "            tgt = val\n",
    "            memory, cross_attn_padding_mask, self_attn_padding_mask = self.get_non_temporal_tgt_memory(key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\n",
    "            decoder_layers = self.non_temporal_decoder_layers\n",
    "\n",
    "        for mod in decoder_layers:\n",
    "            tgt, cross_attn_weight, self_attn_weight = mod(tgt, memory, cross_attn_padding_mask, self_attn_padding_mask)\n",
    "        \n",
    "        return tgt, cross_attn_weight, self_attn_weight\n",
    "\n",
    "    def get_temporal_tgt_memory(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device):\n",
    "        memory_li = []\n",
    "\n",
    "        # Temporal memory\n",
    "        for col in temporal_cols:\n",
    "            if col == key:\n",
    "                continue\n",
    "            memory_li.append(data_dict[col])\n",
    "        memory = torch.stack(memory_li, dim=-2)\n",
    "        cross_attn_padding_mask = torch.ones(memory.shape[:-1]).to(device)\n",
    "        \n",
    "        # Static memory\n",
    "        for col in img_cols + nlp_cols:\n",
    "            static_data = data_dict[col].unsqueeze(1).repeat(1, memory.shape[1], 1, 1)\n",
    "            static_padding_msak = padding_mask_dict[f\"{col}_revert_padding_mask\"].unsqueeze(1).repeat(1, memory.shape[1], 1)\n",
    "\n",
    "            memory = torch.cat([memory, static_data], dim=-2)\n",
    "            cross_attn_padding_mask = torch.cat([cross_attn_padding_mask, static_padding_msak], dim=-1)\n",
    "        \n",
    "        # Self attn padding mask\n",
    "        self_attn_padding_mask = padding_mask_dict[\"temporal_revert_padding_mask\"].squeeze()\n",
    "        self_attn_padding_mask = torch.where(self_attn_padding_mask==1, 0, -torch.inf)\n",
    "        cross_attn_padding_mask = torch.where(cross_attn_padding_mask==1, 0, -torch.inf)\n",
    "\n",
    "        return memory, cross_attn_padding_mask, self_attn_padding_mask\n",
    "\n",
    "    def get_non_temporal_tgt_memory(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device):\n",
    "        memory_li, cross_attn_padding_mask_li = [], []\n",
    "        \n",
    "        # Temporal memory\n",
    "        for col in temporal_cols:\n",
    "            temporal_data = data_dict[col]\n",
    "            temporal_padding_mask = padding_mask_dict[f\"temporal_revert_padding_mask\"].squeeze()\n",
    "            \n",
    "            memory_li.append(temporal_data)\n",
    "            cross_attn_padding_mask_li.append(temporal_padding_mask)\n",
    "        \n",
    "        memory = torch.cat(memory_li, dim=1)\n",
    "        cross_attn_padding_mask = torch.cat(cross_attn_padding_mask_li, dim=-1)\n",
    "\n",
    "        # Img memory or Nlp memory\n",
    "        for col in img_cols + nlp_cols:\n",
    "            if key == col: continue\n",
    "            static_data = data_dict[col]\n",
    "            static_padding_mask = padding_mask_dict[f\"{col}_revert_padding_mask\"]\n",
    "\n",
    "            memory = torch.cat([memory, static_data], dim=1)\n",
    "            print(cross_attn_padding_mask.shape)\n",
    "            print(static_padding_mask.shape)\n",
    "            raise\n",
    "            cross_attn_padding_mask = torch.cat([cross_attn_padding_mask, static_padding_mask], dim=-1)\n",
    "        \n",
    "        # Self attn padding mask\n",
    "        self_attn_padding_mask = padding_mask_dict[f\"{key}_revert_padding_mask\"].squeeze()\n",
    "        self_attn_padding_mask = torch.where(self_attn_padding_mask==1, 0, -torch.inf)\n",
    "        cross_attn_padding_mask = torch.where(cross_attn_padding_mask==1, 0, -torch.inf)\n",
    "\n",
    "        return memory, cross_attn_padding_mask, self_attn_padding_mask\n",
    "\n",
    "class TemporalOutput(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, 1))\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, num_cls))\n",
    "\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        return self.output(val)\n",
    "\n",
    "class ImgOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_model),\n",
    "            torch.nn.Linear(d_model, 3*patch_size*patch_size))\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        return self.output(val)\n",
    "        \n",
    "class NlpOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, num_vocab=30522):\n",
    "        super().__init__()\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, d_model),\n",
    "            torch.nn.Linear(d_model, num_vocab))\n",
    "    \n",
    "    def forward(self, key, val):\n",
    "        return self.output(val)\n",
    "1==1\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                d_model, num_layers, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.data_info, self.label_encoder_dict = data_info, label_encoder_dict\n",
    "        self.temporal_cols, self.temporal_cols_g, self.img_cols, self.nlp_cols, self.total_cols, self.total_cols_g = self.define_cols()\n",
    "        self.self_attn_weight_dict, self.cross_attn_weight_dict = {}, {}\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embedding_dict = self.init_process(mod=Embedding, args=[data_info, label_encoder_dict, d_model[\"encoder\"]], target_cols=self.total_cols)\n",
    "        # 2. Global token\n",
    "        self.global_token_dict = GlobalToken(data_info, d_model[\"encoder\"])\n",
    "        # 3. Pos encoding & Modality encoding\n",
    "        encoder_pos_enc = PositionalEncoding(d_model[\"encoder\"], dropout)\n",
    "        num_modality = len(self.total_cols_g)\n",
    "        modality_dict = {col:n for n, col in enumerate(self.total_cols_g)}\n",
    "        enc_modality_embedding = torch.nn.Embedding(num_modality, d_model[\"encoder\"])\n",
    "        self.enc_posmod_encoding_dict = self.init_process(mod=PosModEncoding, args=[encoder_pos_enc, modality_dict, enc_modality_embedding, d_model[\"encoder\"]], target_cols=self.total_cols_g)\n",
    "        # 4. Remain masking\n",
    "        self.remain_dict = Remain()\n",
    "        # 5. Encoding\n",
    "        to_decoder_dim = torch.nn.Linear(d_model[\"encoder\"], d_model[\"decoder\"])\n",
    "        self.encoding = Encoder(d_model[\"encoder\"], nhead, d_ff[\"encoder\"], dropout, activation, num_layers[\"encoder\"], to_decoder_dim)\n",
    "        # 6. Revert\n",
    "        mask_token = torch.nn.Parameter(torch.rand(1, d_model[\"decoder\"]))\n",
    "        self.revert = Revert(mask_token)\n",
    "        # 7. Pos encoding & Modality encoding\n",
    "        decoder_pos_enc = PositionalEncoding(d_model[\"decoder\"], dropout)\n",
    "        dec_modality_embedding = torch.nn.Embedding(num_modality, d_model[\"decoder\"])\n",
    "        self.dec_posmod_encoding_dict = self.init_process(mod=PosModEncoding, args=[decoder_pos_enc, modality_dict, dec_modality_embedding, d_model[\"decoder\"]], target_cols=self.total_cols_g)\n",
    "       \n",
    "        # 7. Decoding\n",
    "        self.decoding = self.init_process(mod=Decoder, args=[d_model[\"decoder\"], nhead, d_ff[\"decoder\"], dropout, activation, num_layers[\"decoder\"]], target_cols=self.total_cols_g)\n",
    "    \n",
    "    def forward(self, data_input, remain_rto, device):\n",
    "        data_dict, self.idx_dict, self.padding_mask_dict = self.to_gpu(data_input, device)\n",
    "        \n",
    "        # 1. Embedding\n",
    "        embedding_dict = self.apply_process(data=data_dict, mod=self.embedding_dict, args=[self.padding_mask_dict, device], target_cols=self.total_cols)\n",
    "        # 2. Global & dummy token\n",
    "        global_token_dict = self.global_token_dict(embedding_dict)\n",
    "        embedding_dict.update(global_token_dict)\n",
    "        # 3. Pos encoding & Modality encoding\n",
    "        enc_posmod_encoding_dict = self.apply_process(data=embedding_dict, mod=self.enc_posmod_encoding_dict, args=[device], target_cols=self.total_cols_g)\n",
    "        # 4. Remain masing\n",
    "        temporal_remain_dict, img_remain_dict, nlp_remain_dict, self.idx_dict, self.padding_mask_dict = self.remain_dict(enc_posmod_encoding_dict, self.idx_dict, self.padding_mask_dict, remain_rto, self.temporal_cols_g, self.img_cols, self.nlp_cols, device)\n",
    "        # 5. Encoding\n",
    "        temporal_encoding_dict, img_encoding_dict, nlp_encoding_dict = self.encoding(temporal_remain_dict, img_remain_dict, nlp_remain_dict, self.padding_mask_dict, self.img_cols, self.nlp_cols)\n",
    "        # 6. Revert\n",
    "        revert_dict = self.revert(temporal_encoding_dict, img_encoding_dict, nlp_encoding_dict, self.idx_dict, self.padding_mask_dict, self.temporal_cols_g, self.img_cols, self.nlp_cols)\n",
    "        # 7. Pos encoding & Modality encoding\n",
    "        dec_posmod_encoding_dict = self.apply_process(data=revert_dict, mod=self.dec_posmod_encoding_dict, args=[device], target_cols=self.total_cols_g)\n",
    "\n",
    "        # 7. Decoder\n",
    "        decoding = self.apply_process(data=dec_posmod_encoding_dict, mod=self.decoding, args=[dec_posmod_encoding_dict, self.padding_mask_dict, self.temporal_cols_g, self.img_cols, self.nlp_cols, device], collate_fn=self.tidy_decoding, target_cols=self.total_cols_g)\n",
    "\n",
    "    \n",
    "    def define_cols(self):\n",
    "        temporal_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        img_cols = self.data_info.modality_info[\"img\"]\n",
    "        nlp_cols = self.data_info.modality_info[\"nlp\"]\n",
    "        total_cols = temporal_cols + img_cols + nlp_cols\n",
    "        \n",
    "        total_cols_gd = [\"global\"] + total_cols\n",
    "        temporal_cols_gd = [\"global\"] + temporal_cols\n",
    "\n",
    "        return temporal_cols, temporal_cols_gd, img_cols, nlp_cols, total_cols, total_cols_gd\n",
    "\n",
    "    def to_gpu(self, data_input, device):\n",
    "        data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        for key, val in data_input.items():\n",
    "            if key in self.temporal_cols + self.img_cols + self.nlp_cols:\n",
    "                data_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"idx\"):\n",
    "                idx_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"mask\"):\n",
    "                padding_mask_dict[key] = data_input[key].to(device)\n",
    "            \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def init_process(self, mod, args=[], target_cols=None):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod(col, *args)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def apply_process(self, data, mod, args=[], target_cols=None, collate_fn=None):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod[col](col, data[col], *args)\n",
    "        \n",
    "        if collate_fn is not None:\n",
    "            return collate_fn(result_dict)\n",
    "        else:\n",
    "            return result_dict\n",
    "\n",
    "    def tidy_decoding(self, decoding_dict):\n",
    "        result_dict = {}\n",
    "        for key, val in decoding_dict.items():\n",
    "            result_dict[key], self_attn_weight, cross_attn_weight = val\n",
    "            self.self_attn_weight_dict.update({key:self_attn_weight})\n",
    "            self.cross_attn_weight_dict.update({key:cross_attn_weight})\n",
    "\n",
    "        return result_dict\n",
    "1==1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Pseudo code\n",
    "(1). embedding -> each dict\n",
    "(2). make global token seq & dummy toekn seq -> each dict\n",
    "(3). pos and modality encoding -> each dict\n",
    "(4). remain masking -> concat\n",
    "(5). encoding -> concat\n",
    "(6). revert -> each dict\n",
    "(7). pos and modality encoding -> each dict\n",
    "(8). decoding\n",
    "9. output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 49])\n",
      "torch.Size([2, 9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(data_info, train_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder_dict,\n\u001b[1;32m      2\u001b[0m                         d_model, num_layers, nhead, d_ff, dropout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremain_rto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_parent_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_model_summary/model_summary.py:128\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m model_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_training:\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[36], line 57\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, data_input, remain_rto, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m dec_posmod_encoding_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_process(data\u001b[38;5;241m=\u001b[39mrevert_dict, mod\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_posmod_encoding_dict, args\u001b[38;5;241m=\u001b[39m[device], target_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cols_g)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 7. Decoder\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m decoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_posmod_encoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdec_posmod_encoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_cols_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtidy_decoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_cols_g\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 93\u001b[0m, in \u001b[0;36mTransformer.apply_process\u001b[0;34m(self, data, mod, args, target_cols, collate_fn)\u001b[0m\n\u001b[1;32m     91\u001b[0m result_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m target_cols:\n\u001b[0;32m---> 93\u001b[0m     result_dict[col] \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn(result_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[35], line 202\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m img_cols \u001b[38;5;241m+\u001b[39m nlp_cols:\n\u001b[1;32m    201\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m--> 202\u001b[0m     memory, cross_attn_padding_mask, self_attn_padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_non_temporal_tgt_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     decoder_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_temporal_decoder_layers\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m decoder_layers:\n",
      "Cell \u001b[0;32mIn[35], line 259\u001b[0m, in \u001b[0;36mDecoder.get_non_temporal_tgt_memory\u001b[0;34m(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cross_attn_padding_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mprint\u001b[39m(static_padding_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     cross_attn_padding_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([cross_attn_padding_mask, static_padding_mask], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Self attn padding mask\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout, \"gelu\")\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"img_model\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"nlp_model\" in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremain_rto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# raise\u001b[39;00m\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.100.0 (block token)/train.py:313\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e, dataloader, optimizer, scheduler, model, data_info, remain_rto, device)\u001b[0m\n\u001b[1;32m    311\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    312\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 313\u001b[0m temporal_output, img_output, nlp_output, self_attn_dict, cross_attn_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m model(data, remain_rto, device)\n\u001b[1;32m    315\u001b[0m temporal_loss_dict, temporal_masked_pred_dict, temporal_masked_y_dict \u001b[38;5;241m=\u001b[39m get_temporal_loss(temporal_output, data, idx_dict, padding_mask_dict, data_info, device)\n\u001b[1;32m    316\u001b[0m img_loss_dict, img_masked_pred_dict, img_masked_y_dict \u001b[38;5;241m=\u001b[39m get_img_loss(img_output, data, idx_dict, padding_mask_dict, data_info, device)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from train import *\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "epoch = 3\n",
    "for e in range(epoch):\n",
    "    train(e, train_dataloader, optimizer, scheduler, model, data_info, remain_rto, device)\n",
    "    scheduler.step()\n",
    "    # raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
