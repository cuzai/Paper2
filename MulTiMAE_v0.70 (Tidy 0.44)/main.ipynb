{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 365\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 32\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "is_identical = False\n",
    "\n",
    "d_model = {\"encoder\":64, \"decoder\":32}\n",
    "d_ff = {\"encoder\":64, \"decoder\":32}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"target\": 0.25, \"temporal\":0.25, \"cat\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2442.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 365, 1])\n",
      "sales_remain_idx torch.Size([2, 91])\n",
      "sales_masked_idx torch.Size([2, 274])\n",
      "sales_revert_idx torch.Size([2, 365])\n",
      "sales_remain_padding_mask torch.Size([2, 91])\n",
      "sales_masked_padding_mask torch.Size([2, 274])\n",
      "sales_revert_padding_mask torch.Size([2, 365])\n",
      "day torch.Size([2, 365])\n",
      "day_remain_idx torch.Size([2, 91])\n",
      "day_masked_idx torch.Size([2, 274])\n",
      "day_revert_idx torch.Size([2, 365])\n",
      "day_remain_padding_mask torch.Size([2, 91])\n",
      "day_masked_padding_mask torch.Size([2, 274])\n",
      "day_revert_padding_mask torch.Size([2, 365])\n",
      "dow torch.Size([2, 365])\n",
      "dow_remain_idx torch.Size([2, 91])\n",
      "dow_masked_idx torch.Size([2, 274])\n",
      "dow_revert_idx torch.Size([2, 365])\n",
      "dow_remain_padding_mask torch.Size([2, 91])\n",
      "dow_masked_padding_mask torch.Size([2, 274])\n",
      "dow_revert_padding_mask torch.Size([2, 365])\n",
      "month torch.Size([2, 365])\n",
      "month_remain_idx torch.Size([2, 91])\n",
      "month_masked_idx torch.Size([2, 274])\n",
      "month_revert_idx torch.Size([2, 365])\n",
      "month_remain_padding_mask torch.Size([2, 91])\n",
      "month_masked_padding_mask torch.Size([2, 274])\n",
      "month_revert_padding_mask torch.Size([2, 365])\n",
      "holiday torch.Size([2, 365])\n",
      "holiday_remain_idx torch.Size([2, 91])\n",
      "holiday_masked_idx torch.Size([2, 274])\n",
      "holiday_revert_idx torch.Size([2, 365])\n",
      "holiday_remain_padding_mask torch.Size([2, 91])\n",
      "holiday_masked_padding_mask torch.Size([2, 274])\n",
      "holiday_revert_padding_mask torch.Size([2, 365])\n",
      "price torch.Size([2, 365, 1])\n",
      "price_remain_idx torch.Size([2, 91])\n",
      "price_masked_idx torch.Size([2, 274])\n",
      "price_revert_idx torch.Size([2, 365])\n",
      "price_remain_padding_mask torch.Size([2, 91])\n",
      "price_masked_padding_mask torch.Size([2, 274])\n",
      "price_revert_padding_mask torch.Size([2, 365])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.embedding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                d_model, num_layers, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.data_info = data_info\n",
    "        self.temporal_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        \n",
    "        # 1. Embedding\n",
    "        self.embedding_dict = self.init_process(self.temporal_cols, Embedding, **{\"data_info\":data_info, \"label_encoder_dict\":label_encoder_dict, \"d_model\":d_model[\"encoder\"]})\n",
    "\n",
    "    def forward(self, data_input, remain_rto, device):\n",
    "        # 0. To gpu\n",
    "        data_dict, idx_dict, padding_mask_dict = self.to_gpu(data_input, device)\n",
    "\n",
    "        # 1. Embedding\n",
    "        embedding_dict = self.apply_process(data_dict, self.temporal_cols, self.embedding_dict)\n",
    "        \n",
    "        return embedding_dict\n",
    "    \n",
    "    def to_gpu(self, data_input, device):\n",
    "        data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "\n",
    "        for col in data.keys():\n",
    "            if col in self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]:\n",
    "                data_dict[col] = data_input[col].to(device)\n",
    "            if col.endswith(\"idx\"):\n",
    "                idx_dict[col] = data_input[col].to(device)\n",
    "            if col.endswith(\"padding_mask\"):\n",
    "                padding_mask_dict[col] = data_input[col].to(device)\n",
    "        \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def init_process(self, target_cols, mod, **kwargs):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = Embedding(col, **kwargs)\n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "    \n",
    "    def apply_process(self, data, target_cols, mod, **kwargs):\n",
    "        result_dict = {}\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod[col](data[col], **kwargs)\n",
    "        return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "   Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================================\n",
      "=======================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'---------------------------------------------------------------------------------------\\n   Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\\n=======================================================================================\\n=======================================================================================\\nTotal params: 0\\nTrainable params: 0\\nNon-trainable params: 0\\n---------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout)\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
