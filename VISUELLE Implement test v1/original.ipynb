{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data MultiTrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        # Get the Gtrends time series associated with each product\n",
    "        # Read the images (extracted image features) as well\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], \\\n",
    "                row['release_date'], row['image_path']\n",
    "\n",
    "            # Get the gtrend signal up to the previous year (52 weeks) of the release date\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends =  np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "\n",
    "            # Read images\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            # Append them to the lists\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        # Remove non-numerical information\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # Create tensors for each part of the input/output\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(\n",
    "            data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        \n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('Starting dataset creation process...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        data_loader = None\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=1, shuffle=False, num_workers=4)\n",
    "        print('Done.')\n",
    "\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-30 10:39:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import pipeline\n",
    "from torchvision import models\n",
    "from fairseq.optim.adafactor import Adafactor\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    # Takes any module and stacks the time dimension with the batch dimenison of inputs before applying the module\n",
    "    # Insipired from https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "    # https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module # Can be any layer we wish to apply like Linear, Conv etc\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, dropout=0.2):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        \n",
    "        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)\n",
    "        self.use_img = use_img\n",
    "        self.use_text = use_text\n",
    "        input_dim = embedding_dim + (embedding_dim*use_img) + (embedding_dim*use_text)\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_encoding, text_encoding, dummy_encoding):\n",
    "        # Fuse static features together\n",
    "        pooled_img = self.img_pool(img_encoding)\n",
    "        condensed_img = self.img_linear(pooled_img.flatten(1))\n",
    "\n",
    "        # Build input\n",
    "        decoder_inputs = []\n",
    "        if self.use_img == 1:\n",
    "            decoder_inputs.append(condensed_img) \n",
    "        if self.use_text == 1:\n",
    "            decoder_inputs.append(text_encoding) \n",
    "        decoder_inputs.append(dummy_encoding)\n",
    "        concat_features = torch.cat(decoder_inputs, dim=1)\n",
    "\n",
    "        final = self.feature_fusion(concat_features)\n",
    "        # final = self.feature_fusion(dummy_encoding)\n",
    "\n",
    "        return final\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends,  gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "        \n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cat_dict = {v: k for k, v in cat_dict.items()}\n",
    "        self.col_dict = {v: k for k, v in col_dict.items()}\n",
    "        self.fab_dict = {v: k for k, v in fab_dict.items()}\n",
    "        self.word_embedder = pipeline('feature-extraction', model='bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def forward(self, category, color, fabric):\n",
    "        textual_description = [self.col_dict[color.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.fab_dict[fabric.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.cat_dict[category.detach().cpu().numpy().tolist()[i]] for i in range(len(category))]\n",
    "\n",
    "\n",
    "        # Use BERT to extract features\n",
    "        word_embeddings = self.word_embedder(textual_description)\n",
    "\n",
    "        # BERT gives us embeddings for [CLS] ..  [EOS], which is why we only average the embeddings in the range [1:-1] \n",
    "        # We're not fine tuning BERT and we don't want the noise coming from [CLS] or [EOS]\n",
    "        word_embeddings = [torch.FloatTensor(x[0][1:-1]).mean(axis=0) for x in word_embeddings] \n",
    "        word_embeddings = torch.stack(word_embeddings).to('cuda:'+str(self.gpu_num))\n",
    "        \n",
    "        # Embed to our embedding space\n",
    "        word_embeddings = self.dropout(self.fc(word_embeddings))\n",
    "\n",
    "        return word_embeddings\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Img feature extraction\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Fine tune resnet\n",
    "        # for c in list(self.resnet.children())[6:]:\n",
    "        #     for p in c.parameters():\n",
    "        #         p.requires_grad = True\n",
    "        \n",
    "    def forward(self, images):        \n",
    "        img_embeddings = self.resnet(images)  \n",
    "        size = img_embeddings.size()\n",
    "        out = img_embeddings.view(*size[:2],-1)\n",
    "\n",
    "        return out.view(*size).contiguous() # batch_size, 2048, image_size/32, image_size/32\n",
    "\n",
    "class DummyEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        # Temporal dummy variables (day, week, month, year)\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "\n",
    "        return temporal_embeddings\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None, tgt_key_padding_mask = None, \n",
    "            memory_key_padding_mask = None):\n",
    "\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attn_weights\n",
    "\n",
    "# class GTM(pl.LightningModule):\n",
    "class GTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, use_text, use_img, \\\n",
    "                cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_len = output_dim\n",
    "        self.use_encoder_mask = use_encoder_mask\n",
    "        self.autoregressive = autoregressive\n",
    "        self.gpu_num = gpu_num\n",
    "        # self.save_hyperparameters()\n",
    "\n",
    "         # Encoder\n",
    "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
    "        self.image_encoder = ImageEmbedder()\n",
    "        self.text_encoder = TextEmbedder(embedding_dim, cat_dict, col_dict, fab_dict, gpu_num)\n",
    "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n",
    "        self.static_feature_encoder = FusionNetwork(embedding_dim, hidden_dim, use_img, use_text)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_linear = TimeDistributed(nn.Linear(1, hidden_dim))\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \\\n",
    "                                                dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n",
    "        \n",
    "        if self.autoregressive: self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
    "        # Encode features and get inputs\n",
    "        img_encoding = self.image_encoder(images)\n",
    "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
    "        text_encoding = self.text_encoder(category, color, fabric)\n",
    "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
    "\n",
    "        # Fuse static features together\n",
    "        static_feature_fusion = self.static_feature_encoder(img_encoding, text_encoding, dummy_encoding)\n",
    "\n",
    "        if self.autoregressive == 1:\n",
    "            # Decode\n",
    "            tgt = torch.zeros(self.output_len, gtrend_encoding.shape[1], gtrend_encoding.shape[-1]).to('cuda:'+str(self.gpu_num))\n",
    "            tgt[0] = static_feature_fusion\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(self.output_len)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory, tgt_mask)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "        else:\n",
    "            # Decode (generatively/non-autoregressively)\n",
    "            tgt = static_feature_fusion.unsqueeze(0)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "\n",
    "        return forecast.view(-1, self.output_len), attn_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adafactor(self.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    \n",
    "        return [optimizer]\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, test_batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = test_batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        \n",
    "        return item_sales.squeeze(), forecasted_sales.squeeze()\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n",
    "        item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n",
    "        rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065 # 1065 is the normalization factor (max of the sales of the training set)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
    "        self.log('val_mae', mae)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        print('Validation MAE:', mae.detach().cpu().numpy(), 'LR:', self.optimizers().param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5080/5080 [00:52<00:00, 97.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Starting dataset creation process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 497/497 [00:06<00:00, 77.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "args = {\n",
    "    \"data_folder\": \"../visuelle/\",\n",
    "    \"log_dir\": \"log\",\n",
    "    \"seed\": 21,\n",
    "    \"epochs\": 200,\n",
    "    \"gpu_num\": 0,\n",
    "\n",
    "    \"model_type\": \"GTM\",\n",
    "    \"use_trends\": 1,\n",
    "    \"use_img\": 1,\n",
    "    \"use_text\": 1,\n",
    "    \"trend_len\": 52,\n",
    "    \"num_trends\": 3,\n",
    "    \"batch_size\": 128,\n",
    "    \"embedding_dim\": 32,\n",
    "    \"hidden_dim\": 64,\n",
    "    \"output_dim\": 12,\n",
    "    \"use_encoder_mask\": 1,\n",
    "    \"autoregressive\": 0,\n",
    "    \"num_attn_heads\": 4,\n",
    "    \"num_hidden_layers\": 1,\n",
    "}\n",
    "# Seeds for reproducibility (By default we use the number 21)\n",
    "pl.seed_everything(args[\"seed\"])\n",
    "\n",
    "# Load sales data\n",
    "train_df = pd.read_csv(Path(args[\"data_folder\"] + 'train.csv'), parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(Path(args[\"data_folder\"] + 'test.csv'), parse_dates=['release_date'])\n",
    "\n",
    "# Load category and color encodings\n",
    "cat_dict = torch.load(Path(args[\"data_folder\"] + 'category_labels.pt'))\n",
    "col_dict = torch.load(Path(args[\"data_folder\"] + 'color_labels.pt'))\n",
    "fab_dict = torch.load(Path(args[\"data_folder\"] + 'fabric_labels.pt'))\n",
    "\n",
    "# Load Google trends\n",
    "gtrends = pd.read_csv(Path(args[\"data_folder\"] + 'gtrends.csv'), index_col=[0], parse_dates=True)\n",
    "\n",
    "# Model Training\n",
    "# Define model saving procedure\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "model_savename = args[\"model_type\"]\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args[\"log_dir\"] + '/'+args[\"model_type\"],\n",
    "    filename=model_savename+'---{epoch}---'+dt_string,\n",
    "    monitor='val_mae',\n",
    "    mode='min',\n",
    "    save_top_k=1\n",
    ")\n",
    "\n",
    "# If you wish to use Tensorboard you can change the logger to:\n",
    "# tb_logger = pl_loggers.TensorBoardLogger(args.log_dir+'/', name=model_savename)\n",
    "trainer = pl.Trainer(\n",
    "                    gpus=[args[\"gpu_num\"]], \n",
    "                    max_epochs=args[\"epochs\"], check_val_every_n_epoch=5,\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                        )\n",
    "\n",
    "train_loader = ZeroShotDataset(train_df, Path(args[\"data_folder\"] + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, args[\"trend_len\"]).get_loader(batch_size=args[\"batch_size\"], train=True)\n",
    "test_loader = ZeroShotDataset(test_df, Path(args[\"data_folder\"] + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, args[\"trend_len\"]).get_loader(batch_size=1, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4:0/40 mean_loss: 0.11721235513687134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m train_loss_li, val_loss_li \u001b[39m=\u001b[39m [], []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(e) \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m val()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     train_loss_li\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m n, train_batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):        \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     item_sales, category, color, fabric, temporal_features, gtrends, images \u001b[39m=\u001b[39m train_batch \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     forecasted_sales, _ \u001b[39m=\u001b[39m model(category\u001b[39m.\u001b[39;49mto(device), color\u001b[39m.\u001b[39;49mto(device), fabric\u001b[39m.\u001b[39;49mto(device), temporal_features\u001b[39m.\u001b[39;49mto(device), gtrends\u001b[39m.\u001b[39;49mto(device), images\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(item_sales\u001b[39m.\u001b[39mto(device), forecasted_sales\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=272'>273</a>\u001b[0m img_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder(images)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=273'>274</a>\u001b[0m dummy_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdummy_encoder(temporal_features)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=274'>275</a>\u001b[0m text_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(category, color, fabric)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=275'>276</a>\u001b[0m gtrend_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgtrend_encoder(gtrends)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=277'>278</a>\u001b[0m \u001b[39m# Fuse static features together\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m textual_description \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_dict[color\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfab_dict[fabric\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_dict[category\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(category))]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# Use BERT to extract features\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embedder(textual_description)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# BERT gives us embeddings for [CLS] ..  [EOS], which is why we only average the embeddings in the range [1:-1] \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# We're not fine tuning BERT and we don't want the noise coming from [CLS] or [EOS]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m word_embeddings \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mFloatTensor(x[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_embeddings] \n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/feature_extraction.py:107\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    Extract the features of the input(s).\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m        A nested list of `float`: The features computed by the model.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/feature_extraction.py:85\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m---> 85\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:325\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    322\u001b[0m     past_key_value \u001b[39m=\u001b[39m (key_layer, value_layer)\n\u001b[1;32m    324\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelative_key_query\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    328\u001b[0m     query_length, key_length \u001b[39m=\u001b[39m query_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], key_layer\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GTM(\n",
    "    embedding_dim=args[\"embedding_dim\"],\n",
    "    hidden_dim=args[\"hidden_dim\"],\n",
    "    output_dim=args[\"output_dim\"],\n",
    "    num_heads=args[\"num_attn_heads\"],\n",
    "    num_layers=args[\"num_hidden_layers\"],\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    use_text=args[\"use_text\"],\n",
    "    use_img=args[\"use_img\"],\n",
    "    trend_len=args[\"trend_len\"],\n",
    "    num_trends=args[\"num_trends\"],\n",
    "    use_encoder_mask=args[\"use_encoder_mask\"],\n",
    "    autoregressive=args[\"autoregressive\"],\n",
    "    gpu_num=args[\"gpu_num\"]\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# for train_batch in train_loader:\n",
    "#     item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n",
    "#     forecasted_sales, _ = model(category.to(device), color.to(device), fabric.to(device), temporal_features.to(device), gtrends.to(device), images.to(device))\n",
    "#     loss = F.mse_loss(item_sales.to(device), forecasted_sales.squeeze())\n",
    "#     print(loss)\n",
    "#     break\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_len = len(train_loader)\n",
    "    total_loss = 0\n",
    "    for n, train_batch in enumerate(train_loader):        \n",
    "        # Train\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n",
    "        forecasted_sales, _ = model(category.to(device), color.to(device), fabric.to(device), temporal_features.to(device), gtrends.to(device), images.to(device))\n",
    "        loss = F.mse_loss(item_sales.to(device), forecasted_sales.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss / (n+1)\n",
    "        print(f\"\\r {epoch}:{n}/{total_len} mean_loss: {mean_loss}\", end=\"\")\n",
    "\n",
    "    print()\n",
    "    return mean_loss\n",
    "\n",
    "def val():\n",
    "    model.eval()\n",
    "    total_len = len(test_loader)\n",
    "    total_loss = 0\n",
    "    for n, valid_batch in enumerate(test_loader):\n",
    "        # Pred\n",
    "        with torch.no_grad():\n",
    "            item_sales, category, color, fabric, temporal_features, gtrends, images = valid_batch \n",
    "            forecasted_sales, _ = model(category.to(device), color.to(device), fabric.to(device), temporal_features.to(device), gtrends.to(device), images.to(device))\n",
    "            loss = F.mse_loss(item_sales.to(device), forecasted_sales.squeeze())\n",
    "\n",
    "            # Report\n",
    "            total_loss += loss.item()\n",
    "            mean_loss = total_loss / (n+1)\n",
    "            return mean_loss\n",
    " \n",
    "def plot(train_loss_li, val_loss_li):\n",
    "    # Plot loss\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(train_loss_li, label=\"train\")\n",
    "    plt.plot(val_loss_li, label=\"valid\")\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "\n",
    "epoch = 1000\n",
    "train_loss_li, val_loss_li = [], []\n",
    "for e in range(epoch):\n",
    "    train_loss = train(e) # Train\n",
    "    val_loss = val()\n",
    "\n",
    "    train_loss_li.append(train_loss)\n",
    "    val_loss_li.append(val_loss)\n",
    "    plot(train_loss_li, val_loss_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import argparse\n",
    "# import wandb\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import loggers as pl_loggers\n",
    "# from pathlib import Path\n",
    "# from datetime import datetime\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# def run(args):\n",
    "#     print(args)\n",
    "#     # Seeds for reproducibility (By default we use the number 21)\n",
    "#     pl.seed_everything(args[\"seed\"])\n",
    "\n",
    "#     # Load sales data\n",
    "#     train_df = pd.read_csv(Path(args[\"data_folder\"] + 'train.csv'), parse_dates=['release_date'])\n",
    "#     test_df = pd.read_csv(Path(args[\"data_folder\"] + 'test.csv'), parse_dates=['release_date'])\n",
    "\n",
    "#     # Load category and color encodings\n",
    "#     cat_dict = torch.load(Path(args[\"data_folder\"] + 'category_labels.pt'))\n",
    "#     col_dict = torch.load(Path(args[\"data_folder\"] + 'color_labels.pt'))\n",
    "#     fab_dict = torch.load(Path(args[\"data_folder\"] + 'fabric_labels.pt'))\n",
    "\n",
    "#     # Load Google trends\n",
    "#     gtrends = pd.read_csv(Path(args[\"data_folder\"] + 'gtrends.csv'), index_col=[0], parse_dates=True)\n",
    "\n",
    "#     model = GTM(\n",
    "#         embedding_dim=args[\"embedding_dim\"],\n",
    "#         hidden_dim=args[\"hidden_dim\"],\n",
    "#         output_dim=args[\"output_dim\"],\n",
    "#         num_heads=args[\"num_attn_heads\"],\n",
    "#         num_layers=args[\"num_hidden_layers\"],\n",
    "#         cat_dict=cat_dict,\n",
    "#         col_dict=col_dict,\n",
    "#         fab_dict=fab_dict,\n",
    "#         use_text=args[\"use_text\"],\n",
    "#         use_img=args[\"use_img\"],\n",
    "#         trend_len=args[\"trend_len\"],\n",
    "#         num_trends=args[\"num_trends\"],\n",
    "#         use_encoder_mask=args[\"use_encoder_mask\"],\n",
    "#         autoregressive=args[\"autoregressive\"],\n",
    "#         gpu_num=args[\"gpu_num\"]\n",
    "#     )\n",
    "\n",
    "#     # Model Training\n",
    "#     # Define model saving procedure\n",
    "#     dt_string = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "#     model_savename = args[\"model_type\"]\n",
    "\n",
    "#     checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "#         dirpath=args[\"log_dir\"] + '/'+args[\"model_type\"],\n",
    "#         filename=model_savename+'---{epoch}---'+dt_string,\n",
    "#         monitor='val_mae',\n",
    "#         mode='min',\n",
    "#         save_top_k=1\n",
    "#     )\n",
    "\n",
    "#     # If you wish to use Tensorboard you can change the logger to:\n",
    "#     # tb_logger = pl_loggers.TensorBoardLogger(args.log_dir+'/', name=model_savename)\n",
    "#     trainer = pl.Trainer(\n",
    "#                         gpus=[args[\"gpu_num\"]], \n",
    "#                         max_epochs=args[\"epochs\"], check_val_every_n_epoch=5,\n",
    "#                         callbacks=[checkpoint_callback]\n",
    "#                          )\n",
    "\n",
    "#     train_loader = ZeroShotDataset(train_df, Path(args[\"data_folder\"] + '/images'), gtrends, cat_dict, col_dict,\n",
    "#                                    fab_dict, args[\"trend_len\"]).get_loader(batch_size=args[\"batch_size\"], train=True)\n",
    "#     test_loader = ZeroShotDataset(test_df, Path(args[\"data_folder\"] + '/images'), gtrends, cat_dict, col_dict,\n",
    "#                                   fab_dict, args[\"trend_len\"]).get_loader(batch_size=1, train=False)\n",
    "\n",
    "#     # Fit model\n",
    "#     trainer.fit(model, train_dataloaders=train_loader,\n",
    "#                 val_dataloaders=test_loader)\n",
    "\n",
    "#     # Print out path of best model\n",
    "#     print(checkpoint_callback.best_model_path)\n",
    "\n",
    "# args = {\n",
    "#     \"data_folder\": \"../visuelle/\",\n",
    "#     \"log_dir\": \"log\",\n",
    "#     \"seed\": 21,\n",
    "#     \"epochs\": 200,\n",
    "#     \"gpu_num\": 0,\n",
    "\n",
    "#     \"model_type\": \"GTM\",\n",
    "#     \"use_trends\": 1,\n",
    "#     \"use_img\": 1,\n",
    "#     \"use_text\": 1,\n",
    "#     \"trend_len\": 52,\n",
    "#     \"num_trends\": 3,\n",
    "#     \"batch_size\": 128,\n",
    "#     \"embedding_dim\": 32,\n",
    "#     \"hidden_dim\": 64,\n",
    "#     \"output_dim\": 12,\n",
    "#     \"use_encoder_mask\": 1,\n",
    "#     \"autoregressive\": 0,\n",
    "#     \"num_attn_heads\": 4,\n",
    "#     \"num_hidden_layers\": 1,\n",
    "# }\n",
    "\n",
    "# run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
