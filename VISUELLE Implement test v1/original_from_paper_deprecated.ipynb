{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class ZeroShotDataset():\n",
    "    def __init__(self, data_df, img_root, gtrends, cat_dict, col_dict, fab_dict, trend_len):\n",
    "        self.data_df = data_df\n",
    "        self.gtrends = gtrends\n",
    "        self.cat_dict = cat_dict\n",
    "        self.col_dict = col_dict\n",
    "        self.fab_dict = fab_dict\n",
    "        self.trend_len = trend_len\n",
    "        self.img_root = img_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_df.iloc[idx, :]\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        data = self.data_df\n",
    "\n",
    "        # Get the Gtrends time series associated with each product\n",
    "        # Read the images (extracted image features) as well\n",
    "        gtrends, image_features = [], []\n",
    "        img_transforms = Compose([Resize((256, 256)), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        for (idx, row) in tqdm(data.iterrows(), total=len(data), ascii=True):\n",
    "            cat, col, fab, fiq_attr, start_date, img_path = row['category'], row['color'], row['fabric'], row['extra'], row['release_date'], row['image_path']\n",
    "\n",
    "            # Get the gtrend signal up to the previous year (52 weeks) of the release date\n",
    "            gtrend_start = start_date - pd.DateOffset(weeks=52)\n",
    "            cat_gtrend = self.gtrends.loc[gtrend_start:start_date][cat][-52:].values[:self.trend_len]\n",
    "            col_gtrend = self.gtrends.loc[gtrend_start:start_date][col][-52:].values[:self.trend_len]\n",
    "            fab_gtrend = self.gtrends.loc[gtrend_start:start_date][fab][-52:].values[:self.trend_len]\n",
    "\n",
    "            cat_gtrend = MinMaxScaler().fit_transform(cat_gtrend.reshape(-1,1)).flatten()\n",
    "            col_gtrend = MinMaxScaler().fit_transform(col_gtrend.reshape(-1,1)).flatten()\n",
    "            fab_gtrend = MinMaxScaler().fit_transform(fab_gtrend.reshape(-1,1)).flatten()\n",
    "            multitrends =  np.vstack([cat_gtrend, col_gtrend, fab_gtrend])\n",
    "\n",
    "\n",
    "            # Read images\n",
    "            img = Image.open(os.path.join(self.img_root, img_path)).convert('RGB')\n",
    "\n",
    "            # Append them to the lists\n",
    "            gtrends.append(multitrends)\n",
    "            image_features.append(img_transforms(img))\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        gtrends = np.array(gtrends)\n",
    "\n",
    "        # Remove non-numerical information\n",
    "        data.drop(['external_code', 'season', 'release_date', 'image_path'], axis=1, inplace=True)\n",
    "\n",
    "        # Create tensors for each part of the input/output\n",
    "        item_sales, temporal_features = torch.FloatTensor(data.iloc[:, :12].values), torch.FloatTensor(data.iloc[:, 13:17].values)\n",
    "        categories, colors, fabrics = [self.cat_dict[val] for val in data.iloc[:].category.values], \\\n",
    "                                       [self.col_dict[val] for val in data.iloc[:].color.values], \\\n",
    "                                       [self.fab_dict[val] for val in data.iloc[:].fabric.values]\n",
    "\n",
    "        \n",
    "        categories, colors, fabrics = torch.LongTensor(categories), torch.LongTensor(colors), torch.LongTensor(fabrics)\n",
    "        gtrends = torch.FloatTensor(gtrends)\n",
    "        images = torch.stack(image_features)\n",
    "\n",
    "        return TensorDataset(item_sales, categories, colors, fabrics, temporal_features, gtrends, images)\n",
    "\n",
    "    def get_loader(self, batch_size, train=True):\n",
    "        print('Starting dataset creation process...')\n",
    "        data_with_gtrends = self.preprocess_data()\n",
    "        data_loader = None\n",
    "        if train:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        else:\n",
    "            data_loader = DataLoader(data_with_gtrends, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        print('Done.')\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "# train_loader = ZeroShotDataset(train_df, Path(data_folder + '/images'), gtrends, cat_dict, col_dict,\n",
    "#                                 fab_dict, trend_len).get_loader(batch_size=batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-10-29 18:05:10 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import pipeline\n",
    "from torchvision import models\n",
    "from fairseq.optim.adafactor import Adafactor\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=52):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    # Takes any module and stacks the time dimension with the batch dimenison of inputs before applying the module\n",
    "    # Insipired from https://keras.io/api/layers/recurrent_layers/time_distributed/\n",
    "    # https://discuss.pytorch.org/t/any-pytorch-function-can-work-as-keras-timedistributed/1346/4\n",
    "    def __init__(self, module, batch_first=True):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module # Can be any layer we wish to apply like Linear, Conv etc\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  \n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "class FusionNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_img, use_text, dropout=0.2):\n",
    "        super(FusionNetwork, self).__init__()\n",
    "        \n",
    "        self.img_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.img_linear = nn.Linear(2048, embedding_dim)\n",
    "        self.use_img = use_img\n",
    "        self.use_text = use_text\n",
    "        input_dim = embedding_dim + (embedding_dim*use_img) + (embedding_dim*use_text)\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, input_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_encoding, text_encoding, dummy_encoding):\n",
    "        # Fuse static features together\n",
    "        pooled_img = self.img_pool(img_encoding)\n",
    "        condensed_img = self.img_linear(pooled_img.flatten(1))\n",
    "\n",
    "        # Build input\n",
    "        decoder_inputs = []\n",
    "        if self.use_img == 1:\n",
    "            decoder_inputs.append(condensed_img) \n",
    "        if self.use_text == 1:\n",
    "            decoder_inputs.append(text_encoding) \n",
    "        decoder_inputs.append(dummy_encoding)\n",
    "        concat_features = torch.cat(decoder_inputs, dim=1)\n",
    "\n",
    "        final = self.feature_fusion(concat_features)\n",
    "        # final = self.feature_fusion(dummy_encoding)\n",
    "\n",
    "        return final\n",
    "\n",
    "class GTrendEmbedder(nn.Module):\n",
    "    def __init__(self, forecast_horizon, embedding_dim, use_mask, trend_len, num_trends,  gpu_num):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.input_linear = TimeDistributed(nn.Linear(num_trends, embedding_dim))\n",
    "        self.pos_embedding = PositionalEncoding(embedding_dim, max_len=trend_len)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dropout=0.2)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.use_mask = use_mask\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def _generate_encoder_mask(self, size, forecast_horizon):\n",
    "        mask = torch.zeros((size, size))\n",
    "        split = math.gcd(size, forecast_horizon)\n",
    "        for i in range(0, size, split):\n",
    "            mask[i:i+split, i:i+split] = 1\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, gtrends):\n",
    "        gtrend_emb = self.input_linear(gtrends.permute(0,2,1))\n",
    "        gtrend_emb = self.pos_embedding(gtrend_emb.permute(1,0,2))\n",
    "        input_mask = self._generate_encoder_mask(gtrend_emb.shape[0], self.forecast_horizon)\n",
    "        if self.use_mask == 1:\n",
    "            gtrend_emb = self.encoder(gtrend_emb, input_mask)\n",
    "        else:\n",
    "            gtrend_emb = self.encoder(gtrend_emb)\n",
    "        return gtrend_emb\n",
    "        \n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim, cat_dict, col_dict, fab_dict, gpu_num):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.cat_dict = {v: k for k, v in cat_dict.items()}\n",
    "        self.col_dict = {v: k for k, v in col_dict.items()}\n",
    "        self.fab_dict = {v: k for k, v in fab_dict.items()}\n",
    "        self.word_embedder = pipeline('feature-extraction', model='bert-base-uncased')\n",
    "        self.fc = nn.Linear(768, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "    def forward(self, category, color, fabric):\n",
    "        textual_description = [self.col_dict[color.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.fab_dict[fabric.detach().cpu().numpy().tolist()[i]] + ' ' \\\n",
    "                + self.cat_dict[category.detach().cpu().numpy().tolist()[i]] for i in range(len(category))]\n",
    "\n",
    "\n",
    "        # Use BERT to extract features\n",
    "        word_embeddings = self.word_embedder(textual_description)\n",
    "\n",
    "        # BERT gives us embeddings for [CLS] ..  [EOS], which is why we only average the embeddings in the range [1:-1] \n",
    "        # We're not fine tuning BERT and we don't want the noise coming from [CLS] or [EOS]\n",
    "        word_embeddings = [torch.FloatTensor(x[0][1:-1]).mean(axis=0) for x in word_embeddings] \n",
    "        word_embeddings = torch.stack(word_embeddings).to('cuda:'+str(self.gpu_num))\n",
    "        \n",
    "        # Embed to our embedding space\n",
    "        word_embeddings = self.dropout(self.fc(word_embeddings))\n",
    "\n",
    "        return word_embeddings\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Img feature extraction\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Fine tune resnet\n",
    "        # for c in list(self.resnet.children())[6:]:\n",
    "        #     for p in c.parameters():\n",
    "        #         p.requires_grad = True\n",
    "        \n",
    "    def forward(self, images):        \n",
    "        img_embeddings = self.resnet(images)  \n",
    "        size = img_embeddings.size()\n",
    "        out = img_embeddings.view(*size[:2],-1)\n",
    "\n",
    "        return out.view(*size).contiguous() # batch_size, 2048, image_size/32, image_size/32\n",
    "\n",
    "class DummyEmbedder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.day_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.week_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.month_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.year_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.dummy_fusion = nn.Linear(embedding_dim*4, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "\n",
    "    def forward(self, temporal_features):\n",
    "        # Temporal dummy variables (day, week, month, year)\n",
    "        d, w, m, y = temporal_features[:, 0].unsqueeze(1), temporal_features[:, 1].unsqueeze(1), \\\n",
    "            temporal_features[:, 2].unsqueeze(1), temporal_features[:, 3].unsqueeze(1)\n",
    "        d_emb, w_emb, m_emb, y_emb = self.day_embedding(d), self.week_embedding(w), self.month_embedding(m), self.year_embedding(y)\n",
    "        temporal_embeddings = self.dummy_fusion(torch.cat([d_emb, w_emb, m_emb, y_emb], dim=1))\n",
    "        temporal_embeddings = self.dropout(temporal_embeddings)\n",
    "\n",
    "        return temporal_embeddings\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = F.relu\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask = None, memory_mask = None, tgt_key_padding_mask = None, \n",
    "            memory_key_padding_mask = None):\n",
    "\n",
    "        tgt2, attn_weights = self.multihead_attn(tgt, memory, memory)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt, attn_weights\n",
    "\n",
    "class GTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, num_heads, num_layers, use_text, use_img, \\\n",
    "                cat_dict, col_dict, fab_dict, trend_len, num_trends, gpu_num, use_encoder_mask=1, autoregressive=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_len = output_dim\n",
    "        self.use_encoder_mask = use_encoder_mask\n",
    "        self.autoregressive = autoregressive\n",
    "        self.gpu_num = gpu_num\n",
    "\n",
    "         # Encoder\n",
    "        self.dummy_encoder = DummyEmbedder(embedding_dim)\n",
    "        self.image_encoder = ImageEmbedder()\n",
    "        self.text_encoder = TextEmbedder(embedding_dim, cat_dict, col_dict, fab_dict, gpu_num)\n",
    "        self.gtrend_encoder = GTrendEmbedder(output_dim, hidden_dim, use_encoder_mask, trend_len, num_trends, gpu_num)\n",
    "        self.static_feature_encoder = FusionNetwork(embedding_dim, hidden_dim, use_img, use_text)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_linear = TimeDistributed(nn.Linear(1, hidden_dim))\n",
    "        decoder_layer = TransformerDecoderLayer(d_model=self.hidden_dim, nhead=num_heads, \\\n",
    "                                                dim_feedforward=self.hidden_dim * 4, dropout=0.1)\n",
    "        \n",
    "        if self.autoregressive: self.pos_encoder = PositionalEncoding(hidden_dim, max_len=12)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        self.decoder_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, self.output_len if not self.autoregressive else 1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def _generate_square_subsequent_mask(self, size):\n",
    "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to('cuda:'+str(self.gpu_num))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, category, color, fabric, temporal_features, gtrends, images):\n",
    "        # Encode features and get inputs\n",
    "        img_encoding = self.image_encoder(images)\n",
    "        dummy_encoding = self.dummy_encoder(temporal_features)\n",
    "        text_encoding = self.text_encoder(category, color, fabric)\n",
    "        gtrend_encoding = self.gtrend_encoder(gtrends)\n",
    "\n",
    "        # Fuse static features together\n",
    "        static_feature_fusion = self.static_feature_encoder(img_encoding, text_encoding, dummy_encoding)\n",
    "\n",
    "        if self.autoregressive == 1:\n",
    "            # Decode\n",
    "            tgt = torch.zeros(self.output_len, gtrend_encoding.shape[1], gtrend_encoding.shape[-1]).to('cuda:'+str(self.gpu_num))\n",
    "            tgt[0] = static_feature_fusion\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "            tgt_mask = self._generate_square_subsequent_mask(self.output_len)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory, tgt_mask)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "        else:\n",
    "            # Decode (generatively/non-autoregressively)\n",
    "            tgt = static_feature_fusion.unsqueeze(0)\n",
    "            memory = gtrend_encoding\n",
    "            decoder_out, attn_weights = self.decoder(tgt, memory)\n",
    "            forecast = self.decoder_fc(decoder_out)\n",
    "\n",
    "        return forecast.view(-1, self.output_len), attn_weights\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adafactor(self.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "    \n",
    "        return [optimizer]\n",
    "\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = train_batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, test_batch, batch_idx):\n",
    "        item_sales, category, color, fabric, temporal_features, gtrends, images = test_batch \n",
    "        forecasted_sales, _ = self.forward(category, color, fabric, temporal_features, gtrends, images)\n",
    "        \n",
    "        return item_sales.squeeze(), forecasted_sales.squeeze()\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        item_sales, forecasted_sales = [x[0] for x in val_step_outputs], [x[1] for x in val_step_outputs]\n",
    "        item_sales, forecasted_sales = torch.stack(item_sales), torch.stack(forecasted_sales)\n",
    "        rescaled_item_sales, rescaled_forecasted_sales = item_sales*1065, forecasted_sales*1065 # 1065 is the normalization factor (max of the sales of the training set)\n",
    "        loss = F.mse_loss(item_sales, forecasted_sales.squeeze())\n",
    "        mae = F.l1_loss(rescaled_item_sales, rescaled_forecasted_sales)\n",
    "        self.log('val_mae', mae)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "        print('Validation MAE:', mae.detach().cpu().numpy(), 'LR:', self.optimizers().param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5080/5080 [00:54<00:00, 92.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Starting dataset creation process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 497/497 [00:06<00:00, 74.15it/s]\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import wandb\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "data_folder='..//visuelle/'\n",
    "log_dir='log'\n",
    "seed=21\n",
    "epochs=200\n",
    "gpu_num=0\n",
    "\n",
    "model_type='GTM'\n",
    "use_trends=1\n",
    "use_img=1\n",
    "use_text=1\n",
    "trend_len=52\n",
    "num_trends=3\n",
    "batch_size=128\n",
    "embedding_dim=32\n",
    "hidden_dim=64\n",
    "output_dim=12\n",
    "use_encoder_mask=1\n",
    "autoregressive=0\n",
    "num_attn_heads=4\n",
    "num_hidden_layers=1\n",
    "\n",
    "wandb_entity='username-here'\n",
    "wandb_proj='GTM'\n",
    "wandb_run='Run1'\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Seeds for reproducibility (By default we use the number 21)\n",
    "pl.seed_everything(seed)\n",
    "\n",
    "# Load sales data\n",
    "train_df = pd.read_csv(Path(data_folder + 'train.csv'), parse_dates=['release_date'])\n",
    "test_df = pd.read_csv(Path(data_folder + 'test.csv'), parse_dates=['release_date'])\n",
    "\n",
    "# Load category and color encodings\n",
    "cat_dict = torch.load(Path(data_folder + 'category_labels.pt'))\n",
    "col_dict = torch.load(Path(data_folder + 'color_labels.pt'))\n",
    "fab_dict = torch.load(Path(data_folder + 'fabric_labels.pt'))\n",
    "\n",
    "# Load Google trends\n",
    "gtrends = pd.read_csv(Path(data_folder + 'gtrends.csv'), index_col=[0], parse_dates=True)\n",
    "\n",
    "train_loader = ZeroShotDataset(train_df, Path(data_folder + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, trend_len).get_loader(batch_size=batch_size, train=True)\n",
    "test_loader = ZeroShotDataset(test_df, Path(data_folder + '/images'), gtrends, cat_dict, col_dict,\n",
    "                                fab_dict, trend_len).get_loader(batch_size=1, train=False)\n",
    "\n",
    "# Create model\n",
    "model = GTM(\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_heads=num_attn_heads,\n",
    "    num_layers=num_hidden_layers,\n",
    "    cat_dict=cat_dict,\n",
    "    col_dict=col_dict,\n",
    "    fab_dict=fab_dict,\n",
    "    use_text=use_text,\n",
    "    use_img=use_img,\n",
    "    trend_len=trend_len,\n",
    "    num_trends=num_trends,\n",
    "    use_encoder_mask=use_encoder_mask,\n",
    "    autoregressive=autoregressive,\n",
    "    gpu_num=gpu_num\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5/40  train_loss: 1540.7533976236986"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original_from_paper.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m epoch \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epoch):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     mean_train_loss \u001b[39m=\u001b[39m train(); train_loss_li\u001b[39m.\u001b[39mappend(mean_train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     mean_valid_loss \u001b[39m=\u001b[39m val(); valid_loss_li\u001b[39m.\u001b[39mappend(mean_valid_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m  train_loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(mean_train_loss)\u001b[39m}\u001b[39;00m\u001b[39m, valid_loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(mean_valid_loss)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original_from_paper.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m train_y, category, color, fabric, temporal_features, gtrends, images \u001b[39m=\u001b[39m train_data\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m train_pred, _ \u001b[39m=\u001b[39m model(category\u001b[39m.\u001b[39;49mto(device), color\u001b[39m.\u001b[39;49mto(device), fabric\u001b[39m.\u001b[39;49mto(device), temporal_features\u001b[39m.\u001b[39;49mto(device), gtrends\u001b[39m.\u001b[39;49mto(device), images\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Get train loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m train_loss \u001b[39m=\u001b[39m loss_fn(train_pred, train_y\u001b[39m.\u001b[39mto(device)) \u001b[39m# Shape: (batch_size, predict_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original_from_paper.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=270'>271</a>\u001b[0m img_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_encoder(images)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=271'>272</a>\u001b[0m dummy_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdummy_encoder(temporal_features)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=272'>273</a>\u001b[0m text_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder(category, color, fabric)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=273'>274</a>\u001b[0m gtrend_encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgtrend_encoder(gtrends)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=275'>276</a>\u001b[0m \u001b[39m# Fuse static features together\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/sh-sungho.park/Test/Paper/VISUELLE Implement test v1/original_from_paper.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m textual_description \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_dict[color\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfab_dict[fabric\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcat_dict[category\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()[i]] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(category))]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# Use BERT to extract features\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embedder(textual_description)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39m# BERT gives us embeddings for [CLS] ..  [EOS], which is why we only average the embeddings in the range [1:-1] \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# We're not fine tuning BERT and we don't want the noise coming from [CLS] or [EOS]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu.isdow.com/home/sh-sungho.park/Test/Paper/VISUELLE%20Implement%20test%20v1/original_from_paper.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m word_embeddings \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mFloatTensor(x[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_embeddings] \n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/feature_extraction.py:107\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    Extract the features of the input(s).\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m        A nested list of `float`: The features computed by the model.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pipelines/feature_extraction.py:85\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m---> 85\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    605\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/pytorch_utils.py:240\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss_fn_ = torch.nn.MSELoss(reduction=\"none\")\n",
    "train_loss_li, valid_loss_li = [], []\n",
    "temp = None\n",
    "\n",
    "def plot_loss(train_loss_li, valid_loss_li):\n",
    "    plt.plot(train_loss_li, label=\"train\")\n",
    "    plt.plot(valid_loss_li, label=\"valid\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "def plot_bestsample(loss, pred, y, iter, msg):\n",
    "    loss = loss.mean(axis=1) # Shape: (batch_size, )\n",
    "    _, best_idx_li = torch.sort(loss)\n",
    "    for best_idx in best_idx_li:\n",
    "        # best_pred = torch.round(pred[best_idx])\n",
    "        best_pred = pred[best_idx]\n",
    "        best_pred = best_pred.cpu().detach().numpy() # Sales is always int  Shape: (predict_length, )\n",
    "        best_pred[best_pred < 0] = 0 # Sales never becomes negative\n",
    "        best_y = y[best_idx].cpu().detach().numpy()\n",
    "        if (np.max(best_y) < 10): # If predicted value is all 0, consider as not the best\n",
    "            continue\n",
    "        break\n",
    "    \n",
    "    plt.plot(best_pred, label=\"pred\")\n",
    "    plt.plot(best_y, label=\"y\", color=\"gray\", alpha=0.3)\n",
    "    plt.title(f\"{iter}th iter: Best example amongst {msg} dataset\")\n",
    "    plt.legend()\n",
    "\n",
    "def train():\n",
    "    global temp\n",
    "    total_train_loss = 0\n",
    "    # for n, (train_data, valid_data) in enumerate(zip(train_dataloader, valid_dataloader)):\n",
    "    for n, train_data in enumerate(train_loader):\n",
    "        # Train\n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_y, category, color, fabric, temporal_features, gtrends, images = train_data\n",
    "        train_pred, _ = model(category.to(device), color.to(device), fabric.to(device), temporal_features.to(device), gtrends.to(device), images.to(device))\n",
    "\n",
    "        # Get train loss\n",
    "        train_loss = loss_fn(train_pred, train_y.to(device)) # Shape: (batch_size, predict_length)\n",
    "        train_loss.backward()\n",
    "        total_train_loss += train_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        print(f\"\\r {n}/{len(train_loader)}  train_loss: {total_train_loss / (n+1)}\", end=\"\")\n",
    "    return total_train_loss / len(train_loader)\n",
    "\n",
    "def val():\n",
    "    total_valid_loss = 0\n",
    "    # for n, (train_data, valid_data) in enumerate(zip(train_dataloader, valid_dataloader)):\n",
    "    for n, valid_data in enumerate(test_loader):\n",
    "        model.eval()\n",
    "        # Predict\n",
    "        valid_y, category, color, fabric, temporal_features, gtrends, images = valid_data\n",
    "        valid_pred, _ = model(category.to(device), color.to(device), fabric.to(device), temporal_features.to(device), gtrends.to(device), images.to(device))\n",
    "\n",
    "        # Get train loss\n",
    "        valid_loss = loss_fn(valid_pred, valid_y.to(device)) # Shape: (batch_size, predict_length)\n",
    "        total_valid_loss += valid_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report\n",
    "        print(f\"\\r {n}/{len(test_loader)}  valid_loss: {total_valid_loss / (n+1)}\", end=\"\")\n",
    "    return total_valid_loss / len(test_loader)\n",
    "\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    mean_train_loss = train(); train_loss_li.append(mean_train_loss)\n",
    "    mean_valid_loss = val(); valid_loss_li.append(mean_valid_loss)\n",
    "    print(f\"\\r {e}/{epoch}  train_loss: {np.mean(mean_train_loss)}, valid_loss: {np.mean(mean_valid_loss)}\", end=\"\")\n",
    "    clear_output(wait=True)\n",
    "    plot_loss(train_loss_li, valid_loss_li)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
