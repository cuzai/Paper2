{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from transformers import ViTConfig, ViTModel\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 365\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"]\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 32\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "is_identical = False\n",
    "\n",
    "d_model = {\"encoder\":64, \"decoder\":32}\n",
    "d_ff = {\"encoder\":64, \"decoder\":32}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"target\": 0.25, \"temporal\":0.25, \"img\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 355.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 365, 1])\n",
      "sales_remain_idx torch.Size([2, 91])\n",
      "sales_masked_idx torch.Size([2, 274])\n",
      "sales_revert_idx torch.Size([2, 365])\n",
      "sales_remain_padding_mask torch.Size([2, 91])\n",
      "sales_masked_padding_mask torch.Size([2, 274])\n",
      "sales_revert_padding_mask torch.Size([2, 365])\n",
      "day torch.Size([2, 365])\n",
      "day_remain_idx torch.Size([2, 91])\n",
      "day_masked_idx torch.Size([2, 274])\n",
      "day_revert_idx torch.Size([2, 365])\n",
      "day_remain_padding_mask torch.Size([2, 91])\n",
      "day_masked_padding_mask torch.Size([2, 274])\n",
      "day_revert_padding_mask torch.Size([2, 365])\n",
      "dow torch.Size([2, 365])\n",
      "dow_remain_idx torch.Size([2, 91])\n",
      "dow_masked_idx torch.Size([2, 274])\n",
      "dow_revert_idx torch.Size([2, 365])\n",
      "dow_remain_padding_mask torch.Size([2, 91])\n",
      "dow_masked_padding_mask torch.Size([2, 274])\n",
      "dow_revert_padding_mask torch.Size([2, 365])\n",
      "month torch.Size([2, 365])\n",
      "month_remain_idx torch.Size([2, 91])\n",
      "month_masked_idx torch.Size([2, 274])\n",
      "month_revert_idx torch.Size([2, 365])\n",
      "month_remain_padding_mask torch.Size([2, 91])\n",
      "month_masked_padding_mask torch.Size([2, 274])\n",
      "month_revert_padding_mask torch.Size([2, 365])\n",
      "holiday torch.Size([2, 365])\n",
      "holiday_remain_idx torch.Size([2, 91])\n",
      "holiday_masked_idx torch.Size([2, 274])\n",
      "holiday_revert_idx torch.Size([2, 365])\n",
      "holiday_remain_padding_mask torch.Size([2, 91])\n",
      "holiday_masked_padding_mask torch.Size([2, 274])\n",
      "holiday_revert_padding_mask torch.Size([2, 365])\n",
      "price torch.Size([2, 365, 1])\n",
      "price_remain_idx torch.Size([2, 91])\n",
      "price_masked_idx torch.Size([2, 274])\n",
      "price_revert_idx torch.Size([2, 365])\n",
      "price_remain_padding_mask torch.Size([2, 91])\n",
      "price_masked_padding_mask torch.Size([2, 274])\n",
      "price_revert_padding_mask torch.Size([2, 365])\n",
      "img_input torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=24, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"img_raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def get_positional_encoding(d_model, seq_len=1000):\n",
    "    position = torch.arange(seq_len).reshape(-1,1)\n",
    "    i = torch.arange(d_model)//2\n",
    "    exp_term = 2*i/d_model\n",
    "    div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "    pos_encoded = position / div_term\n",
    "\n",
    "    pos_encoded[:, 0::2] = torch.sin(pos_encoded[:, 0::2])\n",
    "    pos_encoded[:, 1::2] = torch.cos(pos_encoded[:, 1::2])\n",
    "\n",
    "    return pos_encoded\n",
    "\n",
    "# def get_positional_encoding(d_hidn, n_seq=1000):\n",
    "#     def cal_angle(position, i_hidn):\n",
    "#         return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n",
    "#     def get_posi_angle_vec(position):\n",
    "#         return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n",
    "\n",
    "#     sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n",
    "#     sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin \n",
    "#     sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n",
    "\n",
    "#     res = torch.tensor(sinusoid_table).to(torch.float32)\n",
    "#     print(res.dtype)\n",
    "#     return res\n",
    "\n",
    "class NoneEmbedding(torch.nn.Module):\n",
    "    def forward(self, data):\n",
    "        if len(data.shape) == 3:\n",
    "            return data.to(torch.float)\n",
    "        else:\n",
    "            return data.unsqueeze(-1).to(torch.float)\n",
    "\n",
    "class NumericalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, is_identical):\n",
    "        super().__init__()\n",
    "        self.linear_embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        if is_identical: self.linear_embedding = NoneEmbedding()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.linear_embedding(data)\n",
    "\n",
    "class CategoricalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_cls, d_model, is_identical):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "        \n",
    "        if is_identical: self.embedding = NoneEmbedding()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.embedding(data)\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # config = ViTConfig(hidden_size=d_model, num_hidden_layers=num_layers, num_attention_heads=nhead, intermediate_size=d_ff, patch_size=10, encoder_stride=10)\n",
    "        # self.img_model = ViTModel(config)\n",
    "        self.img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.linear = torch.nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        imgimg_attnoutput, imgimg_attnweight1, imgimg_attnweight2 = self.img_model(data, output_attentions=True).values()\n",
    "        imgimg_attnoutput = self.linear(imgimg_attnoutput)\n",
    "\n",
    "        return imgimg_attnoutput\n",
    "\n",
    "class DynamicOutput(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            # torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, 1)\n",
    "                            )\n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            # torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, num_cls)\n",
    "                            )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \n",
    "        return self.output(data)\n",
    "\n",
    "class MultiheadBlockAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model, self.nhead = d_model, nhead\n",
    "        \n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0,3,1,2,4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn = torch.nn.functional.softmax(QK/math.sqrt(self.d_model//self.nhead), dim=-1)\n",
    "        \n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn @ V\n",
    "        \n",
    "        # Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, -1, seq_len, d_model)\n",
    "        attn_output = attn_output.squeeze()\n",
    "\n",
    "        return attn_output, attn\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, activation):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class CustomDecoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, activation, dropout):\n",
    "        super().__init__()\n",
    "        # self.cross_attn = torch.nn.MultiheadAttention(d_model, num_heads, dropout, batch_first=True)\n",
    "        self.cross_attn = MultiheadBlockAttention(d_model, num_heads, dropout)\n",
    "        self.mlp = torch.nn.Linear(d_model, d_model)\n",
    "        self.mha = torch.nn.MultiheadAttention(d_model, num_heads, dropout, batch_first=True)\n",
    "        self.ff = FeedForward(d_model, d_ff, activation)\n",
    "\n",
    "        self.layernorm_tgt = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm_mem = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(d_model)\n",
    "\n",
    "    \n",
    "    def _get_padding_mask(self, padding_mask, memory, device):\n",
    "        padding_mask_li = []\n",
    "        for col, val in memory.items():\n",
    "            padding_mask_li.append(padding_mask[f\"{col}_revert_padding_mask\"])\n",
    "        \n",
    "        result = torch.cat(padding_mask_li, dim=1).to(device)\n",
    "        result = torch.where(result == 1, 0, -torch.inf)\n",
    "        return result\n",
    "    \n",
    "    def forward(self, col, tgt, memory, padding_mask, device):\n",
    "        tgt = tgt.unsqueeze(-2)\n",
    "        # memory = torch.stack(list(memory.values()), dim=-2)\n",
    "        \n",
    "        tgt = self.layernorm_tgt(tgt)\n",
    "        memory = self.layernorm_mem(memory)\n",
    "        \n",
    "        cross_attn, cross_attn_weight = self.cross_attn(query=tgt, key=memory, value=memory)\n",
    "\n",
    "        cross_attn = tgt.squeeze() + cross_attn\n",
    "\n",
    "        padding_mask = padding_mask[f\"{col}_revert_padding_mask\"]\n",
    "        padding_mask = torch.where(padding_mask == 1, 0, -torch.inf)\n",
    "        \n",
    "        cross_attn = self.layernorm1(cross_attn)\n",
    "        self_attn, self_attn_weight = self.mha(cross_attn, cross_attn, cross_attn, key_padding_mask=padding_mask) # multiheadattention\n",
    "\n",
    "        self_attn = self_attn + cross_attn\n",
    "\n",
    "        # ff\n",
    "        self_attn = self.layernorm2(self_attn)\n",
    "        ff = self_attn + self.ff(self_attn)\n",
    "        \n",
    "        return ff, self_attn_weight, cross_attn_weight\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                    d_model, num_layers, nhead, d_ff, dropout,\n",
    "                    is_identical=False):\n",
    "        super().__init__()\n",
    "        activation = \"gelu\"\n",
    "        self.data_info, self.label_encoder_dict, self.is_identical = data_info, label_encoder_dict, is_identical\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.numerical_embedding_dict = self._init_numerical_embedding_dict(d_model[\"encoder\"])\n",
    "        self.categorical_embedding_dict = self._init_categorical_embedding_dict(d_model[\"encoder\"])\n",
    "        self.img_embedding = ImgEmbedding(d_model[\"encoder\"])\n",
    "\n",
    "        # 3. Apply remain positional encoding\n",
    "        self.encoder_pos_enc = torch.nn.Parameter(get_positional_encoding(d_model[\"encoder\"]), requires_grad=False)  if not self.is_identical else torch.nn.Parameter(torch.zeros(1000, 1))\n",
    "\n",
    "        # 4. Apply modality embedding\n",
    "        self.num_modality = len(self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"] + self.data_info.modality_info[\"img\"])\n",
    "        self.encoder_mod_emb = torch.nn.Embedding(self.num_modality, d_model[\"encoder\"])  if not self.is_identical else NoneEmbedding()\n",
    "\n",
    "        # 5. Encoding\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model[\"encoder\"], nhead=nhead, dim_feedforward=d_ff[\"encoder\"], dropout=dropout, batch_first=True, activation=activation, norm_first=True), num_layers[\"encoder\"])  if not self.is_identical else NoneEmbedding()\n",
    "        self.to_decoder_dim = torch.nn.Linear(d_model[\"encoder\"], d_model[\"decoder\"])  if not self.is_identical else NoneEmbedding()\n",
    "\n",
    "        # 7. Revert\n",
    "        self.mask_token = torch.nn.Parameter(torch.rand(1, d_model[\"decoder\"])) if not self.is_identical else torch.nn.Parameter(torch.zeros(1, 1) + 99)\n",
    "\n",
    "        # 8. Apply revert positional encoding\n",
    "        self.decoder_pos_enc = torch.nn.Parameter(get_positional_encoding(d_model[\"decoder\"]), requires_grad=False)  if not self.is_identical else torch.nn.Parameter(torch.zeros(1000, 1))\n",
    "\n",
    "        # 9. Apply modality embedding\n",
    "        self.decoder_mod_emb = torch.nn.Embedding(self.num_modality, d_model[\"decoder\"])  if not self.is_identical else NoneEmbedding()\n",
    "\n",
    "        # 10. Decoding\n",
    "        self.custom_decoder = self._init_custom_deocder_dict(d_model[\"decoder\"], nhead, d_ff[\"decoder\"], activation, dropout)\n",
    "        self.img_decoder = torch.nn.TransformerDecoder(torch.nn.TransformerDecoderLayer(d_model=d_model[\"decoder\"], nhead=nhead, dim_feedforward=d_ff[\"decoder\"], dropout=dropout, batch_first=True, activation=activation, norm_first=True), num_layers[\"decoder\"])\n",
    "        \n",
    "        # 11. Output\n",
    "        self.temporal_output = self._init_temporal_output(d_model[\"decoder\"])\n",
    "        patch_size = 16\n",
    "        self.img_output = torch.nn.Sequential(torch.nn.Linear(d_model[\"decoder\"], d_model[\"decoder\"]), torch.nn.Linear(d_model[\"decoder\"], d_model[\"decoder\"]), torch.nn.Linear(d_model[\"decoder\"], 3*patch_size*patch_size))\n",
    "        \n",
    "    def forward(self, input_data_dict, remain_rto, device):\n",
    "        # 0. Data to gpu\n",
    "        data_dict, idx_dict, padding_mask_dict = self._to_gpu(input_data_dict, device)\n",
    "\n",
    "        # 1. Embedding\n",
    "        embedding_dict = {}\n",
    "        embedding_dict.update(self._apply_numerical_embedding(data_dict))\n",
    "        embedding_dict.update(self._apply_categorical_embedding(data_dict))\n",
    "        embedding_dict.update(self._apply_img_embedding(data_dict))\n",
    "\n",
    "        # 2. Apply remain\n",
    "        temporal_remain = self._apply_temporal_remain(embedding_dict, idx_dict)\n",
    "        img_remain, img_remain_idx, img_masked_idx, img_revert_idx = self._apply_img_remain(embedding_dict[\"img_input\"])\n",
    "        idx_dict[\"img_remain_idx\"] = img_remain_idx\n",
    "        idx_dict[\"img_masked_idx\"] = img_masked_idx\n",
    "        idx_dict[\"img_revert_idx\"] = img_revert_idx\n",
    "\n",
    "        # 3. Apply temporal remain positional encoding\n",
    "        temporal_remain = self._apply_remain_temporal_positional_encoding(temporal_remain, idx_dict, self.encoder_pos_enc)\n",
    "\n",
    "        # 4. Apply modality embedding\n",
    "        temporal_remain, img_remain = self._apply_modality_embedding(temporal_remain, img_remain, self.encoder_mod_emb, device)\n",
    "\n",
    "        # 5. Encoding\n",
    "        temporal_encoder_padding_mask = self._get_temporal_padding_mask(padding_mask_dict, temporal_remain, device, mode=\"remain\")\n",
    "        img_encoder_padding_mask = torch.ones(img_remain.shape[:-1]).to(device)\n",
    "        padding_mask_dict[\"img_remain_padding_mask\"] = img_encoder_padding_mask\n",
    "        encoder_padding_mask = torch.cat([temporal_encoder_padding_mask, img_encoder_padding_mask], dim=1)\n",
    "        \n",
    "        concat = torch.cat(list(temporal_remain.values()), dim=1)\n",
    "        concat = torch.cat([concat, img_remain], dim=1)\n",
    "\n",
    "        encoded = self.encoder(concat, src_key_padding_mask=encoder_padding_mask)\n",
    "        encoded = self.to_decoder_dim(encoded)\n",
    "\n",
    "        # 6. Split\n",
    "        temporal_encoded, img_encoded = self._split_modalities(encoded, temporal_remain, img_remain)\n",
    "\n",
    "        # 7. Revert\n",
    "        temporal_reverted = self._apply_temporal_revert(temporal_encoded, idx_dict, padding_mask_dict)\n",
    "        img_reverted = self._apply_temporal_revert({\"img\":img_encoded}, idx_dict, padding_mask_dict)[\"img\"]\n",
    "\n",
    "        # 8. Apply revert positional encoding\n",
    "        temporal_reverted = self._apply_reverted_temporal_positional_encoding(temporal_reverted, idx_dict, self.decoder_pos_enc)\n",
    "        img_reverted = img_reverted + get_positional_encoding(d_model[\"decoder\"])[:img_reverted.shape[1], :].to(device)\n",
    "\n",
    "        # 9. Apply modality embedding\n",
    "        temporal_reverted, img_reverted = self._apply_modality_embedding(temporal_reverted, img_reverted, self.decoder_mod_emb, device)\n",
    "\n",
    "        # 10. Decoding\n",
    "        temporal_decoded, img_decoded, self_attn_dict, cross_attn_dict = self._apply_custom_decoder(temporal_reverted, img_reverted, padding_mask_dict, device)\n",
    "\n",
    "        # 12. Ouptut\n",
    "        temporal_output = self._apply_temporal_output(temporal_decoded)\n",
    "        img_output = self.img_output(img_decoded)\n",
    "\n",
    "        return temporal_output, img_output, self_attn_dict, cross_attn_dict, idx_dict\n",
    "\n",
    "    \n",
    "    def _to_gpu(self, data, device):\n",
    "        data_dict = {}\n",
    "        idx_dict = {}\n",
    "        padding_mask_dict = {}\n",
    "\n",
    "        for col in data.keys():\n",
    "            if col.endswith(\"padding_mask\"):\n",
    "                padding_mask_dict[col] = data[col].to(device)\n",
    "            \n",
    "            elif col.endswith(\"idx\"):\n",
    "                idx_dict[col] = data[col].to(device)\n",
    "\n",
    "            elif col in self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]:\n",
    "                data_dict[col] = data[col].to(device)\n",
    "            \n",
    "            elif col == \"img_input\":\n",
    "                img_input = data[\"img_input\"]\n",
    "                data_dict[\"img_input\"] = img_input.to(device)\n",
    "                padding_mask_dict[\"img_input\"] = torch.ones(img_input.shape).to(device)\n",
    "        \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "    \n",
    "    def _init_numerical_embedding_dict(self, d_model):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.processing_info[\"scaling_cols\"]\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = NumericalEmbedding(d_model, self.is_identical) if not self.is_identical else NoneEmbedding()\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def _init_categorical_embedding_dict(self, d_model):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.processing_info[\"embedding_cols\"]\n",
    "        for col in target_cols:\n",
    "            num_cls = self.label_encoder_dict[col].get_num_cls()\n",
    "            result_dict[col] = CategoricalEmbedding(num_cls, d_model, self.is_identical) if not self.is_identical else NoneEmbedding()\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def _init_temporal_output(self, d_model):\n",
    "        result_dict = {}\n",
    "        for col in self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]:\n",
    "            result_dict[col] = DynamicOutput(col, self.data_info, self.label_encoder_dict, d_model) if not self.is_identical else NoneEmbedding()\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def _init_custom_deocder_dict(self, d_model, num_heads, d_ff, activation, dropout):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = torch.nn.ModuleList([CustomDecoder(d_model, num_heads, d_ff, activation, dropout),\n",
    "                                                    CustomDecoder(d_model, num_heads, d_ff, activation, dropout),\n",
    "                                                    # CustomDecoder(d_model, num_heads, d_ff, activation, dropout),\n",
    "                                                    # CustomDecoder(d_model, num_heads, d_ff, activation, dropout)\n",
    "                                                    ])\n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "\n",
    "    def _apply_numerical_embedding(self, data):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.processing_info[\"scaling_cols\"]\n",
    "        \n",
    "        for col in target_cols:\n",
    "            result_dict[col] = self.numerical_embedding_dict[col](data[col])\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def _apply_categorical_embedding(self, data):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.processing_info[\"embedding_cols\"]\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = self.categorical_embedding_dict[col](data[col])\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def _apply_img_embedding(self, data):\n",
    "        result_dict = {}\n",
    "        result_dict[\"img_input\"] = self.img_embedding(data[\"img_input\"])\n",
    "        return result_dict\n",
    "\n",
    "    def _apply_temporal_remain(self, embedding_dict, idx_dict):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        for col, val in {key:val for key, val in embedding_dict.items() if key in target_cols}.items():\n",
    "            # Get remain data\n",
    "            remain_idx = idx_dict[f\"{col}_remain_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            val = torch.gather(val, index=remain_idx, dim=1)\n",
    "\n",
    "            result_dict[col] = val\n",
    "        return result_dict\n",
    "\n",
    "    def _apply_img_remain(self, data):\n",
    "        # Positional encoding\n",
    "        data = data[:, 1:, :]\n",
    "        pos_enc = get_positional_encoding(d_model[\"encoder\"]).to(device)\n",
    "        data += pos_enc[:data.shape[1], :]\n",
    "\n",
    "        # Apply mask\n",
    "        num_remain = int(data.shape[1] * remain_rto[\"img\"])\n",
    "        \n",
    "        noise = torch.rand(data.shape[:-1]).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=1)\n",
    "        \n",
    "        remain_idx = shuffle_idx[:, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "        remain_data = torch.gather(data, index=remain_idx.unsqueeze(-1).repeat(1, 1, data.shape[-1]), dim=1)\n",
    "\n",
    "        return remain_data, remain_idx, masked_idx, revert_idx\n",
    "\n",
    "    def _apply_remain_temporal_positional_encoding(self, temporal_embedding, idx_dict, pos_enc):\n",
    "        result_dict = {}\n",
    "        \n",
    "        for col, val in temporal_embedding.items():\n",
    "            # Get remain pos_enc\n",
    "            remain_idx = idx_dict[f\"{col}_remain_idx\"].unsqueeze(-1).repeat(1, 1, pos_enc.shape[-1])\n",
    "            remain_pos_enc = torch.gather(pos_enc.unsqueeze(0).repeat(val.shape[0], 1, 1), index=remain_idx, dim=1)\n",
    "            \n",
    "            result_dict[col] = val + remain_pos_enc\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def _apply_modality_embedding(self, temporal_embedding, img_embedding, mod_emb, device):\n",
    "        temporal_embedding_result = {}\n",
    "        modality_idx = 0\n",
    "\n",
    "        # Temporal\n",
    "        for col, val in temporal_embedding.items():\n",
    "            modality = torch.zeros(val.shape[1]).to(device) + modality_idx\n",
    "            modality = mod_emb(modality.to(torch.int))\n",
    "            temporal_embedding_result[col] = val + modality\n",
    "\n",
    "            modality_idx += 1\n",
    "        \n",
    "        # Img\n",
    "        modality = torch.zeros(img_embedding.shape[1]).to(device) + modality_idx\n",
    "        modality = mod_emb(modality.to(torch.int))\n",
    "        img_embedding_result = img_embedding + modality\n",
    "        modality_idx += 1\n",
    "\n",
    "        assert modality_idx == self.num_modality\n",
    "        return temporal_embedding_result, img_embedding_result\n",
    "\n",
    "    def _get_temporal_padding_mask(self, padding_mask_dict, temporal_data, device, mode):\n",
    "        padding_mask_li = []\n",
    "        for col, val in temporal_data.items():\n",
    "            padding_mask_li.append(padding_mask_dict[f\"{col}_{mode}_padding_mask\"])\n",
    "        \n",
    "        result = torch.cat(padding_mask_li, dim=1).to(device)\n",
    "        result = torch.where(result == 1, 0, -torch.inf)\n",
    "        return result   \n",
    "\n",
    "    def _split_modalities(self, data, temporal_data, img_data):\n",
    "        temporal_result_dict = {}\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Temporal\n",
    "        for col, val in temporal_data.items():\n",
    "            length = val.shape[1]\n",
    "            temporal_result_dict[col] = data[:, start_idx: start_idx+length, :]\n",
    "            start_idx += length\n",
    "        \n",
    "        # Img\n",
    "        length = img_data.shape[1]\n",
    "        img_result = data[:, start_idx:start_idx+length, :]\n",
    "        start_idx += length\n",
    "\n",
    "        \n",
    "        assert start_idx == data.shape[1]\n",
    "        return temporal_result_dict, img_result\n",
    "\n",
    "    def _apply_temporal_revert(self, temporal_encoded, idx_dict, padding_mask_dict):\n",
    "        result_dict = {}\n",
    "\n",
    "        for col, val in temporal_encoded.items():\n",
    "            # Replace remain padding to mask token\n",
    "            remain_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            val = torch.where(remain_padding_mask==1, val, self.mask_token)\n",
    "            \n",
    "            # Append mask token\n",
    "            revert_idx = idx_dict[f\"{col}_revert_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            mask_tokens = self.mask_token.unsqueeze(0).repeat(val.shape[0],\n",
    "                                                    revert_idx.shape[1] - val.shape[1],\n",
    "                                                    1)\n",
    "            val_with_mask_token = torch.cat([val, mask_tokens], dim=1)\n",
    "            assert val_with_mask_token.shape == revert_idx.shape\n",
    "\n",
    "            # Apply revert\n",
    "            reverted_val = torch.gather(val_with_mask_token, index=revert_idx, dim=1)\n",
    "\n",
    "            result_dict[col] = reverted_val\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def _apply_reverted_temporal_positional_encoding(self, temporal_reverted, idx_dict, pos_enc):\n",
    "        result_dict = {}\n",
    "        \n",
    "        for col, val in temporal_reverted.items():\n",
    "            result_dict[col] = val + pos_enc.unsqueeze(0).repeat(val.shape[0], 1, 1)[:, :val.shape[1], :]\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def _apply_temporal_output(self, temporal_decoded):\n",
    "        result_dict = {}\n",
    "        for col, val in temporal_decoded.items():\n",
    "            result_dict[col] = self.temporal_output[col](val)\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def _apply_custom_decoder(self, temporal, img, padding_mask_dict, device):\n",
    "        temporal_result_dict = {}\n",
    "        img_result_dict = {}\n",
    "        self_attn_dict, cross_attn_dict = {}, {}\n",
    "        img_result = None\n",
    "        target_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"] + self.data_info.modality_info[\"img\"]\n",
    "\n",
    "        for col in target_cols:\n",
    "            if col in self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]:\n",
    "                tgt = temporal[col]\n",
    "                temporal_memory = {key:val for key, val in temporal.items() if col != key}\n",
    "                img_memory = img\n",
    "                memory = torch.stack(list(temporal_memory.values()), dim=-2)\n",
    "                memory = torch.cat([memory, img.unsqueeze(1).repeat(1, memory.shape[1], 1, 1)], dim=-2)\n",
    "\n",
    "                for mod in self.custom_decoder[col]:\n",
    "                    tgt, self_attn_weight, cross_attn_weight = mod(col, tgt, memory, padding_mask_dict, device)\n",
    "                \n",
    "                temporal_result_dict[col] = tgt\n",
    "                self_attn_dict[col] = self_attn_weight\n",
    "                cross_attn_dict[col] = cross_attn_weight\n",
    "\n",
    "            elif col in self.data_info.modality_info[\"img\"]:\n",
    "                tgt = img\n",
    "                memory = torch.cat(list(temporal.values()), dim=-2)\n",
    "                padding_mask = self._get_temporal_padding_mask(padding_mask_dict, temporal, device, mode=\"revert\")\n",
    "                decoding = self.img_decoder(tgt=img, memory=memory, memory_key_padding_mask=padding_mask)\n",
    "                img_result = decoding\n",
    "\n",
    "        return temporal_result_dict, img_result, self_attn_dict, cross_attn_dict\n",
    "\n",
    "\n",
    "    def _____apply_custom_decoder(self, reverted_dict, padding_mask_dict, device):\n",
    "        result_dict = {}\n",
    "        self_attn_dict, cross_attn_dict = {}, {}\n",
    "        for col in reverted_dict.keys():\n",
    "\n",
    "            tgt = reverted_dict[col]\n",
    "            memory = {key:val for key, val in reverted_dict.items() if col != key}\n",
    "            # print(f\"{col} - {memory.keys()}\")\n",
    "            for mod in self.custom_decoder[col]:\n",
    "                tgt, self_attn_weight, cross_attn_weight = mod(col, tgt, memory, padding_mask_dict, device)\n",
    "            result_dict[col] = tgt\n",
    "            self_attn_dict[col] = self_attn_weight\n",
    "            cross_attn_dict[col] = cross_attn_weight\n",
    "\n",
    "        return result_dict, self_attn_dict, cross_attn_dict\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "   Parent Layers            Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "============================================================================================\n",
      "     Transformer          ImgEmbedding-1        [2, 197, 64]      86,438,464      86,438,464\n",
      "     Transformer             Embedding-2            [91, 64]             448             448\n",
      "     Transformer    TransformerEncoder-3        [2, 595, 64]          50,432          50,432\n",
      "     Transformer                Linear-4        [2, 595, 32]           2,080           2,080\n",
      "     Transformer             Embedding-5           [365, 32]             224             224\n",
      "     Transformer    TransformerDecoder-6        [2, 196, 32]          21,504          21,504\n",
      "     Transformer                Linear-7        [2, 196, 32]           1,056           1,056\n",
      "     Transformer                Linear-8        [2, 196, 32]           1,056           1,056\n",
      "     Transformer                Linear-9       [2, 196, 768]          25,344          25,344\n",
      "============================================================================================\n",
      "Total params: 86,540,608\n",
      "Trainable params: 86,540,608\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------------------------------------------------------\\n   Parent Layers            Layer (type)        Output Shape         Param #     Tr. Param #\\n============================================================================================\\n     Transformer          ImgEmbedding-1        [2, 197, 64]      86,438,464      86,438,464\\n     Transformer             Embedding-2            [91, 64]             448             448\\n     Transformer    TransformerEncoder-3        [2, 595, 64]          50,432          50,432\\n     Transformer                Linear-4        [2, 595, 32]           2,080           2,080\\n     Transformer             Embedding-5           [365, 32]             224             224\\n     Transformer    TransformerDecoder-6        [2, 196, 32]          21,504          21,504\\n     Transformer                Linear-7        [2, 196, 32]           1,056           1,056\\n     Transformer                Linear-8        [2, 196, 32]           1,056           1,056\\n     Transformer                Linear-9       [2, 196, 768]          25,344          25,344\\n============================================================================================\\nTotal params: 86,540,608\\nTrainable params: 86,540,608\\nNon-trainable params: 0\\n--------------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout,\n",
    "                        is_identical)\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def temporal_loss(col, pred, y, data, device):\n",
    "    pred = pred.to(device).squeeze()\n",
    "    y = y.to(device).squeeze()\n",
    "\n",
    "    masked_idx = data[f\"{col}_masked_idx\"].squeeze().to(device)\n",
    "    padding_mask = data[f\"{col}_masked_padding_mask\"].squeeze().to(device)\n",
    "\n",
    "    masked_y = torch.gather(y, index=masked_idx, dim=1)\n",
    "\n",
    "    if col in data_info.processing_info[\"embedding_cols\"]:\n",
    "        masked_idx = masked_idx.unsqueeze(-1).repeat(1, 1, pred.shape[-1])\n",
    "        masked_pred = torch.gather(pred, index=masked_idx, dim=1)\n",
    "        \n",
    "        masked_pred = masked_pred.reshape(-1, masked_pred.shape[-1])\n",
    "        masked_y = masked_y.reshape(-1)\n",
    "        \n",
    "        loss = ce_loss(masked_pred, masked_y.to(torch.long)).reshape(padding_mask.shape)\n",
    "    \n",
    "    elif col in data_info.processing_info[\"scaling_cols\"]:\n",
    "        masked_pred = torch.gather(pred, index=masked_idx, dim=1)\n",
    "        loss = mse_loss(masked_pred, masked_y)\n",
    "    \n",
    "    loss = loss * padding_mask\n",
    "    loss = sum(loss.view(-1)) / sum(padding_mask.view(-1))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def plot_epoch_loss():\n",
    "    pass\n",
    "\n",
    "def plot_sample_loss(model, data, remain_rto, device, train_mean_loss_dict, eval_mean_loss_dict):\n",
    "    model.eval()\n",
    "    # Get eval prediction\n",
    "    with torch.no_grad():\n",
    "        temporal_sample_output, img_output, self_attn_dict, cross_attn_dict, idx_dict = model(data, remain_rto, device)\n",
    "        \n",
    "        # Temporal train loss\n",
    "        loss_dict = defaultdict(list)\n",
    "        for col, val in temporal_sample_output.items():\n",
    "            y = data[col]\n",
    "\n",
    "            if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "                scaler = data[f\"{col}_scaler\"]\n",
    "                \n",
    "                for n, (s, _, _) in enumerate(zip(scaler, y, val)):\n",
    "                    y[n] = torch.tensor(s.inverse_transform(y[n].clone().detach()))\n",
    "                    val[n] = torch.tensor(s.inverse_transform(val[n].clone().detach().cpu()))\n",
    "\n",
    "            loss = temporal_loss(col, val, y, data, device)\n",
    "            loss_dict[col].append(loss)\n",
    "        \n",
    "        latest_loss = {key:val[-1] for key, val in loss_dict.items()}\n",
    "        temporal_loss_val = torch.nansum(torch.stack(list(latest_loss.values())))\n",
    "\n",
    "        # Img loss\n",
    "        y = patchify(data[\"img_input\"].squeeze()).to(device)\n",
    "        masked_idx = idx_dict[f\"img_masked_idx\"].squeeze().to(device).unsqueeze(-1).repeat(1, 1, y.shape[-1])\n",
    "        masked_y = torch.gather(y, index=masked_idx, dim=1)\n",
    "\n",
    "        masked_pred = torch.gather(img_output, index=masked_idx, dim=1)\n",
    "        img_loss = mse_loss(masked_pred, masked_y)\n",
    "\n",
    "        img_loss = torch.mean(img_loss)\n",
    "\n",
    "        # Dictionary for plot\n",
    "        eval_mean_loss_dict[\"total_loss\"].append(loss.item())\n",
    "        for key, val in loss_dict.items():\n",
    "            val = np.array([i.item() for i in val])\n",
    "            eval_mean_loss_dict[key].append(val.mean())\n",
    "\n",
    "    # Plot\n",
    "    idx = 0\n",
    "    plt.figure(figsize=(25,13))\n",
    "    nrows, ncols = len(data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"])+1, 5\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(nrows, ncols, 1)\n",
    "    plt.plot(train_mean_loss_dict[\"total_loss\"])\n",
    "    plt.title(f'Train total loss: {train_mean_loss_dict[\"total_loss\"][-1]}')\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(nrows, ncols, 2)\n",
    "    plt.plot(eval_mean_loss_dict[\"total_loss\"])\n",
    "    plt.title(f'Eval total loss: {eval_mean_loss_dict[\"total_loss\"][-1]}')\n",
    "\n",
    "    plot_idx = 6\n",
    "    for key, val in temporal_sample_output.items():\n",
    "        # Plot loss\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(train_mean_loss_dict[key])\n",
    "        plt.title(f\"{key} train loss: {train_mean_loss_dict[key][-1]}\")\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot loss\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(eval_mean_loss_dict[key])\n",
    "        plt.title(f\"{key} eval loss: {eval_mean_loss_dict[key][-1]}\")\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot sample\n",
    "        masked_idx = data[f\"{key}_masked_idx\"][idx]\n",
    "        y = data[key][idx].squeeze()\n",
    "        sample = val[idx].squeeze().detach().cpu()\n",
    "        if key in data_info.processing_info[\"embedding_cols\"]:\n",
    "            sample = torch.argmax(sample, dim=-1).to(torch.float)\n",
    "        \n",
    "        sample_full = torch.zeros(y.shape) + torch.nan\n",
    "        sample = torch.gather(sample, index=masked_idx, dim=-1)\n",
    "        sample_full[masked_idx] = sample\n",
    "\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(y)\n",
    "        plt.plot(sample_full)\n",
    "        plt.scatter(torch.arange(sample_full.shape[0]), sample_full, color=\"red\", s=15)\n",
    "        plt.title(f\"{key} sample\")\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot self attention\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        # print(self_attn_dict.shape)\n",
    "        # raise\n",
    "        sns.heatmap(self_attn_dict[key][idx].detach().cpu())\n",
    "        plt.title(f\"{key} self attention weight\")\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Plot cross attention\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        cross_attn = cross_attn_dict[key][idx].squeeze().detach().cpu() \n",
    "        cross_attn = cross_attn.mean(dim=0)\n",
    "        sns.heatmap(cross_attn)\n",
    "        plt.title(f\"{key} cross attention weight\")\n",
    "        plot_idx += 1\n",
    "\n",
    "    return eval_mean_loss_dict\n",
    "        \n",
    "    \n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    patch_size = 16\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 50\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[33], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     42\u001b[0m         eval_mean_loss_dict \u001b[38;5;241m=\u001b[39m plot_sample_loss(model, data, remain_rto, device, train_mean_loss_dict, eval_mean_loss_dict)\n\u001b[1;32m     43\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m---> 44\u001b[0m         \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[1;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[1;32m   2340\u001b[0m     )\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1388\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1386\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[0;32m-> 1388\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1276\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1274\u001b[0m major_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_majorticklocs()\n\u001b[1;32m   1275\u001b[0m major_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_ticks(major_locs)\n\u001b[0;32m-> 1276\u001b[0m major_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_major_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmajor_locs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mset_locs(major_locs)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1627\u001b[0m, in \u001b[0;36mAxis.get_major_ticks\u001b[0;34m(self, numticks)\u001b[0m\n\u001b[1;32m   1623\u001b[0m     numticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_majorticklocs())\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks) \u001b[38;5;241m<\u001b[39m numticks:\n\u001b[1;32m   1626\u001b[0m     \u001b[38;5;66;03m# Update the new tick label properties from the old.\u001b[39;00m\n\u001b[0;32m-> 1627\u001b[0m     tick \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_tick_props(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks[\u001b[38;5;241m0\u001b[39m], tick)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1563\u001b[0m, in \u001b[0;36mAxis._get_tick\u001b[0;34m(self, major)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Axis subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must define \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tick_class or reimplement _get_tick()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1562\u001b[0m tick_kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\n\u001b[0;32m-> 1563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tick_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmajor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtick_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:417\u001b[0m, in \u001b[0;36mXTick.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# x in data coords, y in axes coords\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:156\u001b[0m, in \u001b[0;36mTick.__init__\u001b[0;34m(self, axes, loc, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m     grid_alpha \u001b[38;5;241m=\u001b[39m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid.alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    154\u001b[0m grid_kw \u001b[38;5;241m=\u001b[39m {k[\u001b[38;5;241m5\u001b[39m:]: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line \u001b[38;5;241m=\u001b[39m \u001b[43mmlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLine2D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzorder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtick1On\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmarkeredgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkersize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmarkeredgewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line \u001b[38;5;241m=\u001b[39m mlines\u001b[38;5;241m.\u001b[39mLine2D(\n\u001b[1;32m    162\u001b[0m     [], [],\n\u001b[1;32m    163\u001b[0m     color\u001b[38;5;241m=\u001b[39mcolor, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, zorder\u001b[38;5;241m=\u001b[39mzorder, visible\u001b[38;5;241m=\u001b[39mtick2On,\n\u001b[1;32m    164\u001b[0m     markeredgecolor\u001b[38;5;241m=\u001b[39mcolor, markersize\u001b[38;5;241m=\u001b[39msize, markeredgewidth\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[1;32m    165\u001b[0m )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline \u001b[38;5;241m=\u001b[39m mlines\u001b[38;5;241m.\u001b[39mLine2D(\n\u001b[1;32m    167\u001b[0m     [], [],\n\u001b[1;32m    168\u001b[0m     color\u001b[38;5;241m=\u001b[39mgrid_color, alpha\u001b[38;5;241m=\u001b[39mgrid_alpha, visible\u001b[38;5;241m=\u001b[39mgridOn,\n\u001b[1;32m    169\u001b[0m     linestyle\u001b[38;5;241m=\u001b[39mgrid_linestyle, linewidth\u001b[38;5;241m=\u001b[39mgrid_linewidth, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrid_kw,\n\u001b[1;32m    171\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/lines.py:388\u001b[0m, in \u001b[0;36mLine2D.__init__\u001b[0;34m(self, xdata, ydata, linewidth, linestyle, color, gapcolor, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markerfacecolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markerfacecoloralt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_markerfacecolor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkerfacecolor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Normalizes None to rc.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_markerfacecoloralt(markerfacecoloralt)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_markeredgecolor(markeredgecolor)  \u001b[38;5;66;03m# Normalizes None to rc.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/lines.py:1227\u001b[0m, in \u001b[0;36mLine2D.set_markerfacecolor\u001b[0;34m(self, fc)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_markerfacecolor\u001b[39m(\u001b[38;5;28mself\u001b[39m, fc):\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Set the marker face color.\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;124;03m    fc : color\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_markercolor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmarkerfacecolor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/lines.py:1207\u001b[0m, in \u001b[0;36mLine2D._set_markercolor\u001b[0;34m(self, name, has_rcdefault, val)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m neq\u001b[38;5;241m.\u001b[39many() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(neq, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m neq:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(e):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_dict, train_mean_loss_dict, eval_mean_loss_dict = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        temporal_output, img_output, self_attn_dict, cross_attn_dict, idx_dict = model(data, remain_rto, device)\n",
    "\n",
    "        # Temporal train loss\n",
    "        for col, val in temporal_output.items():\n",
    "            y = data[col]\n",
    "            loss = temporal_loss(col, val, y, data, device)\n",
    "            loss_dict[col].append(loss)\n",
    "        \n",
    "        latest_loss = {key:val[-1] for key, val in loss_dict.items()}\n",
    "        temporal_loss_val = torch.nansum(torch.stack(list(latest_loss.values())))\n",
    "\n",
    "        # Img tarin loss\n",
    "        y = patchify(data[\"img_input\"].squeeze()).to(device)\n",
    "        masked_idx = idx_dict[f\"img_masked_idx\"].squeeze().to(device).unsqueeze(-1).repeat(1, 1, y.shape[-1])\n",
    "        masked_y = torch.gather(y, index=masked_idx, dim=1)\n",
    "\n",
    "        masked_pred = torch.gather(img_output, index=masked_idx, dim=1)\n",
    "        img_loss = mse_loss(masked_pred, masked_y)\n",
    "\n",
    "        img_loss = torch.mean(img_loss)\n",
    "\n",
    "        loss = img_loss + temporal_loss_val\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Dictionary for plot\n",
    "        train_mean_loss_dict[\"total_loss\"].append(loss.item())\n",
    "        for key, val in loss_dict.items():\n",
    "            val = np.array([i.item() for i in val])\n",
    "            train_mean_loss_dict[key].append(val.mean())\n",
    "\n",
    "        if n % 20 == 0:\n",
    "            plot_epoch_loss()\n",
    "            eval_mean_loss_dict = plot_sample_loss(model, data, remain_rto, device, train_mean_loss_dict, eval_mean_loss_dict)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return \n",
    "\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
