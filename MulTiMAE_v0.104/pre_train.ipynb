{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Torch-related\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Custom defined\n",
    "from config import pre_train, fine_tuning\n",
    "from libs.data import load_dataset, collate_fn\n",
    "from architecture.architecture import *\n",
    "from architecture.shared_module import patchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([16, 300, 1])\n",
      "day torch.Size([16, 300])\n",
      "dow torch.Size([16, 300])\n",
      "month torch.Size([16, 300])\n",
      "holiday torch.Size([16, 300])\n",
      "price torch.Size([16, 300, 1])\n",
      "temporal_padding_mask torch.Size([16, 300])\n",
      "target_fcst_mask torch.Size([16, 300])\n",
      "img_path torch.Size([16, 3, 224, 224])\n",
      "detail_desc torch.Size([16, 81])\n",
      "detail_desc_remain_padding_mask torch.Size([16, 21])\n",
      "detail_desc_masked_padding_mask torch.Size([16, 60])\n",
      "detail_desc_revert_padding_mask torch.Size([16, 81])\n",
      "detail_desc_remain_idx torch.Size([16, 21])\n",
      "detail_desc_masked_idx torch.Size([16, 60])\n",
      "detail_desc_revert_idx torch.Size([16, 81])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "is_test_mode = False\n",
    "is_new_rawdata = False\n",
    "is_new_dataset = False\n",
    "config = pre_train\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "if is_new_dataset:\n",
    "    train_dataset = load_dataset(is_test_mode, is_new_rawdata, config, mode=\"pre_train\", verbose=True)\n",
    "    # train_dataset = load_dataset(is_test_mode, is_new_rawdata, config, mode=\"fine_tuning\", verbose=True)\n",
    "else:\n",
    "    suffix = \"_test\" if is_test_mode else \"\"\n",
    "    train_dataset = torch.load(f\"src/pre_train_dataset{suffix}\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, config), pin_memory=True, num_workers=16, prefetch_factor=32)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, config))\n",
    "for _ in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in _.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "   MaskedBlockAutoencoder      MBAEEncoder-1                          19,232,640      19,157,376\n",
      "   MaskedBlockAutoencoder      MBAEDecoder-2                          11,816,895      11,779,263\n",
      "================================================================================================\n",
      "Total params: 31,049,535\n",
      "Trainable params: 30,936,639\n",
      "Non-trainable params: 112,896\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder_dict = joblib.load(\"src/label_encoder_dict.pkl\")\n",
    "model = MaskedBlockAutoencoder(config, label_encoder_dict)\n",
    "model.to(device)\n",
    "summary(model, _, device, show_parent_layers=True, print_summary=True)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def get_loss(pred_dict, y_dict, idx_dict, mask_dict):\n",
    "    loss_dict = {}\n",
    "    loss_sum, cnt = 0, 0\n",
    "\n",
    "    for n, (key, pred) in enumerate(pred_dict.items()):\n",
    "        y = y_dict[key].to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        ### Temporal\n",
    "        if key in config.temporal_cols:\n",
    "            if key in config.scaling_cols:\n",
    "                loss = mse_loss(pred, y).squeeze()\n",
    "            elif key in config.embedding_cols:\n",
    "                loss = ce_loss(pred.view(-1, pred.shape[-1]), y.view(-1).to(torch.long))\n",
    "                loss = loss.view(y.shape)\n",
    "        ### Img loss\n",
    "        if key in config.img_cols:\n",
    "            y = patchify(y, config.patch_size)\n",
    "            loss = mse_loss(pred, y)\n",
    "        ### Nlp loss\n",
    "        if key in config.nlp_cols:\n",
    "            loss = ce_loss(pred.reshape(-1, pred.shape[-1]), y.reshape(-1).to(torch.long))\n",
    "            loss = loss.view(y.shape)\n",
    "\n",
    "        # Masking loss\n",
    "        ### Temporal\n",
    "        if key in config.temporal_cols:\n",
    "            masked_idx = idx_dict[\"temporal_masked_idx\"]\n",
    "            masking_mask = (masked_idx==n).sum(dim=-1)\n",
    "            padding_mask = mask_dict[\"temporal_padding_mask\"]\n",
    "\n",
    "            total_mask = torch.where((padding_mask==1)&(masking_mask==1), 1, 0)\n",
    "            loss *= total_mask\n",
    "            loss_sum += loss.sum(); cnt += total_mask.sum()\n",
    "            loss = loss.sum()/total_mask.sum()\n",
    "        ### Img\n",
    "        elif key in config.img_cols:\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            \n",
    "            loss = torch.gather(loss, index=masked_idx.unsqueeze(-1).expand(-1, -1, loss.shape[-1]), dim=1)\n",
    "            loss_sum += loss.sum(); cnt += loss.shape[0] * loss.shape[1] * loss.shape[2]\n",
    "            loss = loss.mean()\n",
    "        ### Nlp\n",
    "        elif key in config.nlp_cols:\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            padding_mask = mask_dict[f\"{key}_masked_padding_mask\"]\n",
    "\n",
    "            loss = torch.gather(loss, index=masked_idx, dim=1)\n",
    "            loss *= padding_mask\n",
    "            loss_sum += loss.sum(); cnt += total_mask.sum()\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "        \n",
    "        loss_dict[key] = loss\n",
    "    \n",
    "    total_loss = loss_sum / cnt\n",
    "    return loss_dict, total_loss            \n",
    "\n",
    "def obtain_loss_dict_for_plot(total_loss, loss_dict, loss_li_dict, mean_loss_li_dict):\n",
    "    loss_li_dict[\"total\"].append(total_loss.item())\n",
    "    mean_loss_li_dict[\"total\"].append(np.array(loss_li_dict[\"total\"]).mean())\n",
    "\n",
    "    for key, val in loss_dict.items():\n",
    "        loss_li_dict[key].append(val.item())\n",
    "        mean_loss_li_dict[key].append(np.array(loss_li_dict[key]).mean())\n",
    "\n",
    "    return loss_li_dict, mean_loss_li_dict\n",
    "\n",
    "def plot_sample(nrows, ncols, config, mean_loss_li_dict, decoding_dict, decoding_attn_weight_dict, data_dict):\n",
    "    idx, plot_idx = 0, 1\n",
    "    for key, val in mean_loss_li_dict.items():\n",
    "        # Individual loss\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(val)\n",
    "        plt.title(f\"{key}: {val[-1]}\")\n",
    "        plot_idx += 1; \n",
    "        if key==\"total\": \n",
    "            plot_idx += 3; continue\n",
    "        \n",
    "        pred, y = decoding_dict[key].detach().cpu().squeeze(), data_dict[key].squeeze()\n",
    "\n",
    "        # Temporal sample\n",
    "        if key in config.temporal_cols:\n",
    "            ### Sample\n",
    "            if key in config.embedding_cols: pred = torch.argmax(pred, dim=-1)\n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.plot(y[idx]); plt.plot(pred[idx])\n",
    "            ### Img weight\n",
    "            weight = decoding_attn_weight_dict[\"temporal\"][\"img_path_revert\"][idx].mean(dim=0).detach().cpu()\n",
    "            plt.subplot(nrows, ncols, plot_idx+1)\n",
    "            plt.imshow(weight.reshape(224//config.patch_size, 224//config.patch_size))\n",
    "            ### Nlp weight\n",
    "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "            weight = decoding_attn_weight_dict[\"temporal\"][\"detail_desc_revert\"][idx].mean(dim=0).detach().cpu()\n",
    "            xlabel = tokenizer.tokenize(tokenizer.decode(data_dict[\"detail_desc\"][idx]))\n",
    "\n",
    "            df = pd.DataFrame({\"xlabel\":xlabel, \"weight\":weight})\n",
    "            df = df[df[\"xlabel\"]!=\"[PAD]\"]\n",
    "            plt.subplot(nrows, ncols, plot_idx+2)\n",
    "            sns.barplot(df[\"weight\"])\n",
    "            plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
    "    \n",
    "        # Img sample\n",
    "        elif key in config.img_cols:\n",
    "            pred = unpatchify(pred).permute(0,2,3,1)\n",
    "            y = y.permute(0,2,3,1)\n",
    "            \n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.imshow(data_dict[\"img_path_raw\"][idx].permute(1,2,0))\n",
    "\n",
    "            plt.subplot(nrows, ncols, plot_idx+1)\n",
    "            plt.imshow(y[idx])\n",
    "\n",
    "            plt.subplot(nrows, ncols, plot_idx+2)\n",
    "            plt.imshow(pred[idx])\n",
    "\n",
    "        # Nlp sample\n",
    "        elif key in config.nlp_cols:\n",
    "            pred = tokenizer.decode(torch.argmax(pred, dim=-1)[idx])\n",
    "            y = tokenizer.decode(y[idx])\n",
    "        \n",
    "        plot_idx += 3\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, config, e):\n",
    "    pbar = tqdm(dataloader)\n",
    "    loss_li_dict, mean_loss_li_dict = defaultdict(list), defaultdict(list)\n",
    "    model.train()\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        decoding_dict, decoding_attn_weight_dict, idx_dict, mask_dict = model(data, device)\n",
    "        loss_dict, loss = get_loss(decoding_dict, data, idx_dict, mask_dict)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot\n",
    "        if n % 20 == 0:\n",
    "            nrows, ncols = 12, 4\n",
    "            plt.figure(figsize=(25,25))\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            loss_li_dict, mean_loss_li_dict = obtain_loss_dict_for_plot(loss, loss_dict, loss_li_dict, mean_loss_li_dict)\n",
    "            plot_sample(nrows, ncols, config, mean_loss_li_dict, decoding_dict, decoding_attn_weight_dict, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "/tmp/ipykernel_13890/1857696958.py:114: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  plt.gca().set_xticklabels(df[\"xlabel\"], rotation=90)\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "epoch = 10\n",
    " \n",
    "epoch_loss = {}\n",
    "for e in range(epoch):\n",
    "    loss = train_epoch(model, optimizer, train_dataloader, config, e)\n",
    "    scheduler.step()\n",
    "    epoch_loss[e] = loss\n",
    "\n",
    "    # Save model\n",
    "    if not is_test_mode:\n",
    "        now = datetime.datetime.now()\n",
    "        path = f\"./saved_model_epoch{e}_{now}\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "print(epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
