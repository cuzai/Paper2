{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "is_skip = True\n",
    "\n",
    "# Data params\n",
    "target_batch_size = 128\n",
    "batch_size = 64\n",
    "# batch_size = 4\n",
    "\n",
    "grad_accm_step_max = target_batch_size // batch_size\n",
    "print(grad_accm_step_max)\n",
    "\n",
    "# Model params\n",
    "d_model = 64\n",
    "nhead = 2\n",
    "d_ff = 128\n",
    "dropout = 0.1\n",
    "num_layers = 1\n",
    "remain_rto_general = 0.3\n",
    "remain_rto_cat = 0.7\n",
    "\n",
    "patch_size = 14  # 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "\n",
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from transformers import ViTImageProcessor, MobileViTModel, MobileViTConfig, ViTModel, AutoImageProcessor, ViTConfig, BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer, Dinov2Config, Dinov2Model\n",
    "from skimage import io\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_skip:\n",
    "    us_holiday = holidays.US()\n",
    "    \n",
    "    # Read transaction\n",
    "    df_trans = pd.read_csv(\"../HnM/transactions_train.csv\", parse_dates=[\"t_dat\"], dtype={\"article_id\":str})\n",
    "    df_meta = pd.read_csv(\"../HnM/articles.csv\", dtype={\"article_id\":str})\n",
    "\n",
    "    min_year = df_trans[\"t_dat\"].dt.year.min()\n",
    "    max_year = df_trans[\"t_dat\"].dt.year.max()\n",
    "\n",
    "    holiday = holidays.US(years=(min_year, max_year))\n",
    "    holiday = pd.DataFrame({\"t_dat\":holiday.keys(), \"holiday\":holiday.values()})\n",
    "    holiday[\"t_dat\"] = pd.to_datetime(holiday[\"t_dat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Image path\n",
    "    data[\"img_path\"] = data[\"article_id\"].apply(lambda x: f'../HnM/resized_images/{x[:3]}/{x}.jpg')\n",
    "    data[\"is_valid\"] = data[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0) # Check whether the article has corresponding image file\n",
    "    data = data[data[\"is_valid\"] == 1].drop(\"is_valid\", axis=1) # Valid if having corresponding image\n",
    "\n",
    "    # Make sales\n",
    "    data = data.groupby([\"t_dat\", \"article_id\", \"img_path\"], as_index=False).agg(sales=(\"customer_id\", \"count\"), price=(\"price\", \"mean\"))\n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data = data[(data[\"size\"]>=100)]\n",
    "\n",
    "    # Expand dates\n",
    "    data = data.set_index(\"t_dat\").groupby([\"article_id\"], as_index=False).resample(\"1D\").asfreq().reset_index()\n",
    "    data[\"sales\"] = data[\"sales\"].fillna(0)\n",
    "    data[\"price\"] = data[\"price\"].fillna(method=\"ffill\")\n",
    "    data[\"article_id\"] = data[\"article_id\"].fillna(method=\"ffill\")\n",
    "    data[\"img_path\"] = data[\"img_path\"].fillna(method=\"ffill\")\n",
    "    data = data.sort_values([\"article_id\", \"t_dat\"])\n",
    "    \n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data[\"time_idx\"] = data.groupby(\"article_id\").cumcount()\n",
    "    data[\"time_idx\"] = data[\"size\"] - data[\"time_idx\"]\n",
    "\n",
    "    # Make holidays\n",
    "    data = pd.merge(data, holiday, on=\"t_dat\", how=\"left\")\n",
    "    display(data)\n",
    "\n",
    "    # Temporal information\n",
    "    # Make sure the sequence start from 0\n",
    "    data[\"day\"] = data[\"t_dat\"].dt.day - 1\n",
    "    data[\"dow\"] = data[\"t_dat\"].dt.dayofweek\n",
    "    data[\"month\"] = data[\"t_dat\"].dt.month - 1\n",
    "    data[\"year\"] = data[\"t_dat\"].dt.year / (data[\"t_dat\"].dt.year.max() + 1)\n",
    "\n",
    "    # Append meta data\n",
    "    # data = data.merge(df_meta[[\"index_name\", \"article_id\", \"colour_group_name\", \"graphical_appearance_name\", \"product_type_name\"]], on=\"article_id\")\n",
    "    data = data.merge(df_meta[[\"index_name\", \"article_id\", \"colour_group_name\", \"graphical_appearance_name\", \"prod_name\"]], on=\"article_id\")\n",
    "    data = data.rename(columns={\"index_name\":\"index\", \"colour_group_name\":\"color\", \"graphical_appearance_name\":\"graphic\", \"prod_name\":\"prod\"})\n",
    "\n",
    "    # Output\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "if not is_skip:\n",
    "    df_prep = df_trans.copy()#.iloc[:1000]\n",
    "    data = preprocess(df_prep)\n",
    "    \n",
    "    data.to_parquet(\"data.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        self.a = 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return np.log1p(x)\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        return np.expm1(x)\n",
    "    \n",
    "class NoneScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x\n",
    "\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        x = set(x)\n",
    "        for val in x:\n",
    "            if val not in self.mapper.keys():\n",
    "                self.mapper[val] = self.idx\n",
    "                self.idx += 1\n",
    "\n",
    "        self.mapper[\"unseen\"] = self.idx\n",
    "        self.idx += 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        res = []\n",
    "        for val in x:\n",
    "            if val in self.mapper.keys():\n",
    "                res.append(self.mapper[val])\n",
    "            else:\n",
    "                res.append(self.idx)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def inverse_transform(self, idx):\n",
    "        inverse_mapper = {val:key for key, val in self.mapper.items()}\n",
    "        return inverse_mapper[idx]\n",
    "\n",
    "# 1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info = {\n",
    "    \"group\": [\"article_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    # \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"]\n",
    "    \"temporal\": [\"price\", \"dow\"]\n",
    "}\n",
    "\n",
    "num_scaler = {\"sales\": LogScaler, \"price\": NoneScaler}\n",
    "# embedding_cols = [\"day\", \"dow\", \"month\", \"holiday\"]\n",
    "embedding_cols = [\"dow\"]\n",
    "img_col = []\n",
    "\n",
    "num_modality = len([v for key, val in data_info.items() for v in val if key not in [\"group\"]])\n",
    "num_modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting label encoder for dow: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 26842/26842 [00:01<00:00, 26007.98it/s]\n",
      "100%|██████████| 26842/26842 [00:01<00:00, 26497.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales: torch.Size([64, 365, 1])\n",
      "sales_remain_idx: torch.Size([64, 109])\n",
      "sales_revert_idx: torch.Size([64, 365])\n",
      "sales_remain_padding_mask: torch.Size([64, 109])\n",
      "sales_revert_padding_mask: torch.Size([64, 365])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"data.pq\")\n",
    "df_train = data[data[\"time_idx\"]<=365]\n",
    "df_valid = data[data[\"time_idx\"]<=365+90]\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, data_info, num_scaler, embedding_cols, is_train=True, label_encoder_dict=None):\n",
    "        # Define parameters\n",
    "        self.data_info, self.num_scaler, self.embedding_cols, self.is_train = data_info, num_scaler, embedding_cols, is_train\n",
    "        assert label_encoder_dict is None if is_train else not None # In test mode, label_encoder_dict should be provided\n",
    "\n",
    "        # Fit label encoder\n",
    "        if is_train:\n",
    "            label_encoder_dict = self.encode_label(data, embedding_cols)\n",
    "        self.label_encoder_dict = label_encoder_dict\n",
    "\n",
    "        # Iterate each product\n",
    "        self.data_li = []\n",
    "        data.groupby(data_info[\"group\"]).progress_apply(lambda x: self.data_li.append(x))\n",
    "\n",
    "    def encode_label(self, data, embedding_cols):\n",
    "        label_encoder_dict = {}\n",
    "        pbar = tqdm(embedding_cols)\n",
    "        for col in pbar:\n",
    "            encoder = CustomLabelEncoder()\n",
    "            encoder.fit(data[col])\n",
    "            label_encoder_dict[col] = encoder\n",
    "            pbar.set_description(f\"Fitting label encoder for {col}\")\n",
    "        \n",
    "        return label_encoder_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_li)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_li[idx]\n",
    "        data_input = {}\n",
    "\n",
    "        # Transform with label encoder\n",
    "        for col in self.embedding_cols:\n",
    "            temp_res = self.label_encoder_dict[col].transform(data[col])\n",
    "            data_input[col] = torch.IntTensor(temp_res)\n",
    "        \n",
    "        # Scale target data\n",
    "        for col, scaler in self.num_scaler.items():\n",
    "            scaler = scaler()\n",
    "            data_input[col] = torch.Tensor(scaler.fit_transform(data[col].values.reshape(-1,1)))\n",
    "            data_input[f\"{col}_scaler\"] = scaler\n",
    "\n",
    "        # Mask temporal data\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            remain_idx, revert_idx, masked_idx, remain_padding_mask, revert_padding_mask, masked_padding_mask = self.get_mask(data_input[col])\n",
    "\n",
    "            data_input[f\"{col}_remain_idx\"] = remain_idx\n",
    "            data_input[f\"{col}_revert_idx\"] = revert_idx\n",
    "            data_input[f\"{col}_masked_idx\"] = masked_idx\n",
    "            data_input[f\"{col}_remain_padding_mask\"] = remain_padding_mask\n",
    "            data_input[f\"{col}_revert_padding_mask\"] = revert_padding_mask\n",
    "            data_input[f\"{col}_masked_padding_mask\"] = masked_padding_mask\n",
    "\n",
    "        return data_input\n",
    "\n",
    "    def get_mask(self, data):\n",
    "        num_remain = int(data.shape[0] * remain_rto_general) if self.is_train else -90\n",
    "\n",
    "        # Index for shuffle and revert\n",
    "        if self.is_train:\n",
    "            noise = torch.rand(data.shape[0])\n",
    "            shuffle_idx = torch.argsort(noise, dim=0)\n",
    "            revert_idx = torch.argsort(shuffle_idx, dim=0)\n",
    "        else:\n",
    "            shuffle_idx = torch.arange(data.shape[0])\n",
    "            revert_idx = torch.argsort(shuffle_idx)\n",
    "\n",
    "        remain_idx = shuffle_idx[:num_remain]\n",
    "        masked_idx = shuffle_idx[num_remain:]\n",
    "\n",
    "        remain_padding_mask = torch.ones(remain_idx.shape)\n",
    "        revert_padding_mask = torch.ones(revert_idx.shape)\n",
    "        masked_padding_mask = torch.ones(masked_idx.shape)\n",
    "\n",
    "        return remain_idx, revert_idx, masked_idx, remain_padding_mask, revert_padding_mask, masked_padding_mask\n",
    "\n",
    "def collate_fn(batch_li):\n",
    "    return_dict = {}\n",
    "\n",
    "    # Process temporal data - Apply padding\n",
    "    for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "        data = [batch[col] for batch in batch_li]\n",
    "        data_remain_idx = [batch[f\"{col}_remain_idx\"] for batch in batch_li]\n",
    "        data_revert_idx = [batch[f\"{col}_revert_idx\"] for batch in batch_li]\n",
    "        data_masked_idx = [batch[f\"{col}_masked_idx\"] for batch in batch_li]\n",
    "        data_remain_padding_mask = [batch[f\"{col}_remain_padding_mask\"] for batch in batch_li]\n",
    "        data_revert_padding_mask = [batch[f\"{col}_revert_padding_mask\"] for batch in batch_li]\n",
    "        data_masked_padding_mask = [batch[f\"{col}_masked_padding_mask\"] for batch in batch_li]\n",
    "\n",
    "        # For ordinary data\n",
    "        return_dict[col] = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "        # For remain and revert index\n",
    "        padding_value = return_dict[col].shape[-1] - 1 # The last index of the data\n",
    "        return_dict[f\"{col}_remain_idx\"] = torch.nn.utils.rnn.pad_sequence(data_remain_idx, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_revert_idx\"] = torch.nn.utils.rnn.pad_sequence(data_revert_idx, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_masked_idx\"] = torch.nn.utils.rnn.pad_sequence(data_masked_idx, batch_first=True, padding_value=padding_value)\n",
    "        # For padding masks\n",
    "        padding_value = 0\n",
    "        return_dict[f\"{col}_remain_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_remain_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_revert_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_revert_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_masked_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_masked_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "train_dataset = Dataset(df_train, data_info, num_scaler, embedding_cols)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=24)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = Dataset(df_valid, data_info, num_scaler, embedding_cols, is_train=False, label_encoder_dict=train_dataset.label_encoder_dict)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=16)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    print(\"sales:\", data[\"sales\"].shape)\n",
    "    print(\"sales_remain_idx:\", data[\"sales_remain_idx\"].shape)\n",
    "    print(\"sales_revert_idx:\", data[\"sales_revert_idx\"].shape)\n",
    "    print(\"sales_remain_padding_mask:\", data[\"sales_remain_padding_mask\"].shape)\n",
    "    print(\"sales_revert_padding_mask:\", data[\"sales_revert_padding_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for valid_data in valid_dataloader:\n",
    "#     print(\"sales:\", valid_data[\"sales\"].shape)\n",
    "#     print(\"sales_remain_idx:\", valid_data[\"sales_remain_idx\"].shape)\n",
    "#     print(\"sales_revert_idx:\", valid_data[\"sales_revert_idx\"].shape)\n",
    "#     print(\"sales_remain_padding_mask:\", valid_data[\"sales_remain_padding_mask\"].shape)\n",
    "#     print(\"sales_revert_padding_mask:\", valid_data[\"sales_revert_padding_mask\"].shape)\n",
    "#     print(\"img_input\", valid_data[\"img_input\"].shape)\n",
    "#     print(\"index:\", valid_data[\"index\"].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def _generate_square_subsequent_mask(sz, device, dtype):\n",
    "    return torch.triu(\n",
    "        torch.full((sz, sz), float('-inf'), dtype=dtype, device=device),\n",
    "        diagonal=1,\n",
    "    )\n",
    "\n",
    "def _get_seq_len(src, batch_first):\n",
    "    if src.is_nested:\n",
    "        return None\n",
    "    else:\n",
    "        src_size = src.size()\n",
    "        if len(src_size) == 2:\n",
    "            # unbatched: S, E\n",
    "            return src_size[0]\n",
    "        else:\n",
    "            # batched: B, S, E if batch_first else S, B, E\n",
    "            seq_len_pos = 1 if batch_first else 0\n",
    "            return src_size[seq_len_pos]\n",
    "\n",
    "def _detect_is_causal_mask(mask, is_causal=None,size=None):\n",
    "    # Prevent type refinement\n",
    "    make_causal = (is_causal is True)\n",
    "\n",
    "    if is_causal is None and mask is not None:\n",
    "        sz = size if size is not None else mask.size(-2)\n",
    "        causal_comparison = _generate_square_subsequent_mask(\n",
    "            sz, device=mask.device, dtype=mask.dtype)\n",
    "\n",
    "        # Do not use `torch.equal` so we handle batched masks by\n",
    "        # broadcasting the comparison.\n",
    "        if mask.size() == causal_comparison.size():\n",
    "            make_causal = bool((mask == causal_comparison).all())\n",
    "        else:\n",
    "            make_causal = False\n",
    "\n",
    "    return make_causal\n",
    "\n",
    "class EncoderLayer(torch.nn.TransformerEncoderLayer):\n",
    "    def forward(self, src, pos_enc, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
    "        x = src\n",
    "        attn_output, attn_weight = self._sa_block(x, pos_enc, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x, attn_weight\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x, pos_enc, attn_mask, key_padding_mask, is_causal=False):\n",
    "        x, attn_weight = self.self_attn(x+pos_enc, x+pos_enc, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=True, is_causal=is_causal, average_attn_weights=False)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "class Encoder(torch.nn.TransformerEncoder):\n",
    "    def forward(self, src, pos_enc=0, mask=None, src_key_padding_mask=None, is_causal=None):\n",
    "       ################################################################################################################\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(mask),\n",
    "            other_name=\"mask\",\n",
    "            target_type=src.dtype\n",
    "        )\n",
    "\n",
    "        mask = F._canonical_mask(\n",
    "            mask=mask,\n",
    "            mask_name=\"mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        src_key_padding_mask_for_layers = src_key_padding_mask\n",
    "        why_not_sparsity_fast_path = ''\n",
    "        str_first_layer = \"self.layers[0]\"\n",
    "        batch_first = first_layer.self_attn.batch_first\n",
    "        if not hasattr(self, \"use_nested_tensor\"):\n",
    "            why_not_sparsity_fast_path = \"use_nested_tensor attribute not present\"\n",
    "        elif not self.use_nested_tensor:\n",
    "            why_not_sparsity_fast_path = \"self.use_nested_tensor (set in init) was not True\"\n",
    "        elif first_layer.training:\n",
    "            why_not_sparsity_fast_path = f\"{str_first_layer} was in training mode\"\n",
    "        elif not src.dim() == 3:\n",
    "            why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "        elif src_key_padding_mask is None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask was None\"\n",
    "        elif (((not hasattr(self, \"mask_check\")) or self.mask_check)\n",
    "                and not torch._nested_tensor_from_mask_left_aligned(src, src_key_padding_mask.logical_not())):\n",
    "            why_not_sparsity_fast_path = \"mask_check enabled, and src and src_key_padding_mask was not left aligned\"\n",
    "        elif output.is_nested:\n",
    "            why_not_sparsity_fast_path = \"NestedTensor input is not supported\"\n",
    "        elif mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask and mask were both supplied\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_sparsity_fast_path:\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                first_layer.self_attn.in_proj_weight,\n",
    "                first_layer.self_attn.in_proj_bias,\n",
    "                first_layer.self_attn.out_proj.weight,\n",
    "                first_layer.self_attn.out_proj.bias,\n",
    "                first_layer.norm1.weight,\n",
    "                first_layer.norm1.bias,\n",
    "                first_layer.norm2.weight,\n",
    "                first_layer.norm2.bias,\n",
    "                first_layer.linear1.weight,\n",
    "                first_layer.linear1.bias,\n",
    "                first_layer.linear2.weight,\n",
    "                first_layer.linear2.bias,\n",
    "            )\n",
    "            _supported_device_type = [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif src.device.type not in _supported_device_type:\n",
    "                why_not_sparsity_fast_path = f\"src device is neither one of {_supported_device_type}\"\n",
    "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                              \"input/output projection weights or biases requires_grad\")\n",
    "\n",
    "            if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):\n",
    "                convert_to_nested = True\n",
    "                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
    "                src_key_padding_mask_for_layers = None\n",
    "\n",
    "        seq_len = _get_seq_len(src, batch_first)\n",
    "        is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n",
    "       ################################################################################################################\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, attn_weight = mod(output, pos_enc, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
    "\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0., src.size())\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len=1000):\n",
    "    position = torch.arange(seq_len).reshape(-1,1).to(device)\n",
    "    i = torch.arange(d_model).to(device)//2\n",
    "    exp_term = 2*i/d_model\n",
    "    div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "    pos_encoded = position / div_term\n",
    "\n",
    "    pos_encoded[:, 0::2] = torch.sin(pos_encoded[:, 0::2])\n",
    "    pos_encoded[:, 1::2] = torch.cos(pos_encoded[:, 1::2])\n",
    "\n",
    "    return pos_encoded\n",
    "\n",
    "def apply_mask(data, remain_rto):\n",
    "    num_remain = int(data.shape[1] * remain_rto)\n",
    "\n",
    "    # Index for shuffle and revert\n",
    "    noise = torch.rand(data.shape[:-1]).to(device)\n",
    "    shuffle_idx = torch.argsort(noise, dim=1)\n",
    "    revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "    remain_idx = shuffle_idx[:, :num_remain]\n",
    "    masked_idx = shuffle_idx[:, num_remain:]\n",
    "\n",
    "    # Apply mask\n",
    "    remain_idx_ = remain_idx.unsqueeze(-1).repeat(1, 1, data.shape[-1])\n",
    "    data = torch.gather(data, index=remain_idx_, dim=1)\n",
    "\n",
    "    return data, remain_idx, revert_idx, masked_idx\n",
    "\n",
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        # Embedding\n",
    "        if col in num_scaler:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        elif col in embedding_cols:\n",
    "            num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "        \n",
    "        # Global token\n",
    "        self.global_token = torch.nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_enc = torch.nn.Parameter(get_positional_encoding())\n",
    "        \n",
    "\n",
    "    def forward(self, data, remain_idx):\n",
    "        # Embedding\n",
    "        embedding = self.embedding(data)\n",
    "\n",
    "        # Positional encoding without global token\n",
    "        pos_enc = self.pos_enc[:data.shape[1]+1]\n",
    "        embedding += pos_enc[1:]\n",
    "\n",
    "        # Select visible token only\n",
    "        remain_idx = remain_idx.unsqueeze(-1).repeat(1,1, embedding.shape[-1])\n",
    "        embedding = torch.gather(embedding, index=remain_idx, dim=1)\n",
    "\n",
    "        # Add global token\n",
    "        global_token = self.global_token.repeat(embedding.shape[0], 1, 1)\n",
    "        global_token += pos_enc[0] # Positional encoding\n",
    "        embedding = torch.cat([global_token, embedding], dim=1)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "class TemporalOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        if col in num_scaler:\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, 1)\n",
    "                            )\n",
    "        elif col in embedding_cols:\n",
    "            num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, num_cls)\n",
    "                            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.output(data)\n",
    "\n",
    "class StaticOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "        self.output = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(d_model, d_model), \n",
    "                        torch.nn.Linear(d_model, d_model), \n",
    "                        torch.nn.Linear(d_model, num_cls))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.output(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (modality_embedding): Embedding(3, 64)\n",
       "  (temporal_embedding_li): ModuleList(\n",
       "    (0-1): 2 x TemporalEmbedding(\n",
       "      (embedding): Linear(in_features=1, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): TemporalEmbedding(\n",
       "      (embedding): Embedding(8, 64)\n",
       "    )\n",
       "  )\n",
       "  (encoding): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_enc_li): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 1000x64 (GPU 0)]\n",
       "      (1): Parameter containing: [torch.float32 of size 1000x64 (GPU 0)]\n",
       "      (2): Parameter containing: [torch.float32 of size 1000x64 (GPU 0)]\n",
       "  )\n",
       "  (decoding): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (temporal_output_li): ModuleList(\n",
       "    (0-1): 2 x TemporalOutput(\n",
       "      (output): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TemporalOutput(\n",
       "      (output): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (2): Linear(in_features=64, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_layers, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        activation = \"gelu\"\n",
    "       \n",
    "       # Modality embedding\n",
    "        self.modality_embedding = torch.nn.Embedding(num_modality, d_model)\n",
    "\n",
    "       # Temporal embedding\n",
    "        temporal_embedding_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            temporal_embedding_li.append(TemporalEmbedding(col, d_model))\n",
    "        self.temporal_embedding_li = torch.nn.ModuleList(temporal_embedding_li)\n",
    "\n",
    "       # Encoding\n",
    "        self.encoding = Encoder(EncoderLayer(d_model, nhead, d_ff, dropout, activation, batch_first=True), num_layers)\n",
    "\n",
    "       # Revert\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, d_model))\n",
    "\n",
    "       # Positional encoding\n",
    "        pos_enc_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            pos_enc_li.append(torch.nn.Parameter(get_positional_encoding()))\n",
    "        self.pos_enc_li = torch.nn.ParameterList(pos_enc_li)\n",
    "\n",
    "        # Decoding\n",
    "        self.decoding = Encoder(EncoderLayer(d_model, nhead, d_ff, dropout, activation, batch_first=True), num_layers)\n",
    "\n",
    "        # Output\n",
    "        temporal_output_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            temporal_output_li.append(TemporalOutput(col, d_model))\n",
    "        self.temporal_output_li = torch.nn.ModuleList(temporal_output_li)\n",
    "    \n",
    "    def get_embedding(self, data_input):\n",
    "       # Embedding\n",
    "        modality_idx = 0\n",
    "       ### Temporal embedding\n",
    "        temporal_embedding_dict = {}\n",
    "        for col, temporal_embedding in zip(data_info[\"target\"] + data_info[\"temporal\"], self.temporal_embedding_li):\n",
    "            data = data_input[col].to(device)\n",
    "            remain_idx = data_input[f\"{col}_remain_idx\"].to(device)\n",
    "            temporal_embedding_dict[col] = temporal_embedding(data, remain_idx) # Embedding\n",
    "            temporal_embedding_dict[col] += self.modality_embedding(torch.tensor([modality_idx]).to(device)) # Modality embedding\n",
    "            modality_idx += 1\n",
    "        \n",
    "        return temporal_embedding_dict\n",
    "        \n",
    "    def get_padding_mask(self, data_input, mode):\n",
    "        temporal_padding_mask_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            mask = data_input[f\"{col}_{mode}_padding_mask\"]\n",
    "            mask = torch.cat([torch.ones(mask.shape[0], 1), mask], dim=1) # Add sequence for cls token\n",
    "            temporal_padding_mask_li.append(mask)\n",
    "        temporal_padding_mask = torch.cat(temporal_padding_mask_li, dim=-1).to(device)\n",
    "\n",
    "        ### Concatenate\n",
    "        padding_mask = torch.where(temporal_padding_mask == 1, 0, -torch.inf)\n",
    "        return padding_mask\n",
    "\n",
    "    def split_each(self, data, last_temporal_embedding_dict):\n",
    "        start_idx = 0\n",
    "        temporal_dict = {}\n",
    "        # Temporal\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            # Select specific feature\n",
    "            length = last_temporal_embedding_dict[col].shape[1]\n",
    "            selected_embedding = data[..., start_idx:start_idx+length, :]\n",
    "            temporal_dict[col] = selected_embedding\n",
    "            start_idx += length\n",
    "\n",
    "        return temporal_dict\n",
    "\n",
    "    def revert_embedding(self, data_input, temporal_encoding_dict):\n",
    "        reverted_temporal_dict = {}\n",
    "        modality_idx = 0\n",
    "        ### Revert temporal\n",
    "        for col, pos_enc in zip(data_info[\"target\"] + data_info[\"temporal\"], self.pos_enc_li):\n",
    "            # Process for revert index\n",
    "            reverted_embedding = self.apply_revert(embedding = temporal_encoding_dict[col],\n",
    "                                                    revert_idx = data_input[f\"{col}_revert_idx\"].to(device),\n",
    "                                                    mask_token = self.mask_token,\n",
    "                                                    pos_enc = pos_enc)\n",
    "            reverted_temporal_dict[col] = reverted_embedding + self.modality_embedding(torch.tensor([modality_idx]).to(device)) # Modality embedding\n",
    "            modality_idx += 1\n",
    "    \n",
    "        return reverted_temporal_dict\n",
    "\n",
    "    def forward(self, data_input):\n",
    "        # Embedding\n",
    "        temporal_embedding_dict = self.get_embedding(data_input)\n",
    "        encoder_padding_mask = self.get_padding_mask(data_input, mode=\"remain\")\n",
    "\n",
    "        # Encoding\n",
    "        encoder_input = torch.cat(list(temporal_embedding_dict.values()), dim=1)\n",
    "        try:\n",
    "            encoding, encoding_weight = self.encoding(encoder_input, src_key_padding_mask=encoder_padding_mask)\n",
    "        except :\n",
    "            print(\"encoder_input:\", encoder_input.shape)\n",
    "            print(\"encoder_padding_mask:\", encoder_padding_mask.shape)\n",
    "            raise\n",
    "\n",
    "        # Split\n",
    "        temporal_encoding_dict = self.split_each(encoding, temporal_embedding_dict)\n",
    "\n",
    "        # Revert\n",
    "        reverted_temporal_dict = self.revert_embedding(data_input, temporal_encoding_dict)\n",
    "        decoder_padding_mask = self.get_padding_mask(data_input, mode=\"revert\")\n",
    "\n",
    "        # Decoding\n",
    "        decoder_input = torch.cat(list(reverted_temporal_dict.values()), dim=1)\n",
    "        decoding, decoding_weight = self.decoding(decoder_input, src_key_padding_mask=decoder_padding_mask)\n",
    "\n",
    "        # Split\n",
    "        temporal_decoding_dict = self.split_each(decoding, reverted_temporal_dict)\n",
    "        temporal_decodingweight_dict = self.split_each(decoding_weight, reverted_temporal_dict)\n",
    "\n",
    "        # Output\n",
    "        ### Temporal output\n",
    "        temporal_output_dict = {}\n",
    "        for col, temporal_output in zip(data_info[\"target\"] + data_info[\"temporal\"], self.temporal_output_li):\n",
    "            data = temporal_decoding_dict[col]\n",
    "            temporal_output_dict[col] = temporal_output(data)\n",
    "\n",
    "        return temporal_output_dict, temporal_decodingweight_dict\n",
    "\n",
    "    def apply_revert(self, embedding, revert_idx, mask_token, **kwargs):\n",
    "        # Process for revert index\n",
    "        revert_idx += 1 # Shift one step for cls token\n",
    "        revert_idx = torch.cat([torch.zeros(revert_idx.shape[0], 1).to(device).to(torch.int),\n",
    "                                revert_idx], dim=1) # Append index for cls token, which should be reserved\n",
    "        revert_idx = revert_idx.unsqueeze(-1).repeat(1, 1, embedding.shape[-1])\n",
    "\n",
    "        # Append mask tokens for missing positions\n",
    "        mask_tokens = mask_token.unsqueeze(0).repeat(revert_idx.shape[0],\n",
    "                                                    revert_idx.shape[1]-embedding.shape[1],\\\n",
    "                                                    1)\n",
    "        full_embedding = torch.cat([embedding, mask_tokens], dim=1)\n",
    "\n",
    "        # Revert and re-positional encoding\n",
    "        reverted_embedding = torch.gather(full_embedding, index=revert_idx, dim=1)\n",
    "        \n",
    "        pos_enc = kwargs[\"pos_enc\"][:reverted_embedding.shape[1]]\n",
    "        reverted_embedding += pos_enc\n",
    "\n",
    "        return reverted_embedding\n",
    "\n",
    "model = Transformer(d_model, num_layers, nhead, d_ff, dropout)\n",
    "model.to(device)\n",
    "# data_input = {key:val for key, val in data.items() if key != \"img_raw\" and \"scaler\" not in key}\n",
    "# summary(model,\n",
    "#         data_input,\n",
    "#         show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs\n",
    "\n",
    "# res = patchify(data[\"img_input\"])\n",
    "# print(res.shape)\n",
    "# unpatchify(res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/420 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/420 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'month'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     34\u001b[0m sales_loss \u001b[38;5;241m=\u001b[39m num_temporal_loss_fn(temporal_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_masked_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_masked_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     35\u001b[0m dow_loss \u001b[38;5;241m=\u001b[39m cat_temporal_loss_fn(temporal_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_masked_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_masked_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m---> 36\u001b[0m month_loss \u001b[38;5;241m=\u001b[39m cat_temporal_loss_fn(\u001b[43mtemporal_output_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_masked_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_masked_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     37\u001b[0m holiday_loss \u001b[38;5;241m=\u001b[39m cat_temporal_loss_fn(temporal_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_masked_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_masked_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     38\u001b[0m price_loss \u001b[38;5;241m=\u001b[39m num_temporal_loss_fn(temporal_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, \u001b[38;5;241m1\u001b[39m:], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_masked_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_masked_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'month'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def num_temporal_loss_fn(pred, y, mask, padding_mask):\n",
    "    y = torch.gather(y, index=mask, dim=1)\n",
    "    pred = torch.gather(pred, index=mask, dim=1)\n",
    "    loss = mse_loss(pred, y)\n",
    "    loss = torch.where(padding_mask==1, loss, 0)\n",
    "    loss = loss.sum() / padding_mask.sum()\n",
    "    return loss\n",
    "\n",
    "def cat_temporal_loss_fn(pred, y, mask, padding_mask):\n",
    "    y = torch.gather(y, index=mask, dim=1).to(torch.long)\n",
    "\n",
    "    pred_mask = mask.unsqueeze(-1).repeat(1, 1, pred.shape[-1])\n",
    "    pred = torch.gather(pred, index=pred_mask, dim=1)\n",
    "\n",
    "    loss = ce_loss(pred.view(-1, pred.shape[-1]), y.view(-1)).view(y.shape)\n",
    "    loss = torch.where(padding_mask==1, loss, 0)\n",
    "    loss = loss.sum() / padding_mask.sum()\n",
    "    return loss\n",
    "\n",
    "def train(e):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_li, sales_loss_li, dow_loss_li, month_loss_li, holiday_loss_li, price_loss_li, window = [], [], [], [], [], [], 100\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        temporal_output_dict, temporal_decodingweight_dict = model(data)\n",
    "    \n",
    "        sales_loss = num_temporal_loss_fn(temporal_output_dict[\"sales\"][:, 1:], data[\"sales\"].to(device), data[\"sales_masked_idx\"].unsqueeze(-1).to(device), data[\"sales_masked_padding_mask\"].unsqueeze(-1).to(device))\n",
    "        dow_loss = cat_temporal_loss_fn(temporal_output_dict[\"dow\"][:, 1:], data[\"dow\"].to(device), data[\"dow_masked_idx\"].to(device), data[\"dow_masked_padding_mask\"].to(device))\n",
    "        month_loss = cat_temporal_loss_fn(temporal_output_dict[\"month\"][:, 1:], data[\"month\"].to(device), data[\"month_masked_idx\"].to(device), data[\"month_masked_padding_mask\"].to(device))\n",
    "        holiday_loss = cat_temporal_loss_fn(temporal_output_dict[\"holiday\"][:, 1:], data[\"holiday\"].to(device), data[\"holiday_masked_idx\"].to(device), data[\"holiday_masked_padding_mask\"].to(device))\n",
    "        price_loss = num_temporal_loss_fn(temporal_output_dict[\"price\"][:, 1:], data[\"price\"].to(device), data[\"price_masked_idx\"].unsqueeze(-1).to(device), data[\"price_masked_padding_mask\"].unsqueeze(-1).to(device))\n",
    "\n",
    "        loss = sales_loss + dow_loss + month_loss + holiday_loss + price_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_li.append(loss.item())\n",
    "        sales_loss_li.append(sales_loss.item())\n",
    "        dow_loss_li.append(dow_loss.item())\n",
    "        month_loss_li.append(month_loss.item())\n",
    "        holiday_loss_li.append(holiday_loss.item())\n",
    "        price_loss_li.append(price_loss.item())\n",
    "        pbar.set_description(f\"{e} - sales_loss: {np.mean(sales_loss_li[-window:])}, \\\n",
    "dow_loss: {np.mean(dow_loss_li[-window:])}, \\\n",
    "month_loss: {np.mean(month_loss_li[-window:])}, \\\n",
    "holiday_loss: {np.mean(holiday_loss_li[-window:])}, \\\n",
    "price_loss: {np.mean(price_loss_li[-window:])}, \\\n",
    "lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        if n %20 == 0:\n",
    "            idx = 0\n",
    "            plt.figure(figsize=(25,12))\n",
    "            nrows, ncols = 5, 3\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Loss\n",
    "            plt.subplot(nrows, ncols, 1)\n",
    "            plt.plot(loss_li[-window:])\n",
    "\n",
    "            # Sample\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            plt.plot(data[\"sales\"][idx], label=\"y\")\n",
    "            plt.plot(temporal_output_dict[\"sales\"][idx].detach().cpu(), label=\"pred\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Attn\n",
    "            plt.subplot(nrows, ncols, 3)\n",
    "            print(temporal_decodingweight_dict[\"sales\"].shape)\n",
    "            self_weight = temporal_decodingweight_dict[\"sales\"][..., 1:366][idx].mean(dim=0)\n",
    "            sns.heatmap(self_weight.detach().cpu())\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6711 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6711 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sales_idx_keep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m         sales_output, day_output, dow_output, month_output, holiday_output, price_output, index_output, color_output, graphic_output, prod_output, img_output,\\\n\u001b[1;32m      7\u001b[0m             temp_keep_mask, static_idx_keep, img_idx_keep\\\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;241m=\u001b[39m model(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      9\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     10\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     11\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m---> 12\u001b[0m                 \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales_idx_keep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     13\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     14\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     15\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     16\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     17\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     18\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_keep_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_revert_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m             idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sales_idx_keep'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pbar = tqdm(valid_dataloader)\n",
    "\n",
    "for n, data in enumerate(pbar):\n",
    "    with torch.no_grad():\n",
    "        sales_output, day_output, dow_output, month_output, holiday_output, price_output, index_output, color_output, graphic_output, prod_output, img_output,\\\n",
    "            temp_keep_mask, static_idx_keep, img_idx_keep\\\n",
    "            = model(data[\"sales\"].to(device),\n",
    "                data[\"day\"].to(device), data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "                data[\"index\"].to(device),  data[\"color\"].to(device),  data[\"graphic\"].to(device),  data[\"prod\"].to(device),\n",
    "                data[\"img_input\"].to(device),\n",
    "                data[\"sales_idx_keep\"].to(device), data[\"sales_revert\"].to(device),\n",
    "                data[\"day_idx_keep\"].to(device), data[\"day_revert\"].to(device),\n",
    "                data[\"dow_idx_keep\"].to(device), data[\"dow_revert\"].to(device),\n",
    "                data[\"month_idx_keep\"].to(device), data[\"month_revert\"].to(device),\n",
    "                data[\"holiday_idx_keep\"].to(device), data[\"holiday_revert\"].to(device),\n",
    "                data[\"price_idx_keep\"].to(device), data[\"price_revert\"].to(device),\n",
    "                data[\"temp_keep_padding_mask\"].to(device), data[\"temp_revert_padding_mask\"].to(device))\n",
    "\n",
    "        if n %20 == 0:\n",
    "            idx = 0\n",
    "            plt.figure(figsize=(25,12))\n",
    "            nrows, ncols = 5, 3\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Sample\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            valid_len = data[\"temp_revert_padding_mask\"].max(dim=-1).indices[idx]\n",
    "            valid_len = data[\"sales\"].shape[-1] if valid_len == 0 else valid_len\n",
    "            plt.plot(data[\"sales\"][idx][:valid_len], label=\"y\")\n",
    "            plt.plot(sales_output[idx][1:valid_len].detach().cpu(), label=\"pred\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Img\n",
    "            plt.subplot(nrows, ncols, 4)\n",
    "            img_input = data[\"img_input\"]\n",
    "            img_input = img_input.permute(0,2,3,1)\n",
    "            img_input = img_input.cpu().detach().numpy()\n",
    "            plt.imshow(img_input[idx])\n",
    "\n",
    "            plt.subplot(nrows, ncols, 5)\n",
    "            img_output = unpatchify(img_output[:, 1:]).permute(0,2,3,1)\n",
    "            img_output = img_output.cpu().detach().numpy()\n",
    "            plt.imshow(img_output[idx])\n",
    "\n",
    "            index_output = torch.argmax(index_output, dim=-1).detach().cpu().numpy()\n",
    "            index_output = train_dataset.get_encoder_dict()[\"index_encoder\"].inverse_transform(index_output[idx])\n",
    "            index_y = train_dataset.get_encoder_dict()[\"index_encoder\"].inverse_transform(data[\"index\"][idx].numpy()[0])\n",
    "            print(f\"y: {index_y}, pred: {index_output}\")\n",
    "\n",
    "            color_output = torch.argmax(color_output, dim=-1).detach().cpu().numpy()\n",
    "            color_output = train_dataset.get_encoder_dict()[\"color_encoder\"].inverse_transform(color_output[idx])\n",
    "            color_y = train_dataset.get_encoder_dict()[\"color_encoder\"].inverse_transform(data[\"color\"][idx].numpy()[0])\n",
    "            print(f\"y: {color_y}, pred: {color_output}\")\n",
    "\n",
    "            graphic_output = torch.argmax(graphic_output, dim=-1).detach().cpu().numpy()\n",
    "            graphic_output = train_dataset.get_encoder_dict()[\"graphic_encoder\"].inverse_transform(graphic_output[idx])\n",
    "            graphic_y = train_dataset.get_encoder_dict()[\"graphic_encoder\"].inverse_transform(data[\"graphic\"][idx].numpy()[0])\n",
    "            print(f\"y: {graphic_y}, pred: {graphic_output}\")\n",
    "\n",
    "            prod_output = torch.argmax(prod_output, dim=-1).detach().cpu().numpy()\n",
    "            prod_output = train_dataset.get_encoder_dict()[\"prod_encoder\"].inverse_transform(prod_output[idx])\n",
    "            prod_y = train_dataset.get_encoder_dict()[\"prod_encoder\"].inverse_transform(data[\"prod\"][idx].numpy()[0])\n",
    "            print(f\"y: {prod_y}, pred: {prod_output}\")\n",
    "\n",
    "            plt.show()\n",
    "            input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
