{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from copy import copy\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed = 0\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "kind = 3\n",
    "\n",
    "arr1 = torch.round(torch.rand(batch_size, seq_len, d_model)*10)\n",
    "arr2 = torch.round(torch.rand(batch_size, seq_len, d_model)*10)\n",
    "arr3 = torch.round(torch.rand(batch_size, seq_len, d_model)*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_self_mask(kind, seq_len):\n",
    "    arr = torch.zeros(kind, kind) + 1\n",
    "    mask = torch.block_diag(*[arr]*seq_len)\n",
    "    mask = torch.where(mask==1, 0, -torch.inf)\n",
    "    return mask\n",
    "\n",
    "def get_cross_mask(kind, seq_len):\n",
    "    arr = torch.zeros(1, kind) + 1\n",
    "    mask = torch.block_diag(*[arr]*seq_len)\n",
    "    mask = torch.where(mask==1, 0, -torch.inf)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    # Efficient implementation equivalent to the following:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value, attn_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 12, 8])\n",
      "attn_weight: torch.Size([2, 12, 12])\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1302e-01, 8.8690e-01, 7.9086e-05],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-5.2547, -2.5219, -3.8461, -2.2512, -0.8428,  3.0889, -4.2951, -0.1270],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2).view(batch_size, -1, d_model)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "attn_output, attn_weight = scaled_dot_product_attention(query=Q, key=K, value=V, attn_mask=get_self_mask(kind, seq_len))\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 12, 8])\n",
      "attn_weight: torch.Size([2, 12, 12])\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1302e-01, 8.8690e-01, 7.9086e-05],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-5.2547, -2.5219, -3.8461, -2.2512, -0.8428,  3.0889, -4.2951, -0.1270],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2).view(batch_size, -1, d_model)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "# softmax(QKt)/sqrt(d_model)\n",
    "### QKt\n",
    "QKt = torch.matmul(Q, K.permute(0,2,1))\n",
    "### QKt/sqrt(d_model)\n",
    "QKt /= math.sqrt(d_model)\n",
    "## Mask\n",
    "QKt += (get_self_mask(kind, seq_len))\n",
    "### Softmax()\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "attn_weight = softmax(QKt)\n",
    "### Softmax() @ V\n",
    "attn_output = attn_weight @ V\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 4, 3, 8])\n",
      "attn_weight: torch.Size([2, 4, 3, 3])\n",
      "tensor([[2.6076e-01, 7.3909e-01, 1.5296e-04],\n",
      "        [2.2207e-03, 9.9652e-01, 1.2559e-03],\n",
      "        [1.1302e-01, 8.8690e-01, 7.9086e-05]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-5.6034, -2.4412, -3.4502, -1.4781, -0.6058,  2.9580, -4.1531,  0.1394],\n",
      "        [-4.9994, -2.5835, -4.1425, -2.8293, -1.0179,  3.1918, -4.4018, -0.3230],\n",
      "        [-5.2547, -2.5219, -3.8461, -2.2512, -0.8428,  3.0889, -4.2951, -0.1270]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "\n",
    "# Attention score\n",
    "attn_weight = torch.nn.functional.softmax(Q@K.permute(0,1,3,2)/math.sqrt(d_model), dim=-1)\n",
    "attn_output = attn_weight @ V\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 4, 8])\n",
      "attn_weight: torch.Size([2, 4, 12])\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6076e-01, 7.3909e-01, 1.5296e-04],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-5.6034, -2.4412, -3.4502, -1.4781, -0.6058,  2.9580, -4.1531,  0.1394],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2).view(batch_size, -1, d_model)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr1)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "attn_output, attn_weight = scaled_dot_product_attention(query=Q, key=K, value=V, attn_mask=get_cross_mask(kind, seq_len))\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 4, 8])\n",
      "attn_weight: torch.Size([2, 4, 12])\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6076e-01, 7.3909e-01, 1.5296e-04],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-5.6034, -2.4412, -3.4502, -1.4781, -0.6058,  2.9580, -4.1531,  0.1394],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2).view(batch_size, -1, d_model)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr1)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "# softmax(QKt)/sqrt(d_model)\n",
    "### QKt\n",
    "QKt = torch.matmul(Q, K.permute(0,2,1))\n",
    "\n",
    "### QKt/sqrt(d_model)\n",
    "QKt /= math.sqrt(d_model)\n",
    "## Mask\n",
    "QKt += get_cross_mask(kind, seq_len)\n",
    "### Softmax()\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "attn_weight = softmax(QKt)\n",
    "attn_output = attn_weight @ V\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 4, 1, 8])\n",
      "attn_weight: torch.Size([2, 4, 1, 3])\n",
      "tensor([[2.6076e-01, 7.3909e-01, 1.5296e-04]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-5.6034, -2.4412, -3.4502, -1.4781, -0.6058,  2.9580, -4.1531,  0.1394]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr1).unsqueeze(-2)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "# Attention score\n",
    "attn_weight = torch.nn.functional.softmax(Q@K.permute(0,1,3,2)/math.sqrt(d_model), dim=-1)\n",
    "attn_output = attn_weight @ V\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_self_mask(kind, seq_len):\n",
    "    arr = torch.zeros(kind, kind) + 1\n",
    "    mask = torch.block_diag(*[arr]*seq_len)\n",
    "    mask = torch.where(mask==1, 0, -torch.inf)\n",
    "    return mask\n",
    "\n",
    "def get_cross_mask(kind, seq_len):\n",
    "    arr = torch.zeros(1, kind) + 1\n",
    "    mask = torch.block_diag(*[arr]*seq_len)\n",
    "    mask = torch.where(mask==1, 0, -torch.inf)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n",
    "    # Efficient implementation equivalent to the following:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "    # print(\"query\")\n",
    "    # print(query.shape)\n",
    "    # print(query[0][0])\n",
    "    # print(\"_\"*100)\n",
    "    # print(key.transpose(-2, -1).shape)\n",
    "    # print((key.transpose(-2, -1))[0][0]); print(\"_\"*100)\n",
    "    # print((query @ key.transpose(-2, -1)).shape)\n",
    "    # print(query @ key.transpose(-2, -1))\n",
    "    # print(\"_\"*100)\n",
    "    # print(attn_bias)\n",
    "    # raise\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    # attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 12, 8])\n",
      "attn_weight: torch.Size([2, 2, 12, 12])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.1066, 0.8446, 0.0488], grad_fn=<SelectBackward0>)\n",
      "tensor([-5.6971, -2.4192, -3.3432, -1.2691, -0.7473,  3.2825, -4.3113,  0.0101],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "nhead = 2\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2).view(batch_size, -1, d_model)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "# Split heads\n",
    "Q = Q.view(batch_size, -1, nhead, d_model//nhead).permute(0,2,1,3)\n",
    "K = K.view(batch_size, -1, nhead, d_model//nhead).permute(0,2,1,3)\n",
    "V = V.view(batch_size, -1, nhead, d_model//nhead).permute(0,2,1,3)\n",
    "\n",
    "attn_output, attn_weight = scaled_dot_product_attention(query=Q, key=K, value=V, attn_mask=get_self_mask(kind, seq_len))\n",
    "attn_output = attn_output.permute(0,2,1,3).reshape(batch_size, -1, d_model)\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_output: torch.Size([2, 4, 3, 8])\n",
      "attn_weight: torch.Size([2, 2, 4, 3, 3])\n",
      "tensor([[1.0351e-01, 8.2873e-01, 6.7763e-02],\n",
      "        [2.0962e-04, 9.9912e-01, 6.6725e-04],\n",
      "        [1.0660e-01, 8.4464e-01, 4.8763e-02]], grad_fn=<SelectBackward0>)\n",
      "tensor([[-6.5140, -2.2299, -2.4150,  0.5435, -0.7110,  3.3585, -4.3182,  0.0625],\n",
      "        [-6.3079, -2.4043, -2.9771, -0.5100, -1.0224,  3.1913, -4.4036, -0.3284],\n",
      "        [-5.6971, -2.4192, -3.3432, -1.2691, -0.7473,  3.2825, -4.3113,  0.0101]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "arr = torch.stack([arr1, arr2, arr3], dim=-2)\n",
    "\n",
    "# Linear transform\n",
    "Q_linear = torch.nn.Linear(d_model, d_model)\n",
    "K_linear = torch.nn.Linear(d_model, d_model)\n",
    "V_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Q_linear(arr)\n",
    "K = K_linear(arr)\n",
    "V = V_linear(arr)\n",
    "\n",
    "Q = Q.view(batch_size, seq_len, kind, nhead, d_model//nhead).permute(0,3,1,2,4)\n",
    "K = K.view(batch_size, seq_len, kind, nhead, d_model//nhead).permute(0,3,1,2,4)\n",
    "V = V.view(batch_size, seq_len, kind, nhead, d_model//nhead).permute(0,3,1,2,4)\n",
    "\n",
    "# Attention score\n",
    "# print(\"Q\")\n",
    "# print(Q.shape)\n",
    "# print(Q[0][0])\n",
    "# print(\"_\"*100)\n",
    "# print(\"K\")\n",
    "# print(K.permute(0,1,2,4,3).shape)\n",
    "# print((K.permute(0,1,2,4,3))[0][0]); print(\"_\"*100)\n",
    "# print(Q@K.permute(0,1,2,4,3)[0][0])\n",
    "# raise\n",
    "attn_weight = torch.nn.functional.softmax((Q@K.permute(0,1,2,4,3))/math.sqrt(d_model//nhead), dim=-1)\n",
    "attn_output = attn_weight @ V\n",
    "attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, kind, d_model)\n",
    "\n",
    "print(\"attn_output:\", attn_output.shape)\n",
    "print(\"attn_weight:\", attn_weight.shape)\n",
    "print(attn_weight[-1][-1][-1])\n",
    "print(attn_output[-1][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
