{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "is_skip = True\n",
    "\n",
    "# Data params\n",
    "target_batch_size = 128\n",
    "batch_size = 16\n",
    "# batch_size = 4\n",
    "\n",
    "grad_accm_step_max = target_batch_size // batch_size\n",
    "print(grad_accm_step_max)\n",
    "\n",
    "# Model params\n",
    "d_model = 128\n",
    "nhead = 4\n",
    "d_ff = 256\n",
    "dropout = 0.1\n",
    "num_layers = 4\n",
    "remain_rto_general = 0.3\n",
    "remain_rto_cat = 0.7\n",
    "\n",
    "patch_size = 14  # 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "\n",
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from transformers import ViTImageProcessor, MobileViTModel, MobileViTConfig, ViTModel, AutoImageProcessor, ViTConfig, BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer, Dinov2Config, Dinov2Model\n",
    "from skimage import io\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_skip:\n",
    "    us_holiday = holidays.US()\n",
    "    \n",
    "    # Read transaction\n",
    "    df_trans = pd.read_csv(\"../HnM/transactions_train.csv\", parse_dates=[\"t_dat\"], dtype={\"article_id\":str})\n",
    "    df_meta = pd.read_csv(\"../HnM/articles.csv\", dtype={\"article_id\":str})\n",
    "\n",
    "    min_year = df_trans[\"t_dat\"].dt.year.min()\n",
    "    max_year = df_trans[\"t_dat\"].dt.year.max()\n",
    "\n",
    "    holiday = holidays.US(years=(min_year, max_year))\n",
    "    holiday = pd.DataFrame({\"t_dat\":holiday.keys(), \"holiday\":holiday.values()})\n",
    "    holiday[\"t_dat\"] = pd.to_datetime(holiday[\"t_dat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Image path\n",
    "    data[\"img_path\"] = data[\"article_id\"].apply(lambda x: f'../HnM/resized_images/{x[:3]}/{x}.jpg')\n",
    "    data[\"is_valid\"] = data[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0) # Check whether the article has corresponding image file\n",
    "    data = data[data[\"is_valid\"] == 1].drop(\"is_valid\", axis=1) # Valid if having corresponding image\n",
    "\n",
    "    # Make sales\n",
    "    data = data.groupby([\"t_dat\", \"article_id\", \"img_path\"], as_index=False).agg(sales=(\"customer_id\", \"count\"), price=(\"price\", \"mean\"))\n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data = data[(data[\"size\"]>=100)]\n",
    "\n",
    "    # Expand dates\n",
    "    data = data.set_index(\"t_dat\").groupby([\"article_id\"], as_index=False).resample(\"1D\").asfreq().reset_index()\n",
    "    data[\"sales\"] = data[\"sales\"].fillna(0)\n",
    "    data[\"price\"] = data[\"price\"].fillna(method=\"ffill\")\n",
    "    data[\"article_id\"] = data[\"article_id\"].fillna(method=\"ffill\")\n",
    "    data[\"img_path\"] = data[\"img_path\"].fillna(method=\"ffill\")\n",
    "    data = data.sort_values([\"article_id\", \"t_dat\"])\n",
    "    \n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data[\"time_idx\"] = data.groupby(\"article_id\").cumcount()\n",
    "    data[\"time_idx\"] = data[\"size\"] - data[\"time_idx\"]\n",
    "\n",
    "    # Make holidays\n",
    "    data = pd.merge(data, holiday, on=\"t_dat\", how=\"left\")\n",
    "    display(data)\n",
    "\n",
    "    # Temporal information\n",
    "    # Make sure the sequence start from 0\n",
    "    data[\"day\"] = data[\"t_dat\"].dt.day - 1\n",
    "    data[\"dow\"] = data[\"t_dat\"].dt.dayofweek\n",
    "    data[\"month\"] = data[\"t_dat\"].dt.month - 1\n",
    "    data[\"year\"] = data[\"t_dat\"].dt.year / (data[\"t_dat\"].dt.year.max() + 1)\n",
    "\n",
    "    # Append meta data\n",
    "    # data = data.merge(df_meta[[\"index_name\", \"article_id\", \"colour_group_name\", \"graphical_appearance_name\", \"product_type_name\"]], on=\"article_id\")\n",
    "    data = data.merge(df_meta[[\"index_name\", \"article_id\", \"colour_group_name\", \"graphical_appearance_name\", \"prod_name\"]], on=\"article_id\")\n",
    "    data = data.rename(columns={\"index_name\":\"index\", \"colour_group_name\":\"color\", \"graphical_appearance_name\":\"graphic\", \"prod_name\":\"prod\"})\n",
    "\n",
    "    # Output\n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "if not is_skip:\n",
    "    df_prep = df_trans.copy()#.iloc[:1000]\n",
    "    data = preprocess(df_prep)\n",
    "    \n",
    "    data.to_parquet(\"data.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        self.a = 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return np.log1p(x)\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        return np.expm1(x)\n",
    "    \n",
    "class NoneScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x\n",
    "\n",
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        x = set(x)\n",
    "        for val in x:\n",
    "            if val not in self.mapper.keys():\n",
    "                self.mapper[val] = self.idx\n",
    "                self.idx += 1\n",
    "\n",
    "        self.mapper[\"unseen\"] = self.idx\n",
    "        self.idx += 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        res = []\n",
    "        for val in x:\n",
    "            if val in self.mapper.keys():\n",
    "                res.append(self.mapper[val])\n",
    "            else:\n",
    "                res.append(self.idx)\n",
    "        return np.array(res)\n",
    "    \n",
    "    def inverse_transform(self, idx):\n",
    "        inverse_mapper = {val:key for key, val in self.mapper.items()}\n",
    "        return inverse_mapper[idx]\n",
    "\n",
    "# 1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_info = {\n",
    "    \"group\": [\"article_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"static\": [],\n",
    "    \"img_path\": []\n",
    "}\n",
    "\n",
    "num_scaler = {\"sales\": LogScaler, \"price\": LogScaler}\n",
    "embedding_cols = [\"day\", \"dow\", \"month\", \"holiday\"]\n",
    "img_col = []\n",
    "\n",
    "assert {v for key, val in data_info.items() for v in val if key!=\"group\"} \\\n",
    "        == set(num_scaler.keys()).union(set(embedding_cols)).union(set(img_col))\n",
    "\n",
    "num_modality = len([v for key, val in data_info.items() for v in val if key not in [\"group\"]]) - len(data_info[\"static\"]) + 1\n",
    "num_modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting label encoder for holiday: 100%|██████████| 4/4 [00:00<00:00,  5.10it/s]\n",
      "100%|██████████| 26842/26842 [00:01<00:00, 26169.54it/s]\n",
      "100%|██████████| 26842/26842 [00:01<00:00, 26480.40it/s]\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales: torch.Size([16, 365, 1])\n",
      "sales_remain_idx: torch.Size([16, 109])\n",
      "sales_revert_idx: torch.Size([16, 365])\n",
      "sales_remain_padding_mask: torch.Size([16, 109])\n",
      "sales_revert_padding_mask: torch.Size([16, 365])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_parquet(\"data.pq\")\n",
    "df_train = data[data[\"time_idx\"]<=365]\n",
    "df_valid = data[data[\"time_idx\"]<=365+90]\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, data_info, num_scaler, embedding_cols, is_train=True, label_encoder_dict=None):\n",
    "        # Define parameters\n",
    "        self.data_info, self.num_scaler, self.embedding_cols, self.is_train = data_info, num_scaler, embedding_cols, is_train\n",
    "        assert label_encoder_dict is None if is_train else not None # In test mode, label_encoder_dict should be provided\n",
    "\n",
    "        # Fit label encoder\n",
    "        if is_train:\n",
    "            label_encoder_dict = self.encode_label(data, embedding_cols)\n",
    "        self.label_encoder_dict = label_encoder_dict\n",
    "\n",
    "        # Iterate each product\n",
    "        self.data_li = []\n",
    "        data.groupby(data_info[\"group\"]).progress_apply(lambda x: self.data_li.append(x))\n",
    "\n",
    "    def encode_label(self, data, embedding_cols):\n",
    "        label_encoder_dict = {}\n",
    "        pbar = tqdm(embedding_cols)\n",
    "        for col in pbar:\n",
    "            encoder = CustomLabelEncoder()\n",
    "            encoder.fit(data[col])\n",
    "            label_encoder_dict[col] = encoder\n",
    "            pbar.set_description(f\"Fitting label encoder for {col}\")\n",
    "        \n",
    "        return label_encoder_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_li)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_li[idx]\n",
    "        data_input = {}\n",
    "\n",
    "        # Transform with label encoder\n",
    "        for col in self.embedding_cols:\n",
    "            temp_res = self.label_encoder_dict[col].transform(data[col])\n",
    "            if col in data_info[\"static\"]:\n",
    "                data_input[col] = torch.IntTensor(temp_res[[0]])\n",
    "            else:\n",
    "                data_input[col] = torch.IntTensor(temp_res)\n",
    "        \n",
    "        # Scale target data\n",
    "        for col, scaler in self.num_scaler.items():\n",
    "            scaler = scaler()\n",
    "            data_input[col] = torch.Tensor(scaler.fit_transform(data[col].values.reshape(-1,1)))\n",
    "            data_input[f\"{col}_scaler\"] = scaler\n",
    "\n",
    "        # Mask temporal data\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            remain_idx, revert_idx, masked_idx, remain_padding_mask, revert_padding_mask, masked_padding_mask = self.get_mask(data_input[col])\n",
    "\n",
    "            data_input[f\"{col}_remain_idx\"] = remain_idx\n",
    "            data_input[f\"{col}_revert_idx\"] = revert_idx\n",
    "            data_input[f\"{col}_masked_idx\"] = masked_idx\n",
    "            data_input[f\"{col}_remain_padding_mask\"] = remain_padding_mask\n",
    "            data_input[f\"{col}_revert_padding_mask\"] = revert_padding_mask\n",
    "            data_input[f\"{col}_masked_padding_mask\"] = masked_padding_mask\n",
    "\n",
    "        return data_input\n",
    "\n",
    "    def get_mask(self, data):\n",
    "        num_remain = int(data.shape[0] * remain_rto_general) if self.is_train else -90\n",
    "\n",
    "        # Index for shuffle and revert\n",
    "        if self.is_train:\n",
    "            noise = torch.rand(data.shape[0])\n",
    "            shuffle_idx = torch.argsort(noise, dim=0)\n",
    "            revert_idx = torch.argsort(shuffle_idx, dim=0)\n",
    "        else:\n",
    "            shuffle_idx = torch.arange(data.shape[0])\n",
    "            revert_idx = torch.argsort(shuffle_idx)\n",
    "\n",
    "        remain_idx = shuffle_idx[:num_remain]\n",
    "        masked_idx = shuffle_idx[num_remain:]\n",
    "\n",
    "        remain_padding_mask = torch.ones(remain_idx.shape)\n",
    "        revert_padding_mask = torch.ones(revert_idx.shape)\n",
    "        masked_padding_mask = torch.ones(masked_idx.shape)\n",
    "\n",
    "        return remain_idx, revert_idx, masked_idx, remain_padding_mask, revert_padding_mask, masked_padding_mask\n",
    "\n",
    "def collate_fn(batch_li):\n",
    "    return_dict = {}\n",
    "\n",
    "    # Process temporal data - Apply padding\n",
    "    for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "        data = [batch[col] for batch in batch_li]\n",
    "        data_remain_idx = [batch[f\"{col}_remain_idx\"] for batch in batch_li]\n",
    "        data_revert_idx = [batch[f\"{col}_revert_idx\"] for batch in batch_li]\n",
    "        data_masked_idx = [batch[f\"{col}_masked_idx\"] for batch in batch_li]\n",
    "        data_remain_padding_mask = [batch[f\"{col}_remain_padding_mask\"] for batch in batch_li]\n",
    "        data_revert_padding_mask = [batch[f\"{col}_revert_padding_mask\"] for batch in batch_li]\n",
    "        data_masked_padding_mask = [batch[f\"{col}_masked_padding_mask\"] for batch in batch_li]\n",
    "\n",
    "        # For ordinary data\n",
    "        return_dict[col] = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "        # For remain and revert index\n",
    "        padding_value = return_dict[col].shape[-1] - 1 # The last index of the data\n",
    "        return_dict[f\"{col}_remain_idx\"] = torch.nn.utils.rnn.pad_sequence(data_remain_idx, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_revert_idx\"] = torch.nn.utils.rnn.pad_sequence(data_revert_idx, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_masked_idx\"] = torch.nn.utils.rnn.pad_sequence(data_masked_idx, batch_first=True, padding_value=padding_value)\n",
    "        # For padding masks\n",
    "        padding_value = 0\n",
    "        return_dict[f\"{col}_remain_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_remain_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_revert_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_revert_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "        return_dict[f\"{col}_masked_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_masked_padding_mask, batch_first=True, padding_value=padding_value)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "train_dataset = Dataset(df_train, data_info, num_scaler, embedding_cols)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=24)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = Dataset(df_valid, data_info, num_scaler, embedding_cols, is_train=False, label_encoder_dict=train_dataset.label_encoder_dict)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=16)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    print(\"sales:\", data[\"sales\"].shape)\n",
    "    print(\"sales_remain_idx:\", data[\"sales_remain_idx\"].shape)\n",
    "    print(\"sales_revert_idx:\", data[\"sales_revert_idx\"].shape)\n",
    "    print(\"sales_remain_padding_mask:\", data[\"sales_remain_padding_mask\"].shape)\n",
    "    print(\"sales_revert_padding_mask:\", data[\"sales_revert_padding_mask\"].shape)\n",
    "    # print(\"img_input\", data[\"img_input\"].shape)\n",
    "    # print(\"index:\", data[\"index\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for valid_data in valid_dataloader:\n",
    "#     print(\"sales:\", valid_data[\"sales\"].shape)\n",
    "#     print(\"sales_remain_idx:\", valid_data[\"sales_remain_idx\"].shape)\n",
    "#     print(\"sales_revert_idx:\", valid_data[\"sales_revert_idx\"].shape)\n",
    "#     print(\"sales_remain_padding_mask:\", valid_data[\"sales_remain_padding_mask\"].shape)\n",
    "#     print(\"sales_revert_padding_mask:\", valid_data[\"sales_revert_padding_mask\"].shape)\n",
    "#     print(\"img_input\", valid_data[\"img_input\"].shape)\n",
    "#     print(\"index:\", valid_data[\"index\"].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def _generate_square_subsequent_mask(sz, device, dtype):\n",
    "    return torch.triu(\n",
    "        torch.full((sz, sz), float('-inf'), dtype=dtype, device=device),\n",
    "        diagonal=1,\n",
    "    )\n",
    "\n",
    "def _get_seq_len(src, batch_first):\n",
    "    if src.is_nested:\n",
    "        return None\n",
    "    else:\n",
    "        src_size = src.size()\n",
    "        if len(src_size) == 2:\n",
    "            # unbatched: S, E\n",
    "            return src_size[0]\n",
    "        else:\n",
    "            # batched: B, S, E if batch_first else S, B, E\n",
    "            seq_len_pos = 1 if batch_first else 0\n",
    "            return src_size[seq_len_pos]\n",
    "\n",
    "def _detect_is_causal_mask(mask, is_causal=None,size=None):\n",
    "    # Prevent type refinement\n",
    "    make_causal = (is_causal is True)\n",
    "\n",
    "    if is_causal is None and mask is not None:\n",
    "        sz = size if size is not None else mask.size(-2)\n",
    "        causal_comparison = _generate_square_subsequent_mask(\n",
    "            sz, device=mask.device, dtype=mask.dtype)\n",
    "\n",
    "        # Do not use `torch.equal` so we handle batched masks by\n",
    "        # broadcasting the comparison.\n",
    "        if mask.size() == causal_comparison.size():\n",
    "            make_causal = bool((mask == causal_comparison).all())\n",
    "        else:\n",
    "            make_causal = False\n",
    "\n",
    "    return make_causal\n",
    "\n",
    "class EncoderLayer(torch.nn.TransformerEncoderLayer):\n",
    "    def forward(self, src, pos_enc, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
    "        x = src\n",
    "        attn_output, attn_weight = self._sa_block(x, pos_enc, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x, attn_weight\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x, pos_enc, attn_mask, key_padding_mask, is_causal=False):\n",
    "        x, attn_weight = self.self_attn(x+pos_enc, x+pos_enc, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=True, is_causal=is_causal, average_attn_weights=False)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "class Encoder(torch.nn.TransformerEncoder):\n",
    "    def forward(self, src, pos_enc=0, mask=None, src_key_padding_mask=None, is_causal=None):\n",
    "       ################################################################################################################\n",
    "        src_key_padding_mask = F._canonical_mask(\n",
    "            mask=src_key_padding_mask,\n",
    "            mask_name=\"src_key_padding_mask\",\n",
    "            other_type=F._none_or_dtype(mask),\n",
    "            other_name=\"mask\",\n",
    "            target_type=src.dtype\n",
    "        )\n",
    "\n",
    "        mask = F._canonical_mask(\n",
    "            mask=mask,\n",
    "            mask_name=\"mask\",\n",
    "            other_type=None,\n",
    "            other_name=\"\",\n",
    "            target_type=src.dtype,\n",
    "            check_other=False,\n",
    "        )\n",
    "\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        src_key_padding_mask_for_layers = src_key_padding_mask\n",
    "        why_not_sparsity_fast_path = ''\n",
    "        str_first_layer = \"self.layers[0]\"\n",
    "        batch_first = first_layer.self_attn.batch_first\n",
    "        if not hasattr(self, \"use_nested_tensor\"):\n",
    "            why_not_sparsity_fast_path = \"use_nested_tensor attribute not present\"\n",
    "        elif not self.use_nested_tensor:\n",
    "            why_not_sparsity_fast_path = \"self.use_nested_tensor (set in init) was not True\"\n",
    "        elif first_layer.training:\n",
    "            why_not_sparsity_fast_path = f\"{str_first_layer} was in training mode\"\n",
    "        elif not src.dim() == 3:\n",
    "            why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "        elif src_key_padding_mask is None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask was None\"\n",
    "        elif (((not hasattr(self, \"mask_check\")) or self.mask_check)\n",
    "                and not torch._nested_tensor_from_mask_left_aligned(src, src_key_padding_mask.logical_not())):\n",
    "            why_not_sparsity_fast_path = \"mask_check enabled, and src and src_key_padding_mask was not left aligned\"\n",
    "        elif output.is_nested:\n",
    "            why_not_sparsity_fast_path = \"NestedTensor input is not supported\"\n",
    "        elif mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask and mask were both supplied\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_sparsity_fast_path:\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                first_layer.self_attn.in_proj_weight,\n",
    "                first_layer.self_attn.in_proj_bias,\n",
    "                first_layer.self_attn.out_proj.weight,\n",
    "                first_layer.self_attn.out_proj.bias,\n",
    "                first_layer.norm1.weight,\n",
    "                first_layer.norm1.bias,\n",
    "                first_layer.norm2.weight,\n",
    "                first_layer.norm2.bias,\n",
    "                first_layer.linear1.weight,\n",
    "                first_layer.linear1.bias,\n",
    "                first_layer.linear2.weight,\n",
    "                first_layer.linear2.bias,\n",
    "            )\n",
    "            _supported_device_type = [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif src.device.type not in _supported_device_type:\n",
    "                why_not_sparsity_fast_path = f\"src device is neither one of {_supported_device_type}\"\n",
    "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                              \"input/output projection weights or biases requires_grad\")\n",
    "\n",
    "            if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):\n",
    "                convert_to_nested = True\n",
    "                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
    "                src_key_padding_mask_for_layers = None\n",
    "\n",
    "        seq_len = _get_seq_len(src, batch_first)\n",
    "        is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n",
    "       ################################################################################################################\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output, attn_weight = mod(output, pos_enc, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
    "\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0., src.size())\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len=1000):\n",
    "    position = torch.arange(seq_len).reshape(-1,1).to(device)\n",
    "    i = torch.arange(d_model).to(device)//2\n",
    "    exp_term = 2*i/d_model\n",
    "    div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "    pos_encoded = position / div_term\n",
    "\n",
    "    pos_encoded[:, 0::2] = torch.sin(pos_encoded[:, 0::2])\n",
    "    pos_encoded[:, 1::2] = torch.cos(pos_encoded[:, 1::2])\n",
    "\n",
    "    return pos_encoded\n",
    "\n",
    "def apply_mask(data, remain_rto):\n",
    "    num_remain = int(data.shape[1] * remain_rto)\n",
    "\n",
    "    # Index for shuffle and revert\n",
    "    noise = torch.rand(data.shape[:-1]).to(device)\n",
    "    shuffle_idx = torch.argsort(noise, dim=1)\n",
    "    revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "    remain_idx = shuffle_idx[:, :num_remain]\n",
    "    masked_idx = shuffle_idx[:, num_remain:]\n",
    "\n",
    "    # Apply mask\n",
    "    remain_idx_ = remain_idx.unsqueeze(-1).repeat(1, 1, data.shape[-1])\n",
    "    data = torch.gather(data, index=remain_idx_, dim=1)\n",
    "\n",
    "    return data, remain_idx, revert_idx, masked_idx\n",
    "\n",
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        # Embedding\n",
    "        if col in num_scaler:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        elif col in embedding_cols:\n",
    "            num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "        \n",
    "        # Global token\n",
    "        self.global_token = torch.nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "\n",
    "        # Positional encoding\n",
    "        self.pos_enc = torch.nn.Parameter(get_positional_encoding())\n",
    "        \n",
    "\n",
    "    def forward(self, data, remain_idx):\n",
    "        # Embedding\n",
    "        embedding = self.embedding(data)\n",
    "\n",
    "        # Positional encoding without global token\n",
    "        pos_enc = self.pos_enc[:data.shape[1]+1]\n",
    "        embedding += pos_enc[1:]\n",
    "\n",
    "        # Select visible token only\n",
    "        remain_idx = remain_idx.unsqueeze(-1).repeat(1,1, embedding.shape[-1])\n",
    "        embedding = torch.gather(embedding, index=remain_idx, dim=1)\n",
    "\n",
    "        # Add global token\n",
    "        global_token = self.global_token.repeat(embedding.shape[0], 1, 1)\n",
    "        global_token += pos_enc[0] # Positional encoding\n",
    "        embedding = torch.cat([global_token, embedding], dim=1)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "class StaticIndividualEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        # Embedding\n",
    "        num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "        self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Embedding\n",
    "        embedding = self.embedding(data)\n",
    "        return embedding\n",
    "\n",
    "class StaticEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        # Positional embedding\n",
    "        self.pos_emb = torch.nn.Embedding(4, d_model)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # Stack variables\n",
    "        embedding = torch.stack(list(data.values()), dim=1)\n",
    "\n",
    "        # Positional embedding\n",
    "        pos_emb = self.pos_emb(torch.tensor([0,1,2,3,4]).to(device))\n",
    "        embedding += pos_emb[1:]\n",
    "\n",
    "        # Apply mask\n",
    "        embedding, remain_idx, revert_idx, masked_idx = apply_mask(embedding, remain_rto_cat)\n",
    "        \n",
    "        # Append global token\n",
    "        global_token = self.global_token.repeat(embedding.shape[0], 1, 1)\n",
    "        global_token += pos_emb[1]\n",
    "        embedding = torch.cat([global_token, embedding], dim=1)\n",
    "        \n",
    "        return embedding, remain_idx, revert_idx, masked_idx\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model, patch_size, img_size=224):\n",
    "        super().__init__()\n",
    "        # Embedding\n",
    "        self.conv = torch.nn.Conv2d(3, d_model, patch_size, patch_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        # self.pos_enc = torch.nn.Parameter(get_positional_encoding())\n",
    "        self.pos_enc = get_positional_encoding()\n",
    "\n",
    "        # Global token\n",
    "        # self.global_token = torch.nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.global_token = torch.zeros(1, 1, d_model).to(device)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # Embedding\n",
    "        patches = self.conv(img).permute(0, 2, 3, 1)\n",
    "        bs, img_h, img_w, d_model = patches.shape\n",
    "        embedding = patches.view(bs, -1, d_model)\n",
    "\n",
    "        # Positional encoding without global token\n",
    "        pos_enc = self.pos_enc[:embedding.shape[1]+1]\n",
    "        embedding += pos_enc[1:]\n",
    "\n",
    "        # Apply mask\n",
    "        embedding, remain_idx, revert_idx, masked_idx = apply_mask(embedding, remain_rto_general)\n",
    "\n",
    "        # Append global token\n",
    "        global_token = self.global_token.repeat(embedding.shape[0], 1, 1)\n",
    "        global_token += pos_enc[1]\n",
    "        embedding = torch.cat([global_token, embedding], dim=1)\n",
    "        \n",
    "        return embedding, remain_idx, revert_idx, masked_idx, img_h, img_w\n",
    "\n",
    "class TemporalOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        if col in num_scaler:\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, 1)\n",
    "                            )\n",
    "        elif col in embedding_cols:\n",
    "            num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "            self.output = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, d_model),\n",
    "                            torch.nn.Linear(d_model, num_cls)\n",
    "                            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.output(data)\n",
    "\n",
    "class StaticOutput(torch.nn.Module):\n",
    "    def __init__(self, col, d_model):\n",
    "        super().__init__()\n",
    "        num_cls = train_dataset.label_encoder_dict[col].idx\n",
    "        self.output = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(d_model, d_model), \n",
    "                        torch.nn.Linear(d_model, d_model), \n",
    "                        torch.nn.Linear(d_model, num_cls))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.output(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_layers, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        activation = \"gelu\"\n",
    "       # Modality embedding\n",
    "        self.modality_embedding = torch.nn.Embedding(num_modality, d_model)\n",
    "\n",
    "       # Temporal embedding\n",
    "        temporal_embedding_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            temporal_embedding_li.append(TemporalEmbedding(col, d_model))\n",
    "        self.temporal_embedding_li = torch.nn.ModuleList(temporal_embedding_li)\n",
    "\n",
    "       # Encoding\n",
    "        self.encoding = Encoder(EncoderLayer(d_model, nhead, d_ff, dropout, activation, batch_first=True), num_layers)\n",
    "\n",
    "       # Revert\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, d_model))\n",
    "\n",
    "       # Positional encoding\n",
    "        pos_enc_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            pos_enc_li.append(torch.nn.Parameter(get_positional_encoding()))\n",
    "        self.pos_enc_li = torch.nn.ParameterList(pos_enc_li)\n",
    "\n",
    "        # Decoding\n",
    "        self.decoding = Encoder(EncoderLayer(d_model, nhead, d_ff, dropout, activation, batch_first=True), num_layers)\n",
    "\n",
    "        # Output\n",
    "        temporal_output_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            temporal_output_li.append(TemporalOutput(col, d_model))\n",
    "        self.temporal_output_li = torch.nn.ModuleList(temporal_output_li)\n",
    "    \n",
    "    def get_embedding(self, data_input):\n",
    "       # Embedding\n",
    "        modality_idx = 0\n",
    "       ### Temporal embedding\n",
    "        temporal_embedding_dict = {}\n",
    "        for col, temporal_embedding in zip(data_info[\"target\"] + data_info[\"temporal\"], self.temporal_embedding_li):\n",
    "            data = data_input[col].to(device)\n",
    "            remain_idx = data_input[f\"{col}_remain_idx\"].to(device)\n",
    "            temporal_embedding_dict[col] = temporal_embedding(data, remain_idx) # Embedding\n",
    "            temporal_embedding_dict[col] += self.modality_embedding(torch.tensor([modality_idx]).to(device)) # Modality embedding\n",
    "            modality_idx += 1\n",
    "        \n",
    "        return temporal_embedding_dict\n",
    "        \n",
    "    def get_padding_mask(self, data_input, mode):\n",
    "        temporal_padding_mask_li = []\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            mask = data_input[f\"{col}_{mode}_padding_mask\"]\n",
    "            mask = torch.cat([torch.ones(mask.shape[0], 1), mask], dim=-1) # Add sequence for cls token\n",
    "            temporal_padding_mask_li.append(mask)\n",
    "        temporal_padding_mask = torch.cat(temporal_padding_mask_li, dim=-1).to(device)\n",
    "\n",
    "        ### Concatenate\n",
    "        padding_mask = temporal_padding_mask\n",
    "        padding_mask = torch.where(padding_mask == 1, 0, -torch.inf)\n",
    "        return padding_mask\n",
    "\n",
    "    def split_each(self, data, last_temporal_embedding_dict):\n",
    "        start_idx = 0\n",
    "        temporal_dict = {}\n",
    "        # Temporal\n",
    "        for col in data_info[\"target\"] + data_info[\"temporal\"]:\n",
    "            # Select specific feature\n",
    "            length = last_temporal_embedding_dict[col].shape[1]\n",
    "            selected_embedding = data[..., start_idx:start_idx+length, :]\n",
    "            temporal_dict[col] = selected_embedding\n",
    "            start_idx += length\n",
    "\n",
    "        return temporal_dict\n",
    "\n",
    "    def revert_embedding(self, temporal_encoding_dict):\n",
    "        reverted_temporal_dict = {}\n",
    "        modality_idx = 0\n",
    "        ### Revert temporal\n",
    "        for col, pos_enc in zip(data_info[\"target\"] + data_info[\"temporal\"], self.pos_enc_li):\n",
    "            # Process for revert index\n",
    "            reverted_embedding = self.apply_revert(embedding = temporal_encoding_dict[col],\n",
    "                                                    revert_idx = data_input[f\"{col}_revert_idx\"].to(device),\n",
    "                                                    mask_token = self.mask_token,\n",
    "                                                    pos_enc = pos_enc)\n",
    "            reverted_temporal_dict[col] = reverted_embedding + self.modality_embedding(torch.tensor([modality_idx]).to(device)) # Modality embedding\n",
    "            modality_idx += 1\n",
    "    \n",
    "        return reverted_temporal_dict\n",
    "\n",
    "    def forward(self, data_iput):\n",
    "        # Embedding\n",
    "        temporal_embedding_dict = self.get_embedding(data_input)\n",
    "        encoder_padding_mask = self.get_padding_mask(data_iput, mode=\"remain\")\n",
    "\n",
    "        # Encoding\n",
    "        encoder_input = torch.cat(list(temporal_embedding_dict.values()), dim=1)\n",
    "        encoding, encoding_weight = self.encoding(encoder_input, src_key_padding_mask=encoder_padding_mask)\n",
    "\n",
    "        # Split\n",
    "        temporal_encoding_dict = self.split_each(encoding, temporal_embedding_dict)\n",
    "\n",
    "        # Revert\n",
    "        reverted_temporal_dict = self.revert_embedding(temporal_encoding_dict)\n",
    "        decoder_padding_mask = self.get_padding_mask(data_iput, mode=\"revert\")\n",
    "\n",
    "        # Decoding\n",
    "        decoder_input = torch.cat(list(reverted_temporal_dict.values()), dim=1)\n",
    "        decoding, decoding_weight = self.decoding(decoder_input, src_key_padding_mask=decoder_padding_mask)\n",
    "\n",
    "        # Split\n",
    "        temporal_decoding_dict = self.split_each(decoding, reverted_temporal_dict)\n",
    "        temporal_decodingweight_dict = self.split_each(decoding_weight, reverted_temporal_dict)\n",
    "\n",
    "        # Output\n",
    "        ### Temporal output\n",
    "        temporal_output_dict = {}\n",
    "        for col, temporal_output in zip(data_info[\"target\"] + data_info[\"temporal\"], self.temporal_output_li):\n",
    "            data = temporal_decoding_dict[col]\n",
    "            temporal_output_dict[col] = temporal_output(data)\n",
    "\n",
    "        return temporal_output_dict, temporal_decodingweight_dict\n",
    "\n",
    "    def apply_revert(self, embedding, revert_idx, mask_token, **kwargs):\n",
    "        # Process for revert index\n",
    "        revert_idx += 1 # Shift one step for cls token\n",
    "        revert_idx = torch.cat([torch.zeros(revert_idx.shape[0], 1).to(device).to(torch.int),\n",
    "                                revert_idx], dim=1) # Append index for cls token, which should be reserved\n",
    "        revert_idx = revert_idx.unsqueeze(-1).repeat(1, 1, embedding.shape[-1])\n",
    "\n",
    "        # Append mask tokens for missing positions\n",
    "        mask_tokens = mask_token.unsqueeze(0).repeat(revert_idx.shape[0],\n",
    "                                                    revert_idx.shape[1]-embedding.shape[1],\\\n",
    "                                                    1)\n",
    "        full_embedding = torch.cat([embedding, mask_tokens], dim=1)\n",
    "\n",
    "        # Revert and re-positional encoding\n",
    "        reverted_embedding = torch.gather(full_embedding, index=revert_idx, dim=1)\n",
    "        \n",
    "        if \"pos_enc\" in kwargs.keys():\n",
    "            pos_enc = kwargs[\"pos_enc\"][:reverted_embedding.shape[1]]\n",
    "            reverted_embedding += pos_enc\n",
    "        \n",
    "        elif \"pos_emb\" in kwargs.keys():\n",
    "            pos_emb = kwargs[\"pos_emb\"](torch.tensor(range(reverted_embedding.shape[1])).to(device))\n",
    "            reverted_embedding += pos_emb\n",
    "\n",
    "        return reverted_embedding\n",
    "\n",
    "model = Transformer(d_model, num_layers, nhead, d_ff, dropout)\n",
    "model.to(device)\n",
    "data_input = {key:val for key, val in data.items() if key != \"img_raw\" and \"scaler\" not in key}\n",
    "# summary(model,\n",
    "#         data_input,\n",
    "#         show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(imgs):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs\n",
    "\n",
    "# res = patchify(data[\"img_input\"])\n",
    "# print(res.shape)\n",
    "# unpatchify(res).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 366, 2196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1600/1678 [05:21<00:15,  4.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[13], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     77\u001b[0m self_weight \u001b[38;5;241m=\u001b[39m temporal_decodingweight_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m366\u001b[39m][idx]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     78\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(self_weight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m---> 80\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/backend_bases.py:2342\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[1;32m   2337\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[1;32m   2338\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2339\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[1;32m   2340\u001b[0m     )\n\u001b[1;32m   2341\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2342\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1389\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m renderer\u001b[38;5;241m.\u001b[39mopen_group(\u001b[38;5;18m__name__\u001b[39m, gid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_gid())\n\u001b[1;32m   1388\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 1389\u001b[0m tlb1, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks_to_draw:\n\u001b[1;32m   1392\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1316\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/axis.py:1316\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/text.py:959\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m--> 959\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_renderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[1;32m    961\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_transform()\u001b[38;5;241m.\u001b[39mtransform((x, y))\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/text.py:378\u001b[0m, in \u001b[0;36mText._get_layout\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    375\u001b[0m ys \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Full vertical extent of font, including ascenders and descenders:\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m _, lp_h, lp_d \u001b[38;5;241m=\u001b[39m \u001b[43m_get_text_metrics_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTeX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_usetex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m min_dy \u001b[38;5;241m=\u001b[39m (lp_h \u001b[38;5;241m-\u001b[39m lp_d) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_linespacing\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines):\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/text.py:97\u001b[0m, in \u001b[0;36m_get_text_metrics_with_cache\u001b[0;34m(renderer, text, fontprop, ismath, dpi)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call ``renderer.get_text_width_height_descent``, caching the results.\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Cached based on a copy of fontprop so that later in-place mutations of\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# the passed-in argument do not mess up the cache.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_text_metrics_with_cache_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mismath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/font_manager.py:645\u001b[0m, in \u001b[0;36mFontProperties.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/matplotlib/font_manager.py:634\u001b[0m, in \u001b[0;36mFontProperties.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     l \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_family\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    635\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_slant(),\n\u001b[1;32m    636\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_variant(),\n\u001b[1;32m    637\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_weight(),\n\u001b[1;32m    638\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stretch(),\n\u001b[1;32m    639\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_size(),\n\u001b[1;32m    640\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_file(),\n\u001b[1;32m    641\u001b[0m          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_math_fontfamily())\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(l)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def num_temporal_loss_fn(pred, y, mask, padding_mask):\n",
    "    y = torch.gather(y, index=mask, dim=1)\n",
    "    pred = torch.gather(pred, index=mask, dim=1)\n",
    "    loss = mse_loss(pred, y)\n",
    "    loss = torch.where(padding_mask==1, loss, 0)\n",
    "    loss = loss.sum() / padding_mask.sum()\n",
    "    return loss\n",
    "\n",
    "def cat_temporal_loss_fn(pred, y, mask, padding_mask):\n",
    "    y = torch.gather(y, index=mask, dim=1).to(torch.long)\n",
    "\n",
    "    pred_mask = mask.unsqueeze(-1).repeat(1, 1, pred.shape[-1])\n",
    "    pred = torch.gather(pred, index=pred_mask, dim=1)\n",
    "    loss = ce_loss(pred.view(-1, pred.shape[-1]), y.view(-1)).view(y.shape)\n",
    "    loss = torch.where(padding_mask==1, loss, 0)\n",
    "    loss = loss.sum() / padding_mask.sum()\n",
    "    return loss\n",
    "\n",
    "def train(e):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_li, sales_loss_li, dow_loss_li, month_loss_li, holiday_loss_li, price_loss_li, window = [], [], [], [], [], [], 100\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        temporal_output_dict, temporal_decodingweight_dict = model(data)\n",
    "    \n",
    "        sales_loss = num_temporal_loss_fn(temporal_output_dict[\"sales\"][:, 1:], data[\"sales\"].to(device), data[\"sales_masked_idx\"].unsqueeze(-1).to(device), data[\"sales_masked_padding_mask\"].unsqueeze(-1).to(device))\n",
    "        dow_loss = cat_temporal_loss_fn(temporal_output_dict[\"dow\"][:, 1:], data[\"dow\"].to(device), data[\"dow_masked_idx\"].to(device), data[\"dow_masked_padding_mask\"].to(device))\n",
    "        # month_loss = cat_temporal_loss_fn(temporal_output_dict[\"month\"][:, 1:], data[\"month\"].to(device), data[\"month_masked_idx\"].to(device), data[\"month_masked_padding_mask\"].to(device))\n",
    "        # holiday_loss = cat_temporal_loss_fn(temporal_output_dict[\"holiday\"][:, 1:], data[\"holiday\"].to(device), data[\"holiday_masked_idx\"].to(device), data[\"holiday_masked_padding_mask\"].to(device))\n",
    "        # price_loss = num_temporal_loss_fn(temporal_output_dict[\"price\"][:, 1:], data[\"price\"].to(device), data[\"price_masked_idx\"].unsqueeze(-1).to(device), data[\"price_masked_padding_mask\"].unsqueeze(-1).to(device))\n",
    "\n",
    "        # loss = sales_loss + dow_loss + month_loss + holiday_loss + price_loss\n",
    "        loss = sales_loss + dow_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_li.append(loss.item())\n",
    "        sales_loss_li.append(sales_loss.item())\n",
    "        dow_loss_li.append(dow_loss.item())\n",
    "        # month_loss_li.append(month_loss.item())\n",
    "        # holiday_loss_li.append(holiday_loss.item())\n",
    "        # price_loss_li.append(price_loss.item())\n",
    "        # pbar.set_description(f\"{e} - sales_loss: {np.mean(sales_loss_li[-window:])}, \\\n",
    "# dow_loss: {np.mean(dow_loss_li[-window:])}, \\\n",
    "# month_loss: {np.mean(month_loss_li[-window:])}, \\\n",
    "# holiday_loss: {np.mean(holiday_loss_li[-window:])}, \\\n",
    "# price_loss: {np.mean(price_loss_li[-window:])}, \\\n",
    "# lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        if n %20 == 0:\n",
    "            idx = 0\n",
    "            plt.figure(figsize=(25,12))\n",
    "            nrows, ncols = 5, 3\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Loss\n",
    "            plt.subplot(nrows, ncols, 1)\n",
    "            plt.plot(loss_li[-window:])\n",
    "\n",
    "            # Sample\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            plt.plot(data[\"sales\"][idx], label=\"y\")\n",
    "            plt.plot(temporal_output_dict[\"sales\"][idx].detach().cpu(), label=\"pred\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Attn\n",
    "            plt.subplot(nrows, ncols, 3)\n",
    "            print(temporal_decodingweight_dict[\"sales\"].shape)\n",
    "            self_weight = temporal_decodingweight_dict[\"sales\"][..., 1:366][idx].mean(dim=0)\n",
    "            sns.heatmap(self_weight.detach().cpu())\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "epoch = 5\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6711 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6711 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sales_idx_keep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m         sales_output, day_output, dow_output, month_output, holiday_output, price_output, index_output, color_output, graphic_output, prod_output, img_output,\\\n\u001b[1;32m      7\u001b[0m             temp_keep_mask, static_idx_keep, img_idx_keep\\\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;241m=\u001b[39m model(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      9\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     10\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraphic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),  data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     11\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m---> 12\u001b[0m                 \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales_idx_keep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     13\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     14\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdow_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     15\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     16\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholiday_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     17\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_idx_keep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_revert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     18\u001b[0m                 data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_keep_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_revert_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m             idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sales_idx_keep'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pbar = tqdm(valid_dataloader)\n",
    "\n",
    "for n, data in enumerate(pbar):\n",
    "    with torch.no_grad():\n",
    "        sales_output, day_output, dow_output, month_output, holiday_output, price_output, index_output, color_output, graphic_output, prod_output, img_output,\\\n",
    "            temp_keep_mask, static_idx_keep, img_idx_keep\\\n",
    "            = model(data[\"sales\"].to(device),\n",
    "                data[\"day\"].to(device), data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "                data[\"index\"].to(device),  data[\"color\"].to(device),  data[\"graphic\"].to(device),  data[\"prod\"].to(device),\n",
    "                data[\"img_input\"].to(device),\n",
    "                data[\"sales_idx_keep\"].to(device), data[\"sales_revert\"].to(device),\n",
    "                data[\"day_idx_keep\"].to(device), data[\"day_revert\"].to(device),\n",
    "                data[\"dow_idx_keep\"].to(device), data[\"dow_revert\"].to(device),\n",
    "                data[\"month_idx_keep\"].to(device), data[\"month_revert\"].to(device),\n",
    "                data[\"holiday_idx_keep\"].to(device), data[\"holiday_revert\"].to(device),\n",
    "                data[\"price_idx_keep\"].to(device), data[\"price_revert\"].to(device),\n",
    "                data[\"temp_keep_padding_mask\"].to(device), data[\"temp_revert_padding_mask\"].to(device))\n",
    "\n",
    "        if n %20 == 0:\n",
    "            idx = 0\n",
    "            plt.figure(figsize=(25,12))\n",
    "            nrows, ncols = 5, 3\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Sample\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            valid_len = data[\"temp_revert_padding_mask\"].max(dim=-1).indices[idx]\n",
    "            valid_len = data[\"sales\"].shape[-1] if valid_len == 0 else valid_len\n",
    "            plt.plot(data[\"sales\"][idx][:valid_len], label=\"y\")\n",
    "            plt.plot(sales_output[idx][1:valid_len].detach().cpu(), label=\"pred\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Img\n",
    "            plt.subplot(nrows, ncols, 4)\n",
    "            img_input = data[\"img_input\"]\n",
    "            img_input = img_input.permute(0,2,3,1)\n",
    "            img_input = img_input.cpu().detach().numpy()\n",
    "            plt.imshow(img_input[idx])\n",
    "\n",
    "            plt.subplot(nrows, ncols, 5)\n",
    "            img_output = unpatchify(img_output[:, 1:]).permute(0,2,3,1)\n",
    "            img_output = img_output.cpu().detach().numpy()\n",
    "            plt.imshow(img_output[idx])\n",
    "\n",
    "            index_output = torch.argmax(index_output, dim=-1).detach().cpu().numpy()\n",
    "            index_output = train_dataset.get_encoder_dict()[\"index_encoder\"].inverse_transform(index_output[idx])\n",
    "            index_y = train_dataset.get_encoder_dict()[\"index_encoder\"].inverse_transform(data[\"index\"][idx].numpy()[0])\n",
    "            print(f\"y: {index_y}, pred: {index_output}\")\n",
    "\n",
    "            color_output = torch.argmax(color_output, dim=-1).detach().cpu().numpy()\n",
    "            color_output = train_dataset.get_encoder_dict()[\"color_encoder\"].inverse_transform(color_output[idx])\n",
    "            color_y = train_dataset.get_encoder_dict()[\"color_encoder\"].inverse_transform(data[\"color\"][idx].numpy()[0])\n",
    "            print(f\"y: {color_y}, pred: {color_output}\")\n",
    "\n",
    "            graphic_output = torch.argmax(graphic_output, dim=-1).detach().cpu().numpy()\n",
    "            graphic_output = train_dataset.get_encoder_dict()[\"graphic_encoder\"].inverse_transform(graphic_output[idx])\n",
    "            graphic_y = train_dataset.get_encoder_dict()[\"graphic_encoder\"].inverse_transform(data[\"graphic\"][idx].numpy()[0])\n",
    "            print(f\"y: {graphic_y}, pred: {graphic_output}\")\n",
    "\n",
    "            prod_output = torch.argmax(prod_output, dim=-1).detach().cpu().numpy()\n",
    "            prod_output = train_dataset.get_encoder_dict()[\"prod_encoder\"].inverse_transform(prod_output[idx])\n",
    "            prod_y = train_dataset.get_encoder_dict()[\"prod_encoder\"].inverse_transform(data[\"prod\"][idx].numpy()[0])\n",
    "            print(f\"y: {prod_y}, pred: {prod_output}\")\n",
    "\n",
    "            plt.show()\n",
    "            input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
