{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1701 x 206'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal = 5\n",
    "seq = 300\n",
    "static = 5 + 14*14\n",
    "\n",
    "f\"{(temporal*seq)+static} x {temporal+static}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_skip = True\n",
    "\n",
    "# Data params\n",
    "batch_size = 16\n",
    "# batch_size = 16\n",
    "pred_len = 30\n",
    "\n",
    "# Model params\n",
    "d_model = 128\n",
    "nhead = 4\n",
    "d_ff = 256\n",
    "dropout = 0.1\n",
    "num_layers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "\n",
    "import torch\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from transformers import ViTImageProcessor, MobileViTModel, MobileViTConfig, ViTModel, AutoImageProcessor, ViTConfig, BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "from skimage import io\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_skip:\n",
    "    us_holiday = holidays.US()\n",
    "    \n",
    "    # Read transaction\n",
    "    df_trans = pd.read_csv(\"../HnM/transactions_train.csv\", parse_dates=[\"t_dat\"], dtype={\"article_id\":str})\n",
    "    df_meta = pd.read_csv(\"../HnM/articles.csv\", dtype={\"article_id\":str})\n",
    "\n",
    "    min_year = df_trans[\"t_dat\"].dt.year.min()\n",
    "    max_year = df_trans[\"t_dat\"].dt.year.max()\n",
    "\n",
    "    holiday = holidays.US(years=(min_year, max_year))\n",
    "    holiday = pd.DataFrame({\"t_dat\":holiday.keys(), \"holiday\":holiday.values()})\n",
    "    holiday[\"t_dat\"] = pd.to_datetime(holiday[\"t_dat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = data.copy()\n",
    "\n",
    "    # Image path\n",
    "    data[\"img_path\"] = data[\"article_id\"].apply(lambda x: f'../HnM/resized_images/{x[:3]}/{x}.jpg')\n",
    "    data[\"is_valid\"] = data[\"img_path\"].apply(lambda x: 1 if os.path.isfile(x) else 0) # Check whether the article has corresponding image file\n",
    "    data = data[data[\"is_valid\"] == 1].drop(\"is_valid\", axis=1) # Valid if having corresponding image\n",
    "\n",
    "    # Make sales\n",
    "    data = data.groupby([\"t_dat\", \"article_id\", \"img_path\"], as_index=False).agg(sales=(\"customer_id\", \"count\"), price=(\"price\", \"mean\"))\n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data = data[(data[\"size\"]>=10)]\n",
    "\n",
    "    # Expand dates\n",
    "    data = data.set_index(\"t_dat\").groupby([\"article_id\"], as_index=False).resample(\"1D\").asfreq().reset_index()\n",
    "    data[\"sales\"] = data[\"sales\"].fillna(0)\n",
    "    data[\"price\"] = data[\"price\"].fillna(method=\"ffill\")\n",
    "    data[\"article_id\"] = data[\"article_id\"].fillna(method=\"ffill\")\n",
    "    data[\"img_path\"] = data[\"img_path\"].fillna(method=\"ffill\")\n",
    "    data = data.sort_values([\"article_id\", \"t_dat\"])\n",
    "    \n",
    "    data[\"size\"] = data.groupby([\"article_id\"], as_index=False)[\"sales\"].transform(\"count\")\n",
    "    data = data[(data[\"size\"]>=pred_len*2)]\n",
    "    data[\"time_idx\"] = data.groupby(\"article_id\").cumcount()\n",
    "    data[\"time_idx\"] = data[\"size\"] - data[\"time_idx\"]\n",
    "\n",
    "    # Make holidays\n",
    "    data = pd.merge(data, holiday, on=\"t_dat\", how=\"left\")\n",
    "    display(data)\n",
    "\n",
    "    # Temporal information\n",
    "    # Make sure the sequence start from 0\n",
    "    data[\"day\"] = data[\"t_dat\"].dt.day - 1\n",
    "    data[\"dow\"] = data[\"t_dat\"].dt.dayofweek\n",
    "    data[\"month\"] = data[\"t_dat\"].dt.month - 1\n",
    "    data[\"year\"] = data[\"t_dat\"].dt.year / (data[\"t_dat\"].dt.year.max() + 1)\n",
    "\n",
    "    # Append meta data\n",
    "    data = data.merge(df_meta[[\"index_name\", \"article_id\", \"colour_group_name\", \"graphical_appearance_name\", \"product_type_name\"]], on=\"article_id\")\n",
    "\n",
    "    # Split\n",
    "    df_train = data[data[\"time_idx\"] > 30].copy()\n",
    "    df_train[\"time_idx\"] = df_train[\"time_idx\"] - 30\n",
    "    df_valid = data.copy()\n",
    "\n",
    "    # Output\n",
    "    return df_train.reset_index(drop=True), df_valid.reset_index(drop=True)\n",
    "\n",
    "if not is_skip:\n",
    "    df_prep = df_trans.copy()#.iloc[:1000]\n",
    "    df_train, df_valid = preprocess(df_prep)\n",
    "    \n",
    "    df_train.to_parquet(\"df_train.pq\")\n",
    "    df_valid.to_parquet(\"df_valid.pq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return np.log1p(x)\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        return np.expm1(x)\n",
    "    \n",
    "class NoneScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x\n",
    "    \n",
    "    def inverse_transform(self, x, y=None):\n",
    "        x = x.copy()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.mapper = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        x = set(x)\n",
    "        for val in x:\n",
    "            if val not in self.mapper.keys():\n",
    "                self.mapper[val] = self.idx\n",
    "                self.idx += 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        res = []\n",
    "        for val in x:\n",
    "            if val in self.mapper.keys():\n",
    "                res.append(self.mapper[val])\n",
    "            else:\n",
    "                res.append(self.idx)\n",
    "        return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21093/21093 [00:00<00:00, 27285.11it/s]\n",
      "100%|██████████| 21093/21093 [00:00<00:00, 26840.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales: torch.Size([16, 606])\n",
      "price: torch.Size([16, 606])\n",
      "dow: torch.Size([16, 606])\n",
      "index_name: torch.Size([16, 1])\n",
      "img_input torch.Size([16, 3, 224, 224])\n",
      "y: torch.Size([16, 606])\n",
      "length: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_parquet(\"df_train.pq\")\n",
    "df_valid = pd.read_parquet(\"df_valid.pq\")\n",
    "\n",
    "df_train = df_train[df_train[\"index_name\"] == \"Ladieswear\"]\n",
    "df_valid = df_valid[df_valid[\"index_name\"] == \"Ladieswear\"]\n",
    "\n",
    "fake_val = -99\n",
    "fake_mask_val = 0\n",
    "real_mask_val = 1\n",
    "padding_mask_val = 2\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, is_train=True, encoder_dict=None):\n",
    "        # Define parameters\n",
    "        self.is_train = is_train\n",
    "        self.transform = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        if is_train:\n",
    "            self.holiday_encoder = CustomLabelEncoder()\n",
    "            self.index_encoder = CustomLabelEncoder()\n",
    "            self.color_encoder = CustomLabelEncoder()\n",
    "            self.graphic_encoder = CustomLabelEncoder()\n",
    "            self.product_encoder = CustomLabelEncoder()\n",
    "\n",
    "            self.holiday_encoder.fit(data[\"holiday\"])\n",
    "            self.index_encoder.fit(data[\"index_name\"])\n",
    "            self.color_encoder.fit(data[\"colour_group_name\"])\n",
    "            self.graphic_encoder.fit(data[\"graphical_appearance_name\"])\n",
    "            self.product_encoder.fit(data[\"product_type_name\"])\n",
    "        else: \n",
    "            self.holiday_encoder = encoder_dict[\"holiday_encoder\"]\n",
    "            self.index_encoder = encoder_dict[\"index_encoder\"]\n",
    "            self.color_encoder = encoder_dict[\"color_encoder\"]\n",
    "            self.graphic_encoder = encoder_dict[\"graphic_encoder\"]\n",
    "            self.product_encoder = encoder_dict[\"product_encoder\"]\n",
    "            \n",
    "        # Iterate each product\n",
    "        self.data_li = []\n",
    "        self.min_val, self.max_val = data[\"sales\"].min(), data[\"sales\"].max()\n",
    "        data.groupby(\"article_id\").progress_apply(lambda x: self.data_li.append(x))\n",
    "    \n",
    "    def get_encoder_dict(self):\n",
    "        return {\"holiday_encoder\": self.holiday_encoder,\n",
    "                \"index_encoder\": self.index_encoder,\n",
    "                \"color_encoder\": self.color_encoder,\n",
    "                \"graphic_encoder\": self.graphic_encoder,\n",
    "                \"product_encoder\": self.product_encoder}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_li)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Obtain information about data for each\n",
    "        data = self.data_li[idx]\n",
    "        \n",
    "        # Label encode\n",
    "        data[\"holiday\"] = self.holiday_encoder.transform(data[\"holiday\"])\n",
    "        data[\"index_name\"] = self.index_encoder.transform(data[\"index_name\"])\n",
    "        data[\"colour_group_name\"] = self.color_encoder.transform(data[\"colour_group_name\"])\n",
    "        data[\"graphical_appearance_name\"] = self.graphic_encoder.transform(data[\"graphical_appearance_name\"])\n",
    "        data[\"product_type_name\"] = self.product_encoder.transform(data[\"product_type_name\"])\n",
    "\n",
    "        # Name each and scale data\n",
    "        sales_scaler, price_scaler = LogScaler(), NoneScaler()\n",
    "        ## Sales information\n",
    "        sales = sales_scaler.fit_transform(data[\"sales\"].values.reshape(-1,1)).reshape(-1)\n",
    "        y = sales_scaler.transform(data[\"sales\"].values.reshape(-1,1)).reshape(-1)\n",
    "        ## Temporal information\n",
    "        dow = data[\"dow\"].values\n",
    "        month = data[\"month\"].values\n",
    "        holiday = data[\"holiday\"].values\n",
    "        price = price_scaler.fit_transform(data[\"price\"].values.reshape(-1,1)).reshape(-1)\n",
    "        ## Static information\n",
    "        index_name = data[\"index_name\"].values[[0]]\n",
    "        colour_group_name = data[\"colour_group_name\"].values[[0]]\n",
    "        graphical_appearance_name = data[\"graphical_appearance_name\"].values[[0]]\n",
    "        product_type_name = data[\"product_type_name\"].values[[0]]\n",
    "        img_path = data[\"img_path\"].values[0]\n",
    "        img_raw = Image.open(img_path).convert(\"RGB\")\n",
    "        img_input = self.transform(img_raw, return_tensors=\"pt\", train=False)[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Generate mask\n",
    "        if self.is_train:\n",
    "            # When training, mask random time steps\n",
    "            data, mask = self.generate_mask(sales)\n",
    "        else:\n",
    "            sales, start_idx, end_idx, mask = self.generate_valid_mask(sales)\n",
    "            dow = dow[start_idx:end_idx]\n",
    "            month = month[start_idx:end_idx]\n",
    "            holiday = holiday[start_idx:end_idx]\n",
    "            price = price[start_idx:end_idx]\n",
    "            y = y[start_idx:end_idx]\n",
    "        \n",
    "        return {\"sales\": torch.Tensor(sales), \n",
    "                \"dow\":torch.IntTensor(dow), \"month\":torch.IntTensor(month), \"holiday\":torch.IntTensor(holiday), \"price\":torch.Tensor(price),\n",
    "                \"index_name\": torch.IntTensor(index_name), \"colour_group_name\": torch.IntTensor(colour_group_name), \"graphical_appearance_name\": torch.IntTensor(graphical_appearance_name), \"product_type_name\": torch.IntTensor(product_type_name), \n",
    "                \"img_raw\":img_raw, \"img_input\": img_input,\n",
    "                \"length\": len(y), \"mask\": torch.IntTensor(mask),\n",
    "                \"y\":torch.Tensor(y)}\n",
    "\n",
    "    def generate_mask(self, data):\n",
    "        index = np.arange(len(data)) # Index for each time step\n",
    "        num_masks = int(len(data) * 0.8) # Amont of masked values------------------------------------\n",
    "        candidate_idx = np.random.choice(index, num_masks, replace=False) # Indexes to be masked\n",
    "        assert num_masks == len(candidate_idx), \"Not enough masks\"\n",
    "        \n",
    "        # Generate mask: 0 for valid, 1 for mask(, replacement, real), 2 for padding\n",
    "        mask = []\n",
    "        for i in index:\n",
    "            if i in candidate_idx:\n",
    "                data[i] = fake_val\n",
    "                mask.append(fake_mask_val)\n",
    "            else:\n",
    "                mask.append(real_mask_val)\n",
    "        assert len(mask) == len(data)\n",
    "        return data, mask\n",
    "\n",
    "    def generate_valid_mask(self, data):\n",
    "        start_idx = 0\n",
    "        # end_idx = data.shape[0]\n",
    "        end_idx = 90\n",
    "\n",
    "        data = data[start_idx:end_idx]\n",
    "        mask = np.zeros(len(data)) + 1\n",
    "        data[-30:] = fake_val # Mask last 30 time steps\n",
    "        \n",
    "        mask[-30:] = 0\n",
    "\n",
    "        assert len(mask) == len(data)\n",
    "\n",
    "        return data, start_idx, end_idx, mask\n",
    "\n",
    "def collate_fn(batch_li):\n",
    "    # Get data\n",
    "    sales = [batch[\"sales\"] for batch in batch_li]\n",
    "    dow = [batch[\"dow\"] for batch in batch_li]\n",
    "    month = [batch[\"month\"] for batch in batch_li]\n",
    "    holiday = [batch[\"holiday\"] for batch in batch_li]\n",
    "    index_name = [batch[\"index_name\"] for batch in batch_li]\n",
    "    colour_group_name = [batch[\"colour_group_name\"] for batch in batch_li]\n",
    "    graphical_appearance_name = [batch[\"graphical_appearance_name\"] for batch in batch_li]\n",
    "    product_type_name = [batch[\"product_type_name\"] for batch in batch_li]\n",
    "    price = [batch[\"price\"] for batch in batch_li]\n",
    "    length = [batch[\"length\"] for batch in batch_li]\n",
    "    mask = [batch[\"mask\"] for batch in batch_li]\n",
    "    \n",
    "    img_raw = [batch[\"img_raw\"] for batch in batch_li]\n",
    "    img_input = torch.stack([batch[\"img_input\"] for batch in batch_li])\n",
    "\n",
    "    y = [batch[\"y\"] for batch in batch_li]\n",
    "\n",
    "    # Pad data\n",
    "    # We determine whether each position is valid or mask or padding with only `mask`.\n",
    "    # Therefore, padding value for others are irrelavent but matters only for `mask`\n",
    "    sales = torch.nn.utils.rnn.pad_sequence(sales, batch_first=True)\n",
    "    dow = torch.nn.utils.rnn.pad_sequence(dow, batch_first=True)\n",
    "    month = torch.nn.utils.rnn.pad_sequence(month, batch_first=True)\n",
    "    holiday = torch.nn.utils.rnn.pad_sequence(holiday, batch_first=True)\n",
    "    price = torch.nn.utils.rnn.pad_sequence(price, batch_first=True)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence(mask, batch_first=True, padding_value=padding_mask_val)\n",
    "    y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "    \n",
    "    return {\"sales\": sales, \n",
    "            \"dow\":dow, \"month\":month, \"holiday\":holiday, \"price\":price, \n",
    "            \"index_name\": torch.stack(index_name), \"colour_group_name\": torch.stack(colour_group_name), \"graphical_appearance_name\": torch.stack(graphical_appearance_name), \"product_type_name\": torch.stack(product_type_name),\n",
    "            \"length\": torch.IntTensor(length), \"mask\":mask,\n",
    "            \"img_raw\": img_raw, \"img_input\":img_input,\n",
    "            \"y\":y}\n",
    "\n",
    "train_dataset = Dataset(df_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=16)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = Dataset(df_valid, is_train=False, encoder_dict=train_dataset.get_encoder_dict())\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True, num_workers=16)\n",
    "# valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for data in train_dataloader:\n",
    "    print(\"sales:\", data[\"sales\"].shape)\n",
    "    print(\"price:\", data[\"price\"].shape)\n",
    "    print(\"dow:\", data[\"dow\"].shape)\n",
    "    print(\"index_name:\", data[\"index_name\"].shape)\n",
    "    print(\"img_input\", data[\"img_input\"].shape)\n",
    "    print(\"y:\", data[\"y\"].shape)\n",
    "    print(\"length:\", data[\"length\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in valid_dataloader:\n",
    "#     print(\"sales:\", data[\"sales\"].shape)\n",
    "#     print(\"price:\", data[\"price\"].shape)\n",
    "#     print(\"mask:\", data[\"mask\"].shape)\n",
    "#     print(\"y:\", data[\"y\"].shape)\n",
    "#     print(\"length:\", data[\"length\"].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    # PE(pos, 2i) = sin(pos/10000^{2i/d_model}), \n",
    "    # PE(pos, 2i+1) = cos(pos/10000^{2i/d_model})\n",
    "    def __init__(self, max_len, d_model, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        position = torch.arange(max_len).reshape(-1,1).to(device)\n",
    "        i = torch.arange(d_model).to(device)//2\n",
    "        exp_term = 2*i/d_model\n",
    "        div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "        self.pos_encoded = position / div_term\n",
    "\n",
    "        self.pos_encoded[:, 0::2] = torch.sin(self.pos_encoded[:, 0::2])\n",
    "        self.pos_encoded[:, 1::2] = torch.cos(self.pos_encoded[:, 1::2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x + self.pos_encoded[:x.shape[1], :]\n",
    "        return self.dropout(output)\n",
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, d_model, d_ff, activation):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.dow_embedder = torch.nn.Embedding(7, d_model)\n",
    "        self.month_embedder = torch.nn.Embedding(12, d_model)\n",
    "        self.holiday_embedder = torch.nn.Embedding(train_dataset.holiday_encoder.idx + 1, d_model)\n",
    "        self.price_linear = torch.nn.Linear(1, d_model)\n",
    "    \n",
    "        self.pos_enc = PositionalEncoding(1000, d_model, dropout)\n",
    "    \n",
    "    def forward(self, dow, month, holiday, price):\n",
    "        dow_linear = self.dow_embedder(dow)\n",
    "        month_linear = self.month_embedder(month)\n",
    "        holiday_linear = self.holiday_embedder(holiday)\n",
    "        price_linear = self.price_linear(price.unsqueeze(-1))\n",
    "\n",
    "        dow_posenc = self.pos_enc(dow_linear)\n",
    "        month_posenc = self.pos_enc(month_linear)\n",
    "        holiday_posenc = self.pos_enc(holiday_linear)\n",
    "        price_posenc = self.pos_enc(price_linear)\n",
    "\n",
    "        temporal = torch.stack([dow_posenc, month_posenc, holiday_posenc, price_posenc], dim=-2)\n",
    "\n",
    "        return temporal\n",
    "\n",
    "class StaticEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.index_embedder = torch.nn.Embedding(train_dataset.index_encoder.idx + 1, d_model)\n",
    "        self.color_embedder = torch.nn.Embedding(train_dataset.color_encoder.idx + 1, d_model)\n",
    "        self.graphic_embedder = torch.nn.Embedding(train_dataset.graphic_encoder.idx + 1, d_model)\n",
    "        self.product_embedder = torch.nn.Embedding(train_dataset.product_encoder.idx + 1, d_model)\n",
    "    \n",
    "    def forward(self, index, color, graph, product):\n",
    "        index_linear = self.index_embedder(index.squeeze())\n",
    "        color_linear = self.color_embedder(color.squeeze())\n",
    "        graphic_linear = self.graphic_embedder(graph.squeeze())\n",
    "        product_linear = self.product_embedder(product.squeeze())\n",
    "\n",
    "        static = torch.stack([index_linear, color_linear, graphic_linear, product_linear], dim=-2)\n",
    "\n",
    "        return static\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = torch.nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            torch.nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.positions = torch.nn.Parameter(torch.randn((img_size // patch_size) **2 , emb_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x\n",
    "    \n",
    "class MultiheadBlockAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model, self.num_heads = d_model, num_heads\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        # Linear transform\n",
    "        query = self.q_linear(query)\n",
    "        key = self.k_linear(key)\n",
    "        value = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = query.shape\n",
    "        query = query.view(batch_size, seq_len, -1, self.num_heads, d_model//nhead).permute(0,3,1,2,4)\n",
    "        key = key.view(batch_size, seq_len, -1, self.num_heads, d_model//nhead).permute(0,3,1,2,4)\n",
    "        value = value.view(batch_size, seq_len, -1, self.num_heads, d_model//nhead).permute(0,3,1,2,4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        dot_prod = query @ key.permute(0,1,2,4,3)\n",
    "        scaled_dot_prod = dot_prod / math.sqrt(d_model//nhead)\n",
    "        attn_weight = torch.nn.functional.softmax(scaled_dot_prod, dim=-1)\n",
    "        attn_output = (attn_weight @ value).permute(0,2,3,1,4).reshape(batch_size, seq_len, -1, d_model)\n",
    "\n",
    "        return self.dropout(attn_output), attn_weight\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.01 GiB (GPU 0; 23.68 GiB total capacity; 18.65 GiB already allocated; 4.62 GiB free; 18.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(d_model, num_layers, nhead, d_ff, dropout)\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msales\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mholiday\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindex_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolour_group_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraphical_appearance_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct_type_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_parent_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# model = torch.nn.DataParallel(model)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_model_summary/model_summary.py:128\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m model_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_training:\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, sales, dow, month, holiday, price, index, color, graph, product, img, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m total_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([temporal_embedding, static_embedding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"total_embedding:\", total_embedding.shape); print(\"_\"*100)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sales\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[10], line 125\u001b[0m, in \u001b[0;36mMultiheadBlockAttention.forward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m    123\u001b[0m dot_prod \u001b[38;5;241m=\u001b[39m query \u001b[38;5;241m@\u001b[39m key\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    124\u001b[0m scaled_dot_prod \u001b[38;5;241m=\u001b[39m dot_prod \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(d_model\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mnhead)\n\u001b[0;32m--> 125\u001b[0m attn_weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_dot_prod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m (attn_weight \u001b[38;5;241m@\u001b[39m value)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, d_model)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output), attn_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.01 GiB (GPU 0; 23.68 GiB total capacity; 18.65 GiB already allocated; 4.62 GiB free; 18.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_layers, nhead, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model)\n",
    "        self.static_embedding = StaticEmbedding(d_model)\n",
    "        self.img_embedding = ImgEmbedding(in_channels=3, patch_size=16, emb_size=d_model, img_size=224)\n",
    "\n",
    "        self.encoder = MultiheadBlockAttention(d_model, nhead, dropout)\n",
    "\n",
    "    def forward(self, sales, dow, month, holiday, price, index, color, graph, product, img, mask):\n",
    "        kind = 4\n",
    "        # Preparation\n",
    "        temporal_embedding = self.temporal_embedding(dow, month, holiday, price)\n",
    "        static_embedding = self.static_embedding(index, color, graph, product)\n",
    "        img_embedding = self.img_embedding(img)\n",
    "\n",
    "        # print(\"temporal_embedding:\", temporal_embedding.shape)\n",
    "        # print(\"static_embedding:\", static_embedding.shape)\n",
    "        # print(\"img_embedding:\", img_embedding.shape); print(\"_\"*100)\n",
    "\n",
    "        static_embedding = torch.concat([static_embedding, img_embedding], dim=1).unsqueeze(1).repeat(1, temporal_embedding.shape[1], 1, 1)\n",
    "        # print(\"static_embedding:\", static_embedding.shape); print(\"_\"*100)\n",
    "\n",
    "        total_embedding = torch.concat([temporal_embedding, static_embedding], dim=2)\n",
    "        # print(\"total_embedding:\", total_embedding.shape); print(\"_\"*100)\n",
    "\n",
    "        encoder = self.encoder(total_embedding, total_embedding, total_embedding)\n",
    "\n",
    "        return sales\n",
    "    \n",
    "\n",
    "model = Transformer(d_model, num_layers, nhead, d_ff, dropout)\n",
    "model.to(device)\n",
    "summary(model,\n",
    "        data[\"sales\"].to(device), \n",
    "        data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "        data[\"index_name\"].to(device),  data[\"colour_group_name\"].to(device),  data[\"graphical_appearance_name\"].to(device),  data[\"product_type_name\"].to(device),\n",
    "        data[\"img_input\"].to(device),\n",
    "        data[\"mask\"].to(device),\n",
    "        show_parent_layers=True, print_summary=True)\n",
    "\n",
    "# model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "l1_loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "def loss_num_fn(pred, y, mask):\n",
    "    # mask = torch.where(mask==1, 1, 0)\n",
    "    # mask = torch.where(((mask==fake_mask_val) | (mask==random_mask_val)), 1, 0)\n",
    "    mask = torch.where(((mask==fake_mask_val) | (mask==random_mask_val) | (mask==real_mask_val)), 1, 0)\n",
    "    # mask = torch.where(mask==padding_mask_val, 0, 1)\n",
    "    loss = mse_loss(pred, y)\n",
    "    # loss = l1_loss(pred, y)\n",
    "    loss = mask * loss\n",
    "    loss = loss.sum()/mask.sum()\n",
    "    return loss\n",
    "\n",
    "def loss_cat_fn(pred, y, mask):\n",
    "    bs, t = y.shape\n",
    "    # mask = torch.where(mask==1, 1, 0)\n",
    "    # mask = torch.where(mask==padding_mask_val, 0, 1)\n",
    "    mask = torch.where(((mask==fake_mask_val) | (mask==random_mask_val) | (mask==real_mask_val)), 1, 0)\n",
    "    # mask = torch.where(((mask==fake_mask_val) | (mask==real_mask_val)), 1, 0)\n",
    "    pred = pred.flatten(0, 1)\n",
    "    y = y.flatten(0, 1).to(torch.long)\n",
    "    loss = ce_loss(pred, y).reshape(bs, t)\n",
    "    loss = mask * loss\n",
    "    loss = (loss.sum()/mask.sum())\n",
    "    return loss\n",
    "\n",
    "def train(e):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    loss_li, num_loss_li, cat_loss_li = [], [], []\n",
    "    window = 100\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred, pred_cat, temp_attnweight, sa_attnweight, cs_attnweight, img_attnweight, cs_static_attnweight, static_attnweight = model(\n",
    "                                                                    data[\"sales\"].to(device), \n",
    "                                                                    data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "                                                                    data[\"index_name\"].to(device),  data[\"colour_group_name\"].to(device),  data[\"graphical_appearance_name\"].to(device),  data[\"product_type_name\"].to(device),\n",
    "                                                                    data[\"img_input\"].to(device),\n",
    "                                                                    data[\"mask\"].to(device),\n",
    "                                                                    )\n",
    "        loss_num = loss_num_fn(pred, data[\"y\"].to(device), data[\"mask\"].to(device))\n",
    "        loss_cat = loss_cat_fn(pred_cat, data[\"y_cat\"].to(device), data[\"mask\"].to(device))\n",
    "        loss = loss_num #+ loss_cat\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_li.append(loss.item()); num_loss_li.append(loss_num.item()); cat_loss_li.append(loss_cat.item())\n",
    "        pbar.set_description(f\"{e} - num_loss: {np.mean(num_loss_li[-window:])}, cat_loss: {np.mean(cat_loss_li[-window:])}, lr: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        if n % 20 == 0:\n",
    "            idx = 0\n",
    "            plt.figure(figsize=(25,12))\n",
    "            nrows, ncols = 3, 3\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # Loss\n",
    "            plt.subplot(nrows, ncols, 1)\n",
    "            plt.plot(loss_li[-window:])\n",
    "\n",
    "            # Sample\n",
    "            length = data[\"length\"][idx]\n",
    "            plt.subplot(nrows, ncols, 2)\n",
    "            plt.plot(data[\"y\"][idx][:length], label=\"y\")\n",
    "            plt.plot(pred[idx][:length].detach().cpu(), label=\"pred\")\n",
    "            plt.legend()\n",
    "\n",
    "            # Attn\n",
    "            plt.subplot(nrows, ncols, 4)\n",
    "            temp_attnweight = temp_attnweight.mean(dim=1)\n",
    "            sns.heatmap(temp_attnweight[idx].mean(dim=0).detach().cpu())\n",
    "            plt.title(\"temp_attnweight\")\n",
    "\n",
    "            plt.subplot(nrows, ncols, 5)\n",
    "            sns.heatmap(sa_attnweight[idx].mean(dim=0).detach().cpu()[:, :length])\n",
    "            plt.title(\"sa_attnweight\")\n",
    "\n",
    "            plt.subplot(nrows, ncols, 6)\n",
    "            sns.heatmap(cs_attnweight[idx].mean(dim=0).detach().cpu(), cbar=False)\n",
    "            plt.title(\"cs_attnweight\")\n",
    "\n",
    "            plt.subplot(nrows, ncols, 7)\n",
    "            plt.imshow(data[\"img_raw\"][idx])\n",
    "\n",
    "            plt.subplot(nrows, ncols, 8)\n",
    "            mask = torch.min(static_attnweight, dim=1).values\n",
    "            # mask = torch.mean(mask, dim=1)\n",
    "            mask = mask[:, 2, :]\n",
    "\n",
    "            mask = mask[idx, 4+1:]\n",
    "            mask = torch.nn.functional.softmax(mask, dim=-1).reshape(14,14).cpu().detach().numpy() # 28,28 or 14,14\n",
    "            mask = cv2.resize(mask / mask.max(), data[\"img_raw\"][idx].size)[..., np.newaxis]\n",
    "            plt.imshow(mask)\n",
    "\n",
    "            plt.subplot(nrows, ncols, 9)\n",
    "            mask = torch.min(cs_static_attnweight, dim=1).values\n",
    "            # mask = torch.mean(mask, dim=1)\n",
    "            mask = mask[:, 2, :]\n",
    "\n",
    "            mask = mask[idx, 4+1:]\n",
    "            mask = torch.nn.functional.softmax(mask, dim=-1).reshape(14,14).cpu().detach().numpy() # 28,28 or 14,14\n",
    "            mask = cv2.resize(mask / mask.max(), data[\"img_raw\"][idx].size)[..., np.newaxis]\n",
    "            plt.imshow(mask)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "epoch = 3\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pbar = tqdm(valid_dataloader)\n",
    "\n",
    "for n, data in enumerate(pbar):\n",
    "    with torch.no_grad():\n",
    "        sales = data[\"sales\"].to(device)\n",
    "        mask = data[\"mask\"].to(device)\n",
    "        pred, pred_cat, temp_attnweight, sa_attnweight, cs_attnweight, img_attnweight, cs_static_attnweight, static_attnweight = model(\n",
    "                                                                                data[\"sales\"].to(device), \n",
    "                                                                                data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "                                                                                data[\"index_name\"].to(device),  data[\"colour_group_name\"].to(device),  data[\"graphical_appearance_name\"].to(device),  data[\"product_type_name\"].to(device),\n",
    "                                                                                data[\"img_input\"].to(device),\n",
    "                                                                                data[\"mask\"].to(device),\n",
    "                                                                                )\n",
    "\n",
    "        # for i in range(30):\n",
    "        #     pred, pred_cat, temp_attnweight, sa_attnweight, cs_attnweight = model(\n",
    "        #                                                                             data[\"sales\"].to(device), \n",
    "        #                                                      \n",
    "        #                        data[\"dow\"].to(device), data[\"month\"].to(device), data[\"holiday\"].to(device), data[\"price\"].to(device),\n",
    "        #                                                                             data[\"mask\"].to(device),\n",
    "        #                                                                             )\n",
    "            # idx = torch.argmax(mask, -1)\n",
    "            # for n, i in enumerate(idx):\n",
    "            #     sales[n, i] = pred[n, i]\n",
    "            #     mask[n, i] = 0\n",
    "        \n",
    "        # idx = 0\n",
    "        idx = data[\"y\"][:, -1]\n",
    "        idx = torch.argmax(idx)\n",
    "        plt.figure(figsize=(25,12))\n",
    "        nrows, ncols = 3, 3\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Sample\n",
    "        length = data[\"length\"][idx]\n",
    "        plt.subplot(nrows, ncols, 2)\n",
    "        plt.plot(data[\"y\"][idx][:length], label=\"y\")\n",
    "        plt.plot(pred[idx][:length].detach().cpu(), label=\"pred\")\n",
    "        plt.scatter(np.arange(data[\"mask\"][idx][:length].shape[0]), torch.where(data[\"mask\"][idx]==1, 0, np.nan)[:length], color=\"red\", s=3)\n",
    "        plt.legend()\n",
    "\n",
    "        # Attn\n",
    "        plt.subplot(nrows, ncols, 4)\n",
    "        temp_attnweight = temp_attnweight.mean(dim=1)\n",
    "        sns.heatmap(temp_attnweight[idx].mean(dim=0).detach().cpu())\n",
    "        plt.title(\"temp_attnweight\")\n",
    "\n",
    "        plt.subplot(nrows, ncols, 5)\n",
    "        sns.heatmap(sa_attnweight[idx].mean(dim=0).detach().cpu()[:, :length])\n",
    "        plt.title(\"sa_attnweight\")\n",
    "\n",
    "        plt.subplot(nrows, ncols, 6)\n",
    "        sns.heatmap(cs_attnweight[idx].mean(dim=0).detach().cpu(), cbar=False)\n",
    "        plt.title(\"cs_attnweight\")\n",
    "\n",
    "        plt.subplot(nrows, ncols, 7)\n",
    "        plt.imshow(data[\"img_raw\"][idx])\n",
    "\n",
    "        plt.subplot(nrows, ncols, 8)\n",
    "        mask = torch.min(static_attnweight, dim=1).values\n",
    "        mask = torch.mean(mask, dim=1)\n",
    "        # mask = mask[:, 0, :]\n",
    "\n",
    "        mask = mask[idx, 4+1:]\n",
    "        mask = mask.reshape(14,14).cpu().detach().numpy() # 28,28 or 14,14\n",
    "        mask = cv2.resize(mask / mask.max(), data[\"img_raw\"][idx].size)[..., np.newaxis]\n",
    "        plt.imshow(mask)\n",
    "\n",
    "        plt.subplot(nrows, ncols, 9)\n",
    "        mask = torch.min(cs_static_attnweight, dim=1).values\n",
    "        mask = torch.mean(mask, dim=1)\n",
    "        # mask = mask[:, 0, :]\n",
    "\n",
    "        mask = mask[idx, 4+1:]\n",
    "        mask = mask.reshape(14,14).cpu().detach().numpy() # 28,28 or 14,14\n",
    "        mask = cv2.resize(mask / mask.max(), data[\"img_raw\"][idx].size)[..., np.newaxis]\n",
    "        plt.imshow(mask)\n",
    "        \n",
    "        print(torch.argmax(pred_cat[idx], dim=-1))\n",
    "        plt.show()\n",
    "        input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
