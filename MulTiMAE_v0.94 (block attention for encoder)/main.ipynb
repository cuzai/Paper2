{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from transformers import ViTModel, AutoImageProcessor, BertModel, AutoTokenizer\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "from architecture import Transformer\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 100\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"],\n",
    "    \"nlp\": [\"detail_desc\"]\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"],\n",
    "    \"nlp_cols\": [\"detail_desc\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 16\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "patch_size = 16\n",
    "\n",
    "d_model = {\"encoder\":256, \"decoder\":128}\n",
    "d_ff = {\"encoder\":256, \"decoder\":128}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"temporal\":0.75, \"img\":0.25, \"nlp\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_train = df_train[~pd.isna(df_train[\"detail_desc\"])]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "df_valid = df_valid[~pd.isna(df_valid[\"detail_desc\"])]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2640.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 100, 1])\n",
      "day torch.Size([2, 100])\n",
      "dow torch.Size([2, 100])\n",
      "month torch.Size([2, 100])\n",
      "holiday torch.Size([2, 100])\n",
      "price torch.Size([2, 100, 1])\n",
      "temporal_padding_mask torch.Size([2, 100, 1])\n",
      "img_path torch.Size([2, 3, 224, 224])\n",
      "detail_desc torch.Size([2, 9])\n",
      "detail_desc_remain_idx torch.Size([2, 2])\n",
      "detail_desc_masked_idx torch.Size([2, 6])\n",
      "detail_desc_revert_idx torch.Size([2, 8])\n",
      "detail_desc_remain_padding_mask torch.Size([2, 2])\n",
      "detail_desc_masked_padding_mask torch.Size([2, 6])\n",
      "detail_desc_revert_padding_mask torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiheadBlockSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.static_attention = torch.nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        temporal_attn_output, temporal_attn_weight = self.temporal_side_attention(Q, K, V, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask)\n",
    "        static_attn_output, static_attn_weight = self.static_side_attention(Q, K, V, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask)\n",
    "\n",
    "        # Concat all\n",
    "        temporal_attn_output = temporal_attn_output.view(temporal_attn_output.shape[0], -1, temporal_attn_output.shape[-1])\n",
    "        attn_output = torch.cat([temporal_attn_output, static_attn_output], dim=1)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    def temporal_side_attention(self, Q, K, V, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask):\n",
    "        # Obtain QKV\n",
    "        temporal_idx = temporal_idx.unsqueeze(0).unsqueeze(-1).repeat(Q.shape[0], 1, Q.shape[-1])\n",
    "        static_idx = static_idx.unsqueeze(0).unsqueeze(-1).repeat(Q.shape[0], 1, Q.shape[-1])\n",
    "\n",
    "        Qt = torch.gather(Q, index=temporal_idx, dim=1).view(temporal_shape)\n",
    "\n",
    "        Ktt = torch.gather(K, index=temporal_idx, dim=1).view(temporal_shape)\n",
    "        Kts = torch.gather(K, index=static_idx, dim=1).unsqueeze(1).repeat(1, Ktt.shape[1], 1, 1)\n",
    "        Kt = torch.cat([Ktt, Kts], dim=-2)\n",
    "\n",
    "        Vtt = torch.gather(V, index=temporal_idx, dim=1).view(temporal_shape)\n",
    "        Vts = torch.gather(V, index=static_idx, dim=1).unsqueeze(1).repeat(1, Vtt.shape[1], 1, 1)\n",
    "        Vt = torch.cat([Vtt, Vts], dim=-2)\n",
    "\n",
    "        # Obtain key padding mask\n",
    "        static_padding_mask = static_padding_mask.unsqueeze(1).repeat(1, temporal_padding_mask.shape[1], 1)\n",
    "\n",
    "        key_padding_mask = torch.cat([temporal_padding_mask, static_padding_mask], dim=-1)\n",
    "        key_padding_mask = torch.where(key_padding_mask==1, 0, -torch.inf)\n",
    "\n",
    "        # Attention\n",
    "        temporal_attn_output, temporal_attn_weight = self.multihead_block_attention(Qt, Kt, Vt, key_padding_mask=key_padding_mask)\n",
    "        return temporal_attn_output, temporal_attn_weight\n",
    "    \n",
    "    def static_side_attention(self, Q, K, V, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask):\n",
    "        # Obtaion QKV\n",
    "        temporal_idx = temporal_idx.unsqueeze(0).unsqueeze(-1).repeat(Q.shape[0], 1, Q.shape[-1])\n",
    "        static_idx = static_idx.unsqueeze(0).unsqueeze(-1).repeat(Q.shape[0], 1, Q.shape[-1])\n",
    "\n",
    "        Qs = torch.gather(Q, index=static_idx, dim=1)\n",
    "        \n",
    "        Kst = torch.gather(K, index=temporal_idx, dim=1)\n",
    "        Kss = torch.gather(K, index=static_idx, dim=1)\n",
    "        Ks = torch.cat([Kst, Kss], dim=1)\n",
    "\n",
    "        Vst = torch.gather(V, index=temporal_idx, dim=1)\n",
    "        Vss = torch.gather(V, index=static_idx, dim=1)\n",
    "        Vs = torch.cat([Vst, Vss], dim=1)\n",
    "\n",
    "        # Obtain key padding mask\n",
    "        temporal_padding_mask = temporal_padding_mask.view(temporal_shape[0], -1)\n",
    "        static_padding_mask = static_padding_mask\n",
    "        \n",
    "        key_padding_mask = torch.cat([temporal_padding_mask, static_padding_mask], dim=1)\n",
    "        key_padding_mask = torch.where(key_padding_mask==1, 0, -torch.inf)\n",
    "\n",
    "        # Attention\n",
    "        static_attn_output, static_attn_weight = self.static_attention(Qs, Ks, Vs, key_padding_mask=key_padding_mask)\n",
    "\n",
    "        return static_attn_output, static_attn_weight \n",
    "\n",
    "\n",
    "    def multihead_block_attention(self, Q, K, V, key_padding_mask):\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "        logits = QK / math.sqrt(d_model//self.nhead)\n",
    "        \n",
    "        ### #. Padding_mask\n",
    "        key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(-2).repeat(1, logits.shape[1], 1, logits.shape[-2], 1)\n",
    "        logits += key_padding_mask\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn_weight @ V\n",
    "\n",
    "        ### 4. Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, -1, d_model)\n",
    "\n",
    "        return attn_output, attn_weight\n",
    "\n",
    "class MultiheadBlockAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(QK / math.sqrt(d_model//self.nhead), dim=-1)\n",
    "\n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn_weight @ V\n",
    "\n",
    "        ### 4. Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, -1, d_model)\n",
    "\n",
    "        return attn_output, attn_weight\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        return self.embedding(val)\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        embedding = self.img_model(val).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        return embedding\n",
    "\n",
    "class NlpEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.nlp_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)   \n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        # Make token_type_ids\n",
    "        token_type_ids = torch.zeros(val.shape).to(torch.int).to(device)\n",
    "\n",
    "        # Make attention mask\n",
    "        attention_mask = padding_mask_dict[f\"{key}_revert_padding_mask\"]\n",
    "        mask_for_global_token = torch.ones(attention_mask.shape[0], 1).to(device)\n",
    "        attention_mask = torch.cat([attention_mask, mask_for_global_token], dim=-1)\n",
    "\n",
    "        # Embed data\n",
    "        inputs = {\"input_ids\":val, \"token_type_ids\":token_type_ids, \"attention_mask\":attention_mask}\n",
    "        embedding = self.nlp_model(**inputs).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class TemporalRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, padding_mask_dict, remain_rto, temporal_cols, device):\n",
    "        result_dict, idx_dict = {}, {}\n",
    "        # Concat data\n",
    "        concat_data_li, concat_padding_mask_li = [], []\n",
    "        for col in temporal_cols:\n",
    "            concat_data_li.append(data_dict[col])\n",
    "            concat_padding_mask_li.append(padding_mask_dict[\"temporal_padding_mask\"])\n",
    "        \n",
    "        concat_data = torch.stack(concat_data_li, dim=-2)\n",
    "        concat_padding_mask = torch.cat(concat_padding_mask_li, dim=-1)\n",
    "    \n",
    "        # Remain mask\n",
    "        num_modality = concat_data.shape[-2]\n",
    "        num_remain = int(num_modality * remain_rto)\n",
    "        \n",
    "        noise = torch.rand(concat_data.shape[:-1]).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=-1)\n",
    "\n",
    "        remain_idx = shuffle_idx[:, :, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, :, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=-1)\n",
    "\n",
    "        # Apply mask\n",
    "        concat_data = torch.gather(concat_data, index=remain_idx.unsqueeze(-1).repeat(1, 1, 1, concat_data.shape[-1]), dim=-2)\n",
    "        concat_padding_mask = torch.gather(concat_padding_mask, index=remain_idx, dim=-1)\n",
    "\n",
    "\n",
    "        result_dict[\"temporal\"] = concat_data\n",
    "        idx_dict.update({\"temporal_remain_idx\":remain_idx, \"temporal_masked_idx\":masked_idx, \"temporal_revert_idx\":revert_idx})\n",
    "        padding_mask_dict.update({\"temporal_remain_padding_mask\":concat_padding_mask})\n",
    "\n",
    "        return result_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class ImgRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, remain_rto, img_cols, device):\n",
    "        result_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        # Get indexs\n",
    "        for col in img_cols:\n",
    "            val = data_dict[col]\n",
    "            \n",
    "            # Split global token\n",
    "            global_token = val[:, :1, :]\n",
    "            val = val[:, 1:, :]\n",
    "\n",
    "            # Apply remain\n",
    "            num_remain = int(val.shape[1] * remain_rto)\n",
    "            noise = torch.rand(val.shape[0], val.shape[1]).to(device)\n",
    "            shuffle_idx = torch.argsort(noise, dim=1)\n",
    "\n",
    "            remain_idx = shuffle_idx[:, :num_remain]\n",
    "            masked_idx = shuffle_idx[:, num_remain:]\n",
    "            revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "            remain_padding_mask = torch.ones(remain_idx.shape[0], remain_idx.shape[1]+1).to(device)\n",
    "            revert_padding_mask = torch.ones(revert_idx.shape[0], revert_idx.shape[1]+1).to(device)\n",
    "\n",
    "            # Apply mask\n",
    "            val = torch.gather(val, index=remain_idx.unsqueeze(-1).repeat(1, 1, val.shape[-1]), dim=1)\n",
    "            val = torch.cat([global_token, val], dim=1)\n",
    "\n",
    "            result_dict[col] = val\n",
    "            idx_dict.update({f\"{col}_remain_idx\":remain_idx, f\"{col}_masked_idx\":masked_idx, f\"{col}_revert_idx\":revert_idx})\n",
    "            padding_mask_dict.update({f\"{col}_remain_padding_mask\":remain_padding_mask, f\"{col}_revert_padding_mask\":revert_padding_mask})\n",
    "\n",
    "        return result_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class NlpRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, remain_rto, nlp_cols, device):\n",
    "        result_dict = {}\n",
    "        for col in nlp_cols:\n",
    "            val = data_dict[col]\n",
    "            remain_idx = idx_dict[f\"{col}_remain_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            val = torch.gather(val, index=remain_idx, dim=1)\n",
    "            result_dict[col] = val\n",
    "        return result_dict\n",
    "\n",
    "\n",
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadBlockSelfAttention(d_model, nhead, dropout)\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask):\n",
    "        x = src\n",
    "        attn_output = self._sa_block(self.norm1(x), temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask)\n",
    "        x = x + attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "    def _sa_block(self, src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask):\n",
    "        x = self.self_attn(src, src, src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "\n",
    "class TemporalRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, temporal, idx_dict, temporal_cols):\n",
    "        # Append mask token\n",
    "        revert_idx = idx_dict[\"temporal_revert_idx\"]\n",
    "        mask_token = self.mask_token.unsqueeze(0).unsqueeze(1).repeat(temporal.shape[0], temporal.shape[1], revert_idx.shape[-1] - temporal.shape[-2], 1)\n",
    "        temporal = torch.cat([temporal, mask_token], dim=-2)\n",
    "\n",
    "        # Apply revert\n",
    "        temporal = torch.gather(temporal, index=revert_idx.unsqueeze(-1).repeat(1, 1, 1, temporal.shape[-1]), dim=-2)\n",
    "\n",
    "        # Split to dictionary\n",
    "        temporal_revert_dict = {}\n",
    "        for n, col in enumerate(temporal_cols):\n",
    "            temporal_revert_dict[col] = temporal[:, :, n, :]\n",
    "\n",
    "        return temporal_revert_dict\n",
    "\n",
    "class ImgRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, img_dict, idx_dict, img_cols):\n",
    "        img_revert_dict = {}\n",
    "        for col in img_cols:\n",
    "            img_data = img_dict[col]\n",
    "            \n",
    "            # Split global token\n",
    "            global_token = img_data[:, :1, :]\n",
    "            img_data = img_data[:, 1:, :]\n",
    "\n",
    "            # Append mask token\n",
    "            revert_idx = idx_dict[f\"{col}_revert_idx\"]\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(img_data.shape[0], revert_idx.shape[1]-img_data.shape[-2], 1)\n",
    "            img_data = torch.cat([img_data, mask_token], dim=-2)\n",
    "\n",
    "            # Apply revert\n",
    "            img_data = torch.gather(img_data, index=revert_idx.unsqueeze(-1).repeat(1, 1, img_data.shape[-1]), dim=-2)\n",
    "            img_data = torch.cat([global_token, img_data], dim=1)\n",
    "\n",
    "            img_revert_dict[col] = img_data\n",
    "\n",
    "        return img_revert_dict\n",
    "\n",
    "class NlpRevert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.mask_token = mask_token\n",
    "    \n",
    "    def forward(self, nlp_dict, idx_dict, nlp_cols):\n",
    "        nlp_revert_dict = {}\n",
    "        for col in nlp_cols:\n",
    "            nlp_data = nlp_dict[col]\n",
    "            \n",
    "            # Append mask token\n",
    "            revert_idx = idx_dict[f\"{col}_revert_idx\"]\n",
    "            mask_token = self.mask_token.unsqueeze(0).repeat(nlp_data.shape[0], revert_idx.shape[1]-nlp_data.shape[-2], 1)\n",
    "            nlp_data = torch.cat([nlp_data, mask_token], dim=-2)\n",
    "\n",
    "            # Apply revert\n",
    "            nlp_data = torch.gather(nlp_data, index=revert_idx.unsqueeze(-1).repeat(1, 1, nlp_data.shape[-1]), dim=-2)\n",
    "\n",
    "            nlp_revert_dict[col] = nlp_data\n",
    "       \n",
    "        return nlp_revert_dict\n",
    "\n",
    "\n",
    "class TemporalDecoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self):\n",
    "        return\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"]:\n",
    "            self.embedding = TemporalEmbedding(col, data_info, label_encoder_dict, d_model)\n",
    "        elif col in data_info.modality_info[\"img\"]:\n",
    "            self.embedding = ImgEmbedding(d_model)\n",
    "        elif col in data_info.modality_info[\"nlp\"]:\n",
    "            self.embedding = NlpEmbedding(d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask, device):\n",
    "        return self.embedding(key, val, padding_mask, device)\n",
    "\n",
    "class PosModEncoding(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc, num_modality, modality, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_enc = pos_enc\n",
    "        self.modality = modality[col]\n",
    "        self.modality_embedding = torch.nn.Embedding(num_modality, d_model)\n",
    "\n",
    "    def forward(self, key, val, device):\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Modality embedding\n",
    "        modality = torch.zeros(val.shape[1]).to(torch.int).to(device) + self.modality\n",
    "        modality = self.modality_embedding(modality)\n",
    "\n",
    "        return val + modality\n",
    "\n",
    "class Remain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temporal_remain = TemporalRemain()\n",
    "        self.img_remain = ImgRemain()\n",
    "        self.nlp_remain = NlpRemain()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, temporal_cols, img_cols, nlp_cols, device):\n",
    "        temporal_dict, temporal_idx_dict, temporal_padding_mask_dict = self.temporal_remain(data_dict, padding_mask_dict, remain_rto[\"temporal\"], temporal_cols, device)\n",
    "        idx_dict.update(temporal_idx_dict)\n",
    "        padding_mask_dict.update(temporal_padding_mask_dict)\n",
    "\n",
    "        img_dict, img_idx_dict, img_padding_mask_dict = self.img_remain(data_dict, remain_rto[\"img\"], img_cols, device)\n",
    "        idx_dict.update(img_idx_dict)\n",
    "        padding_mask_dict.update(img_padding_mask_dict)\n",
    "\n",
    "        nlp_dict = self.nlp_remain(data_dict, idx_dict, remain_rto[\"nlp\"], nlp_cols, device)\n",
    "\n",
    "        return temporal_dict, img_dict, nlp_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation, num_layers, to_decoder_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = torch.nn.ModuleList([copy.deepcopy(EncoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "        self.to_decoder_dim = to_decoder_dim\n",
    "    \n",
    "    def forward(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask, img_idx_li, nlp_idx_li = self.get_src(temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device)\n",
    "        for mod in self.encoder_layers:\n",
    "            src = mod(src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask)\n",
    "        \n",
    "        encoded = self.to_decoder_dim(src)\n",
    "        temporal_dict, img_dict, nlp_dict = self.undo_src(encoded, temporal_shape, temporal_idx, img_idx_li, nlp_idx_li, img_cols, nlp_cols)\n",
    "\n",
    "        return temporal_dict, img_dict, nlp_dict\n",
    "    \n",
    "    def get_src(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        # Temporal\n",
    "        temporal = temporal_dict[\"temporal\"]\n",
    "        temporal_shape = temporal.shape\n",
    "        batch_size, seq_len, _, d_model = temporal_shape\n",
    "        temporal = temporal.view(batch_size, -1, d_model)\n",
    "        temporal_padding_mask = padding_mask_dict[\"temporal_remain_padding_mask\"]\n",
    "        temporal_idx = torch.arange(0, temporal.shape[1]).to(device)\n",
    "\n",
    "        # Static\n",
    "        static_start_idx = temporal_idx[-1]+1\n",
    "        idx = static_start_idx.clone()\n",
    "\n",
    "        ### Img\n",
    "        img_li, img_padding_mask_li, img_idx_li = [], [], []\n",
    "        for col in img_cols:\n",
    "            img_data = img_dict[col]\n",
    "            img_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "\n",
    "            img_li.append(img_data)\n",
    "            img_padding_mask_li.append(img_padding_mask)\n",
    "            img_idx_li.append(torch.arange(idx, idx+img_data.shape[1]).to(device))\n",
    "            idx += img_data.shape[1]\n",
    "        \n",
    "        img = torch.cat(img_li, dim=1)\n",
    "        img_padding_mask = torch.cat(img_padding_mask_li, dim=1)\n",
    "\n",
    "        ### Nlp\n",
    "        nlp_li, nlp_padding_mask_li, nlp_idx_li = [], [], []\n",
    "        for col in nlp_cols:\n",
    "            nlp_data = nlp_dict[col]\n",
    "            nlp_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "\n",
    "            nlp_li.append(nlp_data)\n",
    "            nlp_padding_mask_li.append(nlp_padding_mask)\n",
    "            nlp_idx_li.append(torch.arange(idx, idx+nlp_data.shape[1]).to(device))\n",
    "            idx += nlp_data.shape[1]\n",
    "\n",
    "        nlp = torch.cat(nlp_li, dim=1)\n",
    "        nlp_padding_mask = torch.cat(nlp_padding_mask_li, dim=1)\n",
    "\n",
    "        ### Src\n",
    "        static = torch.cat([img, nlp], dim=1)\n",
    "        static_padding_mask = torch.cat([img_padding_mask, nlp_padding_mask], dim=1)\n",
    "\n",
    "        img_idx = torch.cat(img_idx_li)\n",
    "        nlp_idx = torch.cat(nlp_idx_li)\n",
    "        static_idx = torch.cat([img_idx, nlp_idx])\n",
    "\n",
    "        # Src\n",
    "        src = torch.cat([temporal, static], dim=1)\n",
    "        \n",
    "        return src, temporal_shape, temporal_idx, static_idx, temporal_padding_mask, static_padding_mask, img_idx_li, nlp_idx_li\n",
    "    \n",
    "    def undo_src(self, src, temporal_shape, temporal_idx, img_idx_li, nlp_idx_li, img_cols, nlp_cols):\n",
    "        # Temporal\n",
    "        temporal_dict = {}\n",
    "        temporal_idx = temporal_idx.unsqueeze(0).unsqueeze(-1).repeat(src.shape[0], 1, src.shape[-1])\n",
    "        temporal = torch.gather(src, index=temporal_idx, dim=1).view(temporal_shape[0], temporal_shape[1], temporal_shape[2], -1)\n",
    "        temporal_dict[\"temporal\"] = temporal\n",
    "\n",
    "        # Img\n",
    "        img_dict = {}\n",
    "        for col, idx in zip(img_cols, img_idx_li):\n",
    "            idx = idx.unsqueeze(0).unsqueeze(-1).repeat(src.shape[0], 1, src.shape[-1])\n",
    "            img_dict[col] = torch.gather(src, index=idx, dim=1)\n",
    "\n",
    "        # Nlp\n",
    "        nlp_dict = {}\n",
    "        for col, idx in zip(nlp_cols, nlp_idx_li):\n",
    "            idx = idx.unsqueeze(0).unsqueeze(-1).repeat(src.shape[0], 1, src.shape[-1])\n",
    "            nlp_dict[col] = torch.gather(src, index=idx, dim=1)\n",
    "        \n",
    "        return temporal_dict, img_dict, nlp_dict\n",
    "\n",
    "class Revert(torch.nn.Module):\n",
    "    def __init__(self, mask_token):\n",
    "        super().__init__()\n",
    "        self.temporal_revert = TemporalRevert(mask_token)\n",
    "        self.img_revert = ImgRevert(mask_token)\n",
    "        self.nlp_revert = NlpRevert(mask_token)\n",
    "    \n",
    "    def forward(self, temporal_dict, img_dict, nlp_dict, idx_dict, temporal_cols, img_cols, nlp_cols):\n",
    "        temporal_dict = self.temporal_revert(temporal_dict[\"temporal\"], idx_dict, temporal_cols)\n",
    "        img_dict = self.img_revert(img_dict, idx_dict, img_cols)\n",
    "        nlp_dict = self.nlp_revert(nlp_dict, idx_dict, nlp_cols)\n",
    "\n",
    "        revert_dict = {}\n",
    "        revert_dict.update(temporal_dict)\n",
    "        revert_dict.update(img_dict)\n",
    "        revert_dict.update(nlp_dict)\n",
    "        \n",
    "        return revert_dict\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, col, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.temporal_decoder_layers = torch.nn.ModuleList([copy.deepcopy(TemporalDecoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device):\n",
    "        # Temporal\n",
    "        if key in temporal_cols:\n",
    "            tgt = val\n",
    "            memory = self.get_temporal_tgt_memory(key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\n",
    "            \n",
    "        return\n",
    "\n",
    "    def get_temporal_tgt_memory(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device):\n",
    "        memory_li = []\n",
    "\n",
    "        # Temporal memory\n",
    "        for col in temporal_cols:\n",
    "            if col == key:\n",
    "                continue\n",
    "            memory_li.append(data_dict[col])\n",
    "        memory = torch.stack(memory_li, dim=-2)\n",
    "        cross_attn_padding_mask = torch.ones(memory.shape[:-1]).to(device)\n",
    "        \n",
    "        # Static memory\n",
    "        for col in img_cols + nlp_cols:\n",
    "            static_data = data_dict[col].unsqueeze(1).repeat(1, memory.shape[1], 1, 1)\n",
    "            static_padding_msak = padding_mask_dict[f\"{col}_revert_padding_mask\"].unsqueeze(1).repeat(1, memory.shape[1], 1)\n",
    "\n",
    "            memory = torch.cat([memory, static_data], dim=-2)\n",
    "            cross_attn_padding_mask = torch.cat([cross_attn_padding_mask, static_padding_msak], dim=-1)\n",
    "        \n",
    "        # Self attn padding mask\n",
    "        self_attn_padding_mask = padding_mask_dict[\"temporal_padding_mask\"]\n",
    "        print(self_attn_padding_mask.shape)\n",
    "\n",
    "        raise\n",
    "\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                d_model, num_layers, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.data_info, self.label_encoder_dict = data_info, label_encoder_dict\n",
    "        self.temporal_cols, self.img_cols, self.nlp_cols, self.total_cols = self.define_cols()\n",
    "        self.self_attn_weight_dict, self.cross_attn_weight_dict = {}, {}\n",
    "        \n",
    "        # 1. Embedding\n",
    "        self.embedding_dict = self.init_process(mod=Embedding, args=[self.data_info, self.label_encoder_dict, d_model[\"encoder\"]])\n",
    "        # 2. Pos encoding and modality embedding\n",
    "        num_modality = len(self.total_cols)\n",
    "        modality = {col:n for n, col in enumerate(self.total_cols)}\n",
    "        encoder_pos_enc = PositionalEncoding(d_model[\"encoder\"], dropout)\n",
    "        self.enc_pos_mod_encoding_dict = self.init_process(mod=PosModEncoding, args=[encoder_pos_enc, num_modality, modality, d_model[\"encoder\"]])\n",
    "        # 3. Remain mask\n",
    "        self.remain_dict = Remain()\n",
    "        # 4. Encoding\n",
    "        to_decoder_dim = torch.nn.Linear(d_model[\"encoder\"], d_model[\"decoder\"])\n",
    "        self.encoding = Encoder(d_model[\"encoder\"], nhead, d_ff[\"encoder\"], dropout, activation, num_layers[\"encoder\"], to_decoder_dim)\n",
    "        # 5. Revert\n",
    "        mask_token = torch.nn.Parameter(torch.rand(1, d_model[\"decoder\"]))\n",
    "        self.revert = Revert(mask_token)\n",
    "        # 6. Pos encoding and modality embedding\n",
    "        decoder_pos_enc = PositionalEncoding(d_model[\"decoder\"], dropout)\n",
    "        self.dec_pos_mod_encoding_dict = self.init_process(mod=PosModEncoding, args=[decoder_pos_enc, num_modality, modality, d_model[\"decoder\"]])\n",
    "        # 7. Decoding\n",
    "        self.decoding = self.init_process(mod=Decoder, args=[d_model[\"decoder\"], nhead, d_ff[\"decoder\"], dropout, activation, num_layers[\"decoder\"]])\n",
    "\n",
    "    def forward(self, data_input, remain_rto, device):\n",
    "        data_dict, self.idx_dict, self.padding_mask_dict = self.to_gpu(data_input, device)\n",
    "        \n",
    "        # 1. Embedding\n",
    "        embedding_dict = self.apply_process(data=data_dict, mod=self.embedding_dict, args=[self.padding_mask_dict, device])\n",
    "        # 2. Pos encoding and modality embedding\n",
    "        enc_pos_mod_encoding_dict = self.apply_process(data=embedding_dict, mod=self.enc_pos_mod_encoding_dict, args=[device])\n",
    "        # 3. Remain mask\n",
    "        temporal_dict, img_dict, nlp_dict, self.idx_dict, self.padding_mask_dict = self.remain_dict(enc_pos_mod_encoding_dict, self.idx_dict, self.padding_mask_dict, remain_rto, self.temporal_cols, self.img_cols, self.nlp_cols, device)\n",
    "        # 4. Encoding\n",
    "        temporal_encoding_dict, img_encoding_dict, nlp_encoding_dict = self.encoding(temporal_dict, img_dict, nlp_dict, self.padding_mask_dict, self.img_cols, self.nlp_cols, device)\n",
    "        # 5. Revert\n",
    "        revert_dict = self.revert(temporal_encoding_dict, img_encoding_dict, nlp_encoding_dict, self.idx_dict, self.temporal_cols, self.img_cols, self.nlp_cols)\n",
    "        # 6. Pos encoding and modality embedding\n",
    "        dec_pos_mod_encoding_dict = self.apply_process(data=revert_dict, mod=self.dec_pos_mod_encoding_dict, args=[device])\n",
    "        # 7. Decoder\n",
    "        self.apply_process(data=dec_pos_mod_encoding_dict, mod=self.decoding, args=[dec_pos_mod_encoding_dict, self.padding_mask_dict, self.temporal_cols, self.img_cols, self.nlp_cols, device])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def define_cols(self):\n",
    "        temporal_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        img_cols = self.data_info.modality_info[\"img\"]\n",
    "        nlp_cols = self.data_info.modality_info[\"nlp\"]\n",
    "        total_cols = temporal_cols + img_cols + nlp_cols\n",
    "\n",
    "        return temporal_cols, img_cols, nlp_cols, total_cols\n",
    "\n",
    "    def to_gpu(self, data_input, device):\n",
    "        data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        for key, val in data_input.items():\n",
    "            if key in self.temporal_cols + self.img_cols + self.nlp_cols:\n",
    "                data_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"idx\"):\n",
    "                idx_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"padding_mask\"):\n",
    "                padding_mask_dict[key] = data_input[key].to(device)\n",
    "            \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def init_process(self, mod, args=[], target_cols=None):\n",
    "        result_dict = {}\n",
    "        target_cols = self.total_cols if target_cols is None else target_cols\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod(col, *args)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def apply_process(self, data, mod, args=[], target_cols=None, collate_fn=None):\n",
    "        result_dict = {}\n",
    "        target_cols = self.total_cols if target_cols is None else target_cols\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod[col](col, data[col], *args)\n",
    "        \n",
    "        if collate_fn is not None:\n",
    "            return collate_fn(result_dict)\n",
    "        else:\n",
    "            return result_dict\n",
    "\n",
    "    def tidy_decoding(self, decoding_dict):\n",
    "        result_dict = {}\n",
    "        for key, val in decoding_dict.items():\n",
    "            result_dict[key], self_attn_weight, cross_attn_weight = val\n",
    "            self.self_attn_weight_dict.update({key:self_attn_weight})\n",
    "            self.cross_attn_weight_dict.update({key:cross_attn_weight})\n",
    "\n",
    "        return result_dict\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(data_info, train_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder_dict,\n\u001b[1;32m      2\u001b[0m                         d_model, num_layers, nhead, d_ff, dropout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremain_rto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_parent_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_model_summary/model_summary.py:128\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m model_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_training:\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[92], line 46\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, data_input, remain_rto, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m dec_pos_mod_encoding_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_process(data\u001b[38;5;241m=\u001b[39mrevert_dict, mod\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_pos_mod_encoding_dict, args\u001b[38;5;241m=\u001b[39m[device])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 7. Decoder\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_pos_mod_encoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdec_pos_mod_encoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 83\u001b[0m, in \u001b[0;36mTransformer.apply_process\u001b[0;34m(self, data, mod, args, target_cols, collate_fn)\u001b[0m\n\u001b[1;32m     81\u001b[0m target_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cols \u001b[38;5;28;01mif\u001b[39;00m target_cols \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m target_cols\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m target_cols:\n\u001b[0;32m---> 83\u001b[0m     result_dict[col] \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn(result_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[91], line 170\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m temporal_cols:\n\u001b[1;32m    169\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m--> 170\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_temporal_tgt_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[91], line 197\u001b[0m, in \u001b[0;36mDecoder.get_temporal_tgt_memory\u001b[0;34m(self, key, val, data_dict, padding_mask_dict, temporal_cols, img_cols, nlp_cols, device)\u001b[0m\n\u001b[1;32m    194\u001b[0m self_attn_padding_mask \u001b[38;5;241m=\u001b[39m padding_mask_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(self_attn_padding_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout, \"gelu\")\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"img_model\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"nlp_model\" in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def patchify(imgs, patch_size=16):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size=16):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
