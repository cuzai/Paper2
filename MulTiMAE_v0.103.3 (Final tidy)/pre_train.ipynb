{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Torch-related\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Custom defined\n",
    "from config import pre_train\n",
    "from libs.data import load_dataset, collate_fn\n",
    "from architecture.architecture import *\n",
    "from architecture.shared_module import patchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2525.17it/s]\n",
      "100%|██████████| 29274/29274 [00:01<00:00, 21879.00it/s]\n",
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([64, 300, 1])\n",
      "day torch.Size([64, 300])\n",
      "dow torch.Size([64, 300])\n",
      "month torch.Size([64, 300])\n",
      "holiday torch.Size([64, 300])\n",
      "price torch.Size([64, 300, 1])\n",
      "target_fcst_mask torch.Size([64, 300])\n",
      "temporal_padding_mask torch.Size([64, 300])\n",
      "img_path torch.Size([64, 3, 224, 224])\n",
      "detail_desc torch.Size([64, 77])\n",
      "detail_desc_revert_padding_mask torch.Size([64, 78])\n",
      "detail_desc_remain_idx torch.Size([64, 20])\n",
      "detail_desc_masked_idx torch.Size([64, 57])\n",
      "detail_desc_revert_idx torch.Size([64, 77])\n",
      "information torch.Size([64, 38])\n",
      "information_revert_padding_mask torch.Size([64, 39])\n",
      "information_remain_idx torch.Size([64, 10])\n",
      "information_masked_idx torch.Size([64, 28])\n",
      "information_revert_idx torch.Size([64, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "is_test_mode = False\n",
    "is_new_rawdata = False\n",
    "is_new_dataset = True\n",
    "config = pre_train\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "if is_new_dataset:\n",
    "    train_dataset = load_dataset(is_test_mode, is_new_rawdata, config, mode=\"pre_train\", verbose=True)\n",
    "else:\n",
    "    suffix = \"_test\" if is_test_mode else \"\"\n",
    "    train_dataset = torch.load(f\"src/pre_train_train_dataset{suffix}\")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, config), pin_memory=True, num_workers=16, prefetch_factor=32)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, config))\n",
    "for _ in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in _.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "            Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "   MaskedBlockAutoencoder      MBAEEncoder-1                          23,863,040      23,762,176\n",
      "   MaskedBlockAutoencoder      MBAEDecoder-2                          31,603,897      31,603,897\n",
      "================================================================================================\n",
      "Total params: 55,466,937\n",
      "Trainable params: 55,366,073\n",
      "Non-trainable params: 100,864\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'------------------------------------------------------------------------------------------------\\n            Parent Layers       Layer (type)        Output Shape         Param #     Tr. Param #\\n================================================================================================\\n   MaskedBlockAutoencoder      MBAEEncoder-1                          23,863,040      23,762,176\\n   MaskedBlockAutoencoder      MBAEDecoder-2                          31,603,897      31,603,897\\n================================================================================================\\nTotal params: 55,466,937\\nTrainable params: 55,366,073\\nNon-trainable params: 100,864\\n------------------------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder_dict = joblib.load(\"src/label_encoder_dict.pkl\")\n",
    "model = MaskedBlockAutoencoder(config, label_encoder_dict)\n",
    "model.to(device)\n",
    "summary(model, _, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def get_loss(pred_dict, y_dict, idx_dict, padding_mask_dict):\n",
    "    loss_dict = {}\n",
    "    loss_sum, cnt = 0, 0\n",
    "\n",
    "    for n, (key, pred) in enumerate(pred_dict.items()):\n",
    "        y = y_dict[key].to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        ### Temporal loss\n",
    "        if key in config.temporal_cols:\n",
    "            if key in config.scaling_cols:\n",
    "                loss = mse_loss(pred, y).squeeze()\n",
    "            elif key in config.embedding_cols:\n",
    "                loss = ce_loss(pred.view(-1, pred.shape[-1]), y.view(-1).to(torch.long))\n",
    "                loss = loss.view(y.shape)\n",
    "        ### Img loss\n",
    "        elif key in config.img_cols:\n",
    "            pred = pred[:, 1:, :]\n",
    "            y = patchify(y, config.patch_size)\n",
    "            loss = mse_loss(pred, y)\n",
    "        ### Nlp loss\n",
    "        elif key in config.nlp_cols:\n",
    "            pred = pred[:, 1:, :]\n",
    "            loss = ce_loss(pred.reshape(-1, pred.shape[-1]), y.reshape(-1).to(torch.long))\n",
    "            loss = loss.view(y.shape)\n",
    "        \n",
    "        # Masking loss\n",
    "        ### Temporal\n",
    "        if key in config.temporal_cols:\n",
    "            masked_idx = idx_dict[\"temporal_masked_idx\"]\n",
    "            masking_mask = (masked_idx==n).sum(dim=-1)\n",
    "            padding_mask = padding_mask_dict[\"temporal_padding_mask\"]\n",
    "\n",
    "            total_mask = torch.where((padding_mask==1)&(masking_mask==1), 1, 0)\n",
    "            loss *= total_mask\n",
    "            loss_sum += loss.sum(); cnt += total_mask.sum()\n",
    "            loss = loss.sum()/total_mask.sum()\n",
    "        ### Img\n",
    "        elif key in config.img_cols:\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            \n",
    "            loss = torch.gather(loss, index=masked_idx.unsqueeze(-1).expand(-1, -1, loss.shape[-1]), dim=1)\n",
    "            loss_sum += loss.sum(); cnt += loss.shape[0]*loss.shape[1]*loss.shape[2]\n",
    "            loss = loss.mean()\n",
    "        ### Nlp\n",
    "        elif key in config.nlp_cols:\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            padding_mask = padding_mask_dict[f\"{key}_masked_padding_mask\"][:, 1:]\n",
    "\n",
    "            loss = torch.gather(loss, index=masked_idx, dim=1)\n",
    "            loss *= padding_mask\n",
    "\n",
    "            loss_sum += loss.sum(); cnt += total_mask.sum()\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "            \n",
    "        loss_dict[key] = loss\n",
    "    \n",
    "    total_loss = loss_sum / cnt\n",
    "    return loss_dict, total_loss\n",
    "\n",
    "def obtain_loss_dict_for_plot(total_loss, loss_dict, loss_li_dict, mean_loss_li_dict):\n",
    "    loss_li_dict[\"total\"].append(total_loss.item())\n",
    "    mean_loss_li_dict[\"total\"].append(np.array(loss_li_dict[\"total\"]).mean())\n",
    "\n",
    "    for key, val in loss_dict.items():\n",
    "        loss_li_dict[key].append(val.item())\n",
    "        mean_loss_li_dict[key].append(np.array(loss_li_dict[key]).mean())\n",
    "\n",
    "    return loss_li_dict, mean_loss_li_dict\n",
    "\n",
    "def plot_sample(nrows, ncols, config, mean_loss_li_dict, output_dict, data_dict, decoding_weight_dict):\n",
    "    idx, plot_idx = 0, 1\n",
    "    for key, val in mean_loss_li_dict.items():\n",
    "        # Individual loss\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(val)\n",
    "        plt.title(f\"{key}: {val[-1]}\")\n",
    "        plot_idx += 1; \n",
    "        if key==\"total\": \n",
    "            plot_idx += 3; continue\n",
    "\n",
    "        pred, y = output_dict[key].detach().cpu().squeeze(), data_dict[key].squeeze()\n",
    "        length_dict = {\"temporal\" if key in config.temporal_cols else key :val.shape[1] for key, val in decoding_weight_dict.items()}\n",
    "        \n",
    "        # Temporal sample\n",
    "        if key in config.temporal_cols:\n",
    "            ### Sample\n",
    "            if key in config.embedding_cols: pred = torch.argmax(pred, dim=-1)\n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.plot(y[idx]); plt.plot(pred[idx])\n",
    "            ### Weight\n",
    "            decoder_weight = decoding_weight_dict[key][idx].mean(dim=0).detach().cpu()\n",
    "            # decoder_weight = decoding_weight_dict[key][idx].min(dim=0).values.detach().cpu()\n",
    "            ###### Temporal\n",
    "            img_decoder_weight = decoder_weight[length_dict[\"temporal\"]:length_dict[\"temporal\"]+length_dict[\"img_path\"]]\n",
    "            img_decoder_weight = img_decoder_weight[1:]\n",
    "            plt.subplot(nrows, ncols, plot_idx+1)\n",
    "            plt.imshow(img_decoder_weight.reshape(224//config.patch_size, 224//config.patch_size))\n",
    "            ###### Nlp\n",
    "            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "            nlp_decoder_weight1 = decoder_weight[length_dict[\"temporal\"]+length_dict[\"img_path\"]:length_dict[\"temporal\"]+length_dict[\"img_path\"]+length_dict[\"detail_desc\"]]\n",
    "            nlp_decoder_weight2 = decoder_weight[length_dict[\"temporal\"]+length_dict[\"img_path\"]+length_dict[\"detail_desc\"]:]\n",
    "            nlp_decoder_weight = torch.cat([nlp_decoder_weight1[1:], nlp_decoder_weight2[1:]], dim=-1)\n",
    "\n",
    "            nlp1 = tokenizer.tokenize(tokenizer.decode(data_dict[\"detail_desc\"][idx]))\n",
    "            nlp2 = tokenizer.tokenize(tokenizer.decode(data_dict[\"information\"][idx]))\n",
    "            # nlp2 = []\n",
    "            text = nlp1 + nlp2\n",
    "\n",
    "            df = pd.DataFrame({\"text\":text, \"weight\":nlp_decoder_weight})\n",
    "            df = df[df[\"text\"]!=\"[PAD]\"]\n",
    "            plt.subplot(nrows, ncols, plot_idx+2)\n",
    "            sns.barplot(df[\"weight\"])\n",
    "            plt.gca().set_xticklabels(df[\"text\"], rotation=90)\n",
    "\n",
    "        # Img sample\n",
    "        elif key in config.img_cols:\n",
    "            pred = unpatchify(pred[:, 1:, :]).permute(0,2,3,1)\n",
    "            y = y.permute(0,2,3,1)\n",
    "            \n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.imshow(data_dict[\"img_path_raw\"][idx].permute(1,2,0))\n",
    "\n",
    "            plt.subplot(nrows, ncols, plot_idx+1)\n",
    "            plt.imshow(y[idx])\n",
    "\n",
    "            plt.subplot(nrows, ncols, plot_idx+2)\n",
    "            plt.imshow(pred[idx])\n",
    "\n",
    "        # Nlp sample\n",
    "        elif key in config.nlp_cols:\n",
    "            pred = tokenizer.decode(torch.argmax(pred, dim=-1)[idx])\n",
    "            y = tokenizer.decode(y[idx])\n",
    "        \n",
    "        plot_idx += 3\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, config, e):\n",
    "    pbar = tqdm(dataloader)\n",
    "    loss_li_dict, mean_loss_li_dict = defaultdict(list), defaultdict(list)\n",
    "    model.train()\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        decoding_output_dict, encoding_weight_dict, decoding_weight_dict, idx_dict, padding_mask_dict = model(data, device)\n",
    "        loss_dict, loss = get_loss(decoding_output_dict, data, idx_dict, padding_mask_dict)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot\n",
    "        if n % 20 == 0:\n",
    "            nrows, ncols = 12, 4\n",
    "            plt.figure(figsize=(25,25))\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            loss_li_dict, mean_loss_li_dict = obtain_loss_dict_for_plot(loss, loss_dict, loss_li_dict, mean_loss_li_dict)\n",
    "            plot_sample(nrows, ncols, config, mean_loss_li_dict, decoding_output_dict, data, decoding_weight_dict)\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/458 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.67 GiB total capacity; 22.94 GiB already allocated; 119.25 MiB free; 23.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m----> 7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      9\u001b[0m     epoch_loss[e] \u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[4], line 161\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, config, e)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[1;32m    160\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 161\u001b[0m     decoding_output_dict, encoding_weight_dict, decoding_weight_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     loss_dict, loss \u001b[38;5;241m=\u001b[39m get_loss(decoding_output_dict, data, idx_dict, padding_mask_dict)\n\u001b[1;32m    163\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.103.3 (Final tidy)/architecture/architecture.py:607\u001b[0m, in \u001b[0;36mMaskedBlockAutoencoder.forward\u001b[0;34m(self, data_input, device)\u001b[0m\n\u001b[1;32m    604\u001b[0m data_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_gpu(data_input, device)\n\u001b[1;32m    606\u001b[0m encoding_dict, encoding_weight_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(data_dict, idx_dict, padding_mask_dict, device)\n\u001b[0;32m--> 607\u001b[0m decoding_output_dict, decoding_weight_dict\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoding_output_dict, encoding_weight_dict, decoding_weight_dict, idx_dict, padding_mask_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.103.3 (Final tidy)/architecture/architecture.py:582\u001b[0m, in \u001b[0;36mMBAEDecoder.forward\u001b[0;34m(self, encoding_dict, idx_dict, padding_mask_dict, device)\u001b[0m\n\u001b[1;32m    580\u001b[0m attn_output_dict, attn_weight_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_decoding(indiv_encoding_dict, padding_mask_dict, device, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# 4. Output\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_dict, attn_weight_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.103.3 (Final tidy)/architecture/architecture.py:523\u001b[0m, in \u001b[0;36mOutput.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_cols \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_cols:\n\u001b[1;32m    522\u001b[0m     data \u001b[38;5;241m=\u001b[39m data_dict[col]\n\u001b[0;32m--> 523\u001b[0m     result_dict[col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.67 GiB total capacity; 22.94 GiB already allocated; 119.25 MiB free; 23.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "epoch = 10\n",
    "\n",
    "epoch_loss = {}\n",
    "for e in range(epoch):\n",
    "    loss = train_epoch(model, optimizer, train_dataloader, config, e)\n",
    "    scheduler.step()\n",
    "    epoch_loss[e] = loss\n",
    "\n",
    "    # Save model\n",
    "    if not is_test_mode:\n",
    "        now = datetime.datetime.now()\n",
    "        path = f\"./saved_model_epoch{e}_{now}\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_test_mode:\n",
    "    now = datetime.datetime.now()\n",
    "    path = f\"./saved_model_epoch{e}_{now}\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
