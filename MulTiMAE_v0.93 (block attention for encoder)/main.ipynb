{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "from transformers import ViTModel, AutoImageProcessor, BertModel, AutoTokenizer\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "from architecture import Transformer\n",
    "\n",
    "import cv2\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 100\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"],\n",
    "    \"nlp\": [\"detail_desc\"]\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"],\n",
    "    \"nlp_cols\": [\"detail_desc\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 16\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "\n",
    "d_model = {\"encoder\":256, \"decoder\":128}\n",
    "d_ff = {\"encoder\":256, \"decoder\":128}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"temporal\":0.75, \"img\":0.25, \"nlp\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_train = df_train[~pd.isna(df_train[\"detail_desc\"])]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "df_valid = df_valid[~pd.isna(df_valid[\"detail_desc\"])]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29274/29274 [00:00<00:00, 30087.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([16, 100, 1])\n",
      "day torch.Size([16, 100])\n",
      "dow torch.Size([16, 100])\n",
      "month torch.Size([16, 100])\n",
      "holiday torch.Size([16, 100])\n",
      "price torch.Size([16, 100, 1])\n",
      "temporal_padding_mask torch.Size([16, 100, 1])\n",
      "img_path torch.Size([16, 3, 224, 224])\n",
      "detail_desc torch.Size([16, 57])\n",
      "detail_desc_remain_idx torch.Size([16, 14])\n",
      "detail_desc_masked_idx torch.Size([16, 42])\n",
      "detail_desc_revert_idx torch.Size([16, 56])\n",
      "detail_desc_remain_padding_mask torch.Size([16, 14])\n",
      "detail_desc_masked_padding_mask torch.Size([16, 42])\n",
      "detail_desc_revert_padding_mask torch.Size([16, 56])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = torch.permute(x, (1,0,2))\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiheadBlockSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li, temporal_side=True, non_temporal_side=True):\n",
    "        # Linear transformation\n",
    "        Q = self.q_linear(query)\n",
    "        K = self.k_linear(key)\n",
    "        V = self.v_linear(value)\n",
    "\n",
    "        if temporal_side:\n",
    "            temporal_attn_output, temporal_attn_weight = self.temporal_side_attn(Q, K, V, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "            print(\"temporal_attn_output:\", temporal_attn_output.shape)\n",
    "            print(\"temporal_attn_weight:\", temporal_attn_weight.shape)\n",
    "        \n",
    "        if non_temporal_side:\n",
    "            non_temporal_attn_output, non_temporal_attn_weight = self.non_temporal_side_attn(Q, K, V, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "    \n",
    "    def temporal_side_attn(self, query, key, value, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n",
    "        # Q\n",
    "        Q = torch.gather(query, index=temporal_idx.unsqueeze(0).unsqueeze(-1).repeat(query.shape[0], 1, query.shape[-1]), dim=1)\n",
    "        Q = Q.view(temporal_shape)\n",
    "\n",
    "        # K and V\n",
    "        def get_KV(KV, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n",
    "            KV_temporal = torch.gather(query, index=temporal_idx.unsqueeze(0).unsqueeze(-1).repeat(query.shape[0], 1, query.shape[-1]), dim=1)\n",
    "            KV_temporal = KV_temporal.view(temporal_shape)\n",
    "\n",
    "            KV_img_li = []\n",
    "            for img_idx in img_idx_li:\n",
    "                KV_img_temp = torch.gather(KV, index=img_idx.unsqueeze(0).unsqueeze(-1).repeat(KV.shape[0], 1, KV.shape[-1]), dim=1)\n",
    "                KV_img_li.append(KV_img_temp)\n",
    "            KV_img = torch.cat(KV_img_li, dim=1)\n",
    "\n",
    "            KV_nlp_li = []\n",
    "            for nlp_idx in nlp_idx_li:\n",
    "                KV_nlp_temp = torch.gather(KV, index=nlp_idx.unsqueeze(0).unsqueeze(-1).repeat(KV.shape[0], 1, KV.shape[-1]), dim=1)\n",
    "                KV_nlp_li.append(KV_nlp_temp)\n",
    "            KV_nlp = torch.cat(KV_nlp_li, dim=1)\n",
    "\n",
    "            KV_non_temporal = torch.cat([KV_img, KV_nlp], dim=1)\n",
    "            KV_non_temporal = KV_non_temporal.unsqueeze(1).repeat(1, KV_temporal.shape[1], 1, 1)\n",
    "            \n",
    "            KV = torch.cat([KV_temporal, KV_non_temporal], dim=-2)\n",
    "\n",
    "            temporal_padding_mask = temporal_padding_mask.view(temporal_shape[:-1])\n",
    "            img_padding_mask = img_padding_mask.unsqueeze(1).repeat(1, KV.shape[1], 1)\n",
    "            nlp_padding_mask = nlp_padding_mask.unsqueeze(1).repeat(1, KV.shape[1], 1)\n",
    "\n",
    "            padding_mask = torch.cat([temporal_padding_mask, img_padding_mask, nlp_padding_mask], dim=-1)\n",
    "            padding_mask = torch.where(padding_mask == 1, 0, -torch.inf)\n",
    "\n",
    "            return KV, padding_mask\n",
    "        \n",
    "        K, padding_mask = get_KV(key, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "        V, _ = get_KV(value, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "    \n",
    "        # Split head\n",
    "        batch_size, seq_len, _, d_model = Q.shape\n",
    "        Q = Q.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        K = K.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "        V = V.view(batch_size, seq_len, -1, self.nhead, d_model//self.nhead).permute(0, 3, 1, 2, 4)\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        ### 1. Q·K^t\n",
    "        QK = Q @ K.permute(0,1,2,4,3)\n",
    "        logits = QK / math.sqrt(d_model//self.nhead)\n",
    "        padding_mask = padding_mask.unsqueeze(1).unsqueeze(-2).repeat(1, logits.shape[1], 1, logits.shape[-2], 1)\n",
    "        logits += padding_mask\n",
    "\n",
    "        ### 2. Softmax\n",
    "        attn_weight = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        ### 3. Matmul V\n",
    "        attn_output = attn_weight @ V\n",
    "\n",
    "        ### 4. Concat heads\n",
    "        attn_output = attn_output.permute(0,2,3,1,4).reshape(batch_size, seq_len, -1, d_model)\n",
    "\n",
    "        return attn_output, attn_weight\n",
    "\n",
    "        \n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TemporalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.processing_info[\"scaling_cols\"]:\n",
    "            self.embedding = torch.nn.Linear(1, d_model)\n",
    "        \n",
    "        elif col in data_info.processing_info[\"embedding_cols\"]:\n",
    "            num_cls = label_encoder_dict[col].get_num_cls()\n",
    "            self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        return self.embedding(val)\n",
    "\n",
    "class ImgEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        embedding = self.img_model(val).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        return embedding\n",
    "\n",
    "class NlpEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.nlp_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        self.downsize_linear = torch.nn.Linear(768, d_model)   \n",
    "    \n",
    "    def forward(self, key, val, padding_mask_dict, device):\n",
    "        # Make token_type_ids\n",
    "        token_type_ids = torch.zeros(val.shape).to(torch.int).to(device)\n",
    "\n",
    "        # Make attention mask\n",
    "        attention_mask = padding_mask_dict[f\"{key}_revert_padding_mask\"]\n",
    "        mask_for_global_token = torch.ones(attention_mask.shape[0], 1).to(device)\n",
    "        attention_mask = torch.cat([attention_mask, mask_for_global_token], dim=-1)\n",
    "\n",
    "        # Embed data\n",
    "        inputs = {\"input_ids\":val, \"token_type_ids\":token_type_ids, \"attention_mask\":attention_mask}\n",
    "        embedding = self.nlp_model(**inputs).last_hidden_state\n",
    "        embedding = self.downsize_linear(embedding)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class TemporalRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, remain_rto, temporal_cols, device):\n",
    "        result_dict, idx_dict = {}, {}\n",
    "        # Concat data\n",
    "        concat_data_li = []\n",
    "        for col in temporal_cols:\n",
    "            concat_data_li.append(data_dict[col])\n",
    "        \n",
    "        concat_data = torch.stack(concat_data_li, dim=-2)\n",
    "    \n",
    "        # Remain mask\n",
    "        num_modality = concat_data.shape[-2]\n",
    "        num_remain = int(num_modality * remain_rto)\n",
    "        \n",
    "        noise = torch.rand(concat_data.shape[:-1]).to(device)\n",
    "        shuffle_idx = torch.argsort(noise, dim=-1)\n",
    "\n",
    "        remain_idx = shuffle_idx[:, :, :num_remain]\n",
    "        masked_idx = shuffle_idx[:, :, num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle_idx, dim=-1)\n",
    "\n",
    "        # Apply mask\n",
    "        concat_data = torch.gather(concat_data, index=remain_idx.unsqueeze(-1).repeat(1, 1, 1, concat_data.shape[-1]), dim=-2)\n",
    "\n",
    "        result_dict[\"temporal\"] = concat_data\n",
    "        idx_dict.update({\"temporal_remain_idx\":remain_idx, \"temporal_masked_idx\":masked_idx, \"temporal_revert_idx\":revert_idx})\n",
    "\n",
    "        return result_dict, idx_dict\n",
    "\n",
    "class ImgRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, remain_rto, img_cols, device):\n",
    "        result_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        # Get indexs\n",
    "        for col in img_cols:\n",
    "            val = data_dict[col]\n",
    "            num_remain = int(val.shape[1] * remain_rto)\n",
    "            noise = torch.rand(val.shape[0], val.shape[1]).to(device)\n",
    "            shuffle_idx = torch.argsort(noise, dim=1)\n",
    "\n",
    "            remain_idx = shuffle_idx[:, :num_remain]\n",
    "            masked_idx = shuffle_idx[:, num_remain:]\n",
    "            revert_idx = torch.argsort(shuffle_idx, dim=1)\n",
    "\n",
    "            remain_padding_mask = torch.ones(remain_idx.shape).to(device)\n",
    "            revert_padding_mask = torch.ones(revert_idx.shape).to(device)\n",
    "\n",
    "            # Apply mask\n",
    "            val = torch.gather(val, index=remain_idx.unsqueeze(-1).repeat(1, 1, val.shape[-1]), dim=1)\n",
    "\n",
    "            result_dict[col] = val\n",
    "            idx_dict.update({f\"{col}_remain_idx\":remain_idx, f\"{col}_masked_idx\":masked_idx, f\"{col}_revert_idx\":revert_idx})\n",
    "            padding_mask_dict.update({f\"{col}_remain_padding_mask\":remain_padding_mask, f\"{col}_revert_padding_mask\":revert_padding_mask})\n",
    "\n",
    "        return result_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class NlpRemain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, remain_rto, nlp_cols, device):\n",
    "        result_dict = {}\n",
    "        for col in nlp_cols:\n",
    "            val = data_dict[col]\n",
    "            remain_idx = idx_dict[f\"{col}_remain_idx\"].unsqueeze(-1).repeat(1, 1, val.shape[-1])\n",
    "            val = torch.gather(val, index=remain_idx, dim=1)\n",
    "            result_dict[col] = val\n",
    "        return result_dict\n",
    "\n",
    "\n",
    "class TemporalEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadBlockSelfAttention(d_model, nhead, dropout)\n",
    "\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        # Feed forward\n",
    "        if activation == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        if activation == \"gelu\":\n",
    "            self.activation = torch.nn.GELU()\n",
    "        \n",
    "        self.linear_ff1 = torch.nn.Linear(d_model, d_ff)\n",
    "        self.linear_ff2 = torch.nn.Linear(d_ff, d_model)\n",
    "        self.dropout_ff1 = torch.nn.Dropout(dropout)\n",
    "        self.dropout_ff2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n",
    "        attn_output, attn_weight = self._sa_block(self.norm1(src), temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "        x = x + self_attn_output\n",
    "\n",
    "        x = x + self._ff_block(self.norm2(x))\n",
    "        return x, attn_weight\n",
    "\n",
    "    def _sa_block(self, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n",
    "        x, attn_weight = self.self_attn(src, src, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "        return self.dropout1(x), attn_weight\n",
    "\n",
    "    def _ff_block(self, x):\n",
    "        x = self.linear_ff2(self.dropout_ff1(self.activation(self.linear_ff1(x))))\n",
    "        return self.dropout_ff2(x)\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  8],\n",
       "        [16, 17, 18],\n",
       "        [26, 27, 28]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = torch.tensor([\n",
    "    [1,2,3,4,5,6,7,8,9,10],\n",
    "    [11,12,13,14,15,16,17,18,19,20],\n",
    "    [21,22,23,24,25,26,27,28,29,30]\n",
    "])\n",
    "\n",
    "temporal = torch.tensor([0,1,2,3,4]).unsqueeze(0).repeat(arr.shape[0], 1)\n",
    "img = torch.tensor([5,6,7]).unsqueeze(0).repeat(arr.shape[0], 1)\n",
    "nlp = torch.tensor([8,9]).unsqueeze(0).repeat(arr.shape[0], 1)\n",
    "\n",
    "print(arr.shape)\n",
    "print(temporal.shape)\n",
    "\n",
    "torch.gather(arr, index=img, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, col, data_info, label_encoder_dict, d_model):\n",
    "        super().__init__()\n",
    "        if col in data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"]:\n",
    "            self.embedding = TemporalEmbedding(col, data_info, label_encoder_dict, d_model)\n",
    "        elif col in data_info.modality_info[\"img\"]:\n",
    "            self.embedding = ImgEmbedding(d_model)\n",
    "        elif col in data_info.modality_info[\"nlp\"]:\n",
    "            self.embedding = NlpEmbedding(d_model)\n",
    "    \n",
    "    def forward(self, key, val, padding_mask, device):\n",
    "        return self.embedding(key, val, padding_mask, device)\n",
    "\n",
    "class PosModEncoding(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc, num_modality, modality, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_enc = pos_enc\n",
    "        self.modality = modality[col]\n",
    "        self.modality_embedding = torch.nn.Embedding(num_modality, d_model)\n",
    "\n",
    "    def forward(self, key, val, device):\n",
    "        # Positional encoding\n",
    "        val = self.pos_enc(val)\n",
    "\n",
    "        # Modality embedding\n",
    "        modality = torch.zeros(val.shape[1]).to(torch.int).to(device) + self.modality\n",
    "        modality = self.modality_embedding(modality)\n",
    "\n",
    "        return val + modality\n",
    "\n",
    "class Remain(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temporal_remain = TemporalRemain()\n",
    "        self.img_remain = ImgRemain()\n",
    "        self.nlp_remain = NlpRemain()\n",
    "    \n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, remain_rto, temporal_cols, img_cols, nlp_cols, device):\n",
    "        temporal_dict, temporal_idx_dict = self.temporal_remain(data_dict, remain_rto[\"temporal\"], temporal_cols, device)\n",
    "        idx_dict.update(temporal_idx_dict)\n",
    "\n",
    "        img_dict, img_idx_dict, img_padding_mask_dict = self.img_remain(data_dict, remain_rto[\"img\"], img_cols, device)\n",
    "        idx_dict.update(img_idx_dict)\n",
    "        padding_mask_dict.update(img_padding_mask_dict)\n",
    "\n",
    "        nlp_dict = self.nlp_remain(data_dict, idx_dict, remain_rto[\"nlp\"], nlp_cols, device)\n",
    "\n",
    "        return temporal_dict, img_dict, nlp_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout, activation, num_layers):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = torch.nn.ModuleList([copy.deepcopy(TemporalEncoderLayer(d_model, nhead, d_ff, dropout, activation)) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li = self.get_src(temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device)\n",
    "        for mod in self.encoder_layers:\n",
    "            src, attn_weight = mod(src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n",
    "    \n",
    "    def get_src(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        # Temporal\n",
    "        temporal = temporal_dict[\"temporal\"]\n",
    "        temporal_shape = temporal.shape\n",
    "        batch_size, seq_len, _, d_model = temporal_shape\n",
    "        temporal = temporal.view(batch_size, -1, d_model)\n",
    "        temporal_padding_mask = torch.ones(temporal.shape[:-1]).to(device)\n",
    "        temporal_idx = torch.arange(0, temporal.shape[1]).to(device)\n",
    "\n",
    "        # Image\n",
    "        idx = temporal_idx[-1]+1\n",
    "        img_li, img_padding_mask_li, img_idx_li = [], [], []\n",
    "        for col in img_cols:\n",
    "            img_data = img_dict[col]\n",
    "            img_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "\n",
    "            img_li.append(img_data)\n",
    "            img_padding_mask_li.append(img_padding_mask)\n",
    "            img_idx_li.append(torch.arange(idx, idx+img_data.shape[1]).to(device))\n",
    "            idx += img_data.shape[1]\n",
    "\n",
    "        img = torch.cat(img_li, dim=1)\n",
    "        img_padding_mask = torch.cat(img_padding_mask_li, dim=1)\n",
    "\n",
    "        # Nlp\n",
    "        nlp_li, nlp_padding_mask_li, nlp_idx_li = [], [], []\n",
    "        for col in nlp_cols:\n",
    "            nlp_data = nlp_dict[col]\n",
    "            nlp_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "\n",
    "            nlp_li.append(nlp_data)\n",
    "            nlp_padding_mask_li.append(nlp_padding_mask)\n",
    "            nlp_idx_li.append(torch.arange(idx, idx+nlp_data.shape[1]).to(device))\n",
    "            idx += nlp_data.shape[1]\n",
    "\n",
    "        nlp = torch.cat(nlp_li, dim=1)\n",
    "        nlp_padding_mask = torch.cat(nlp_padding_mask_li, dim=1)\n",
    "        \n",
    "        src = torch.cat([temporal, img, nlp], dim=1)\n",
    "        assert idx == src.shape[1], f\"{idx}, {src.shape}\"\n",
    "        \n",
    "        return src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li\n",
    "\n",
    "\n",
    "\n",
    "    def __forward(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        # Temporal\n",
    "        temporal_query, temporal_keyval, temporal_keyval_padding_mask = self.get_temporal_qkv(temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device)\n",
    "        \n",
    "        for mod in self.temporal_encoder_layers:\n",
    "            temporal_query, temporal_keyval = mod(temporal_query, temporal_keyval, temporal_keyval_padding_mask)\n",
    "        return\n",
    "    \n",
    "    def __get_temporal_qkv(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device):\n",
    "        query = temporal_dict[\"temporal\"]\n",
    "\n",
    "        # Get non temporal keyval\n",
    "        non_temporalkeyval_data_li, non_temporal_keyval_padding_mask_li = [], []\n",
    "        for col in img_cols+nlp_cols:\n",
    "            non_temporal_keyval_data = img_dict[col] if col in img_cols else nlp_dict[col]\n",
    "            non_temporal_keyval_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "            non_temporalkeyval_data_li.append(non_temporal_keyval_data)\n",
    "            non_temporal_keyval_padding_mask_li.append(non_temporal_keyval_padding_mask)\n",
    "\n",
    "        non_temporal_keyval_data = torch.cat(non_temporalkeyval_data_li, dim=1)\n",
    "        non_temporal_keyval_padding_mask = torch.cat(non_temporal_keyval_padding_mask_li, dim=1)\n",
    "\n",
    "        # Get temporal keyval\n",
    "        temporal_keyval_data = temporal_dict[\"temporal\"]\n",
    "        temporal_keyval_padding_mask = torch.ones(temporal_keyval_data.shape[:-1]).to(device)\n",
    "\n",
    "        # Get total keyval\n",
    "        non_temporal_keyval_data = non_temporal_keyval_data.unsqueeze(1).repeat(1, query.shape[1], 1, 1)\n",
    "        non_temporal_keyval_padding_mask = torch.cat(non_temporal_keyval_padding_mask_li, dim=1).unsqueeze(1).repeat(1, query.shape[1], 1)\n",
    "\n",
    "        total_keyval_data = torch.cat([temporal_keyval_data, non_temporal_keyval_data], dim=-2)\n",
    "        total_keyval_padding_mask = torch.cat([temporal_keyval_padding_mask, non_temporal_keyval_padding_mask], dim=-1)\n",
    "\n",
    "        return query, total_keyval_data, total_keyval_padding_mask\n",
    "\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict,\n",
    "                d_model, num_layers, nhead, d_ff, dropout, activation):\n",
    "        super().__init__()\n",
    "        self.data_info, self.label_encoder_dict = data_info, label_encoder_dict\n",
    "        self.temporal_cols, self.img_cols, self.nlp_cols, self.total_cols = self.define_cols()\n",
    "        self.self_attn_weight_dict, self.cross_attn_weight_dict = {}, {}\n",
    "        \n",
    "        # 1. Embedding\n",
    "        self.embedding_dict = self.init_process(mod=Embedding, args=[self.data_info, self.label_encoder_dict, d_model[\"encoder\"]])\n",
    "        # 2. Pos encoding and modality embedding\n",
    "        num_modality = len(self.total_cols)\n",
    "        modality = {col:n for n, col in enumerate(self.total_cols)}\n",
    "        encoder_pos_enc = PositionalEncoding(d_model[\"encoder\"], dropout)\n",
    "        self.pos_mod_encoding_dict = self.init_process(mod=PosModEncoding, args=[encoder_pos_enc, num_modality, modality, d_model[\"encoder\"]])\n",
    "        # 3. Remain mask\n",
    "        self.remain_dict = Remain()\n",
    "        # 4. Encoding\n",
    "        self.encoding = Encoder(d_model[\"encoder\"], nhead, d_ff[\"encoder\"], dropout, activation, num_layers[\"encoder\"])\n",
    "\n",
    "    def forward(self, data_input, remain_rto, device):\n",
    "        data_dict, self.idx_dict, self.padding_mask_dict = self.to_gpu(data_input, device)\n",
    "        \n",
    "        # 1. Embedding\n",
    "        embedding_dict = self.apply_process(data=data_dict, mod=self.embedding_dict, args=[self.padding_mask_dict, device])\n",
    "        # 2. Pos encoding and modality embedding\n",
    "        pos_mod_encoding_dict = self.apply_process(data=embedding_dict, mod=self.pos_mod_encoding_dict, args=[device])\n",
    "        # 3. Remain mask\n",
    "        temporal_dict, img_dict, nlp_dict, self.idx_dict, self.padding_mask_dict = self.remain_dict(pos_mod_encoding_dict, self.idx_dict, self.padding_mask_dict, remain_rto, self.temporal_cols, self.img_cols, self.nlp_cols, device)\n",
    "        # 4. Encoding\n",
    "        encoding = self.encoding(temporal_dict, img_dict, nlp_dict, self.padding_mask_dict, self.img_cols, self.nlp_cols, device)\n",
    "\n",
    "\n",
    "    \n",
    "    def define_cols(self):\n",
    "        temporal_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        img_cols = self.data_info.modality_info[\"img\"]\n",
    "        nlp_cols = self.data_info.modality_info[\"nlp\"]\n",
    "        total_cols = temporal_cols + img_cols + nlp_cols\n",
    "\n",
    "        return temporal_cols, img_cols, nlp_cols, total_cols\n",
    "\n",
    "    def to_gpu(self, data_input, device):\n",
    "        data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "        for key, val in data_input.items():\n",
    "            if key in self.temporal_cols + self.img_cols + self.nlp_cols:\n",
    "                data_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"idx\"):\n",
    "                idx_dict[key] = data_input[key].to(device)\n",
    "            elif key.endswith(\"padding_mask\"):\n",
    "                padding_mask_dict[key] = data_input[key].to(device)\n",
    "            \n",
    "        return data_dict, idx_dict, padding_mask_dict\n",
    "\n",
    "    def init_process(self, mod, args=[], target_cols=None):\n",
    "        result_dict = {}\n",
    "        target_cols = self.total_cols if target_cols is None else target_cols\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod(col, *args)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def apply_process(self, data, mod, args=[], target_cols=None, collate_fn=None):\n",
    "        result_dict = {}\n",
    "        target_cols = self.total_cols if target_cols is None else target_cols\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = mod[col](col, data[col], *args)\n",
    "        \n",
    "        if collate_fn is not None:\n",
    "            return collate_fn(result_dict)\n",
    "        else:\n",
    "            return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal_attn_output: torch.Size([16, 100, 4, 256])\n",
      "temporal_attn_weight: torch.Size([16, 4, 100, 4, 67])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(data_info, train_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder_dict,\n\u001b[1;32m      2\u001b[0m                         d_model, num_layers, nhead, d_ff, dropout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremain_rto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_parent_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_model_summary/model_summary.py:128\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m model_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_training:\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, data_input, remain_rto, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m temporal_dict, img_dict, nlp_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremain_dict(pos_mod_encoding_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mask_dict, remain_rto, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_cols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_cols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_cols, device)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# 4. Encoding\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemporal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_src(temporal_dict, img_dict, nlp_dict, padding_mask_dict, img_cols, nlp_cols, device)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m---> 58\u001b[0m     src, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_idx_li\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_idx_li\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 146\u001b[0m, in \u001b[0;36mTemporalEncoderLayer.forward\u001b[0;34m(self, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n\u001b[0;32m--> 146\u001b[0m     attn_output, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_idx_li\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_idx_li\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m self_attn_output\n\u001b[1;32m    149\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n",
      "Cell \u001b[0;32mIn[7], line 153\u001b[0m, in \u001b[0;36mTemporalEncoderLayer._sa_block\u001b[0;34m(self, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li):\n\u001b[0;32m--> 153\u001b[0m     x, attn_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(src, src, src, temporal_shape, temporal_padding_mask, img_padding_mask, nlp_padding_mask, temporal_idx, img_idx_li, nlp_idx_li)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x), attn_weight\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "model = Transformer(data_info, train_dataset.label_encoder_dict,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout, \"gelu\")\n",
    "model.to(device)\n",
    "summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"img_model\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"nlp_model\" in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def patchify(imgs, patch_size=16):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size=16):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
