{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "from rawdata import RawData, Preprocess\n",
    "from data import DataInfo, Dataset, collate_fn\n",
    "from data import NoneScaler, LogScaler, CustomLabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mode = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 365\n",
    "PRED_LEN = 100\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"others\": [\"sales_channel_id\", \n",
    "                \"prod_name\", \"product_type_name\", \"product_group_name\", \n",
    "                \"graphical_appearance_name\", \"colour_group_name\", \n",
    "                \"perceived_colour_value_name\", \"perceived_colour_master_name\", \n",
    "                \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]}\n",
    "\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"sales_channel_id\", \n",
    "                        \"day\", \"dow\", \"month\", \"holiday\",\n",
    "                        \"prod_name\", \"product_type_name\", \"product_group_name\", \n",
    "                        \"graphical_appearance_name\", \"colour_group_name\", \n",
    "                        \"perceived_colour_value_name\", \"perceived_colour_master_name\", \n",
    "                        \"department_name\", \"index_name\", \"index_group_name\", \"section_name\", \"garment_group_name\"]}\n",
    "\n",
    "# modality_info = {\n",
    "#     \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "#     \"target\": [\"sales\"],\n",
    "#     \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "#     \"others\": []}\n",
    "# processing_info = {\n",
    "#     \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "#     \"embedding_cols\": [\"day\", \"dow\", \"month\", \"holiday\"]}\n",
    "\n",
    "# Model\n",
    "batch_size = 32\n",
    "dropout = 0.1\n",
    "\n",
    "nhead = {\"encoder\":4, \"decoder\":4}\n",
    "d_model = {\"encoder\":64, \"decoder\":32}\n",
    "d_ff = {\"encoder\":64, \"decoder\":32}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"temporal\":0.25, \"others\":0.25}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_mode:\n",
    "    df_preprocessed = pd.read_parquet(\"src/df_preprocessed_test.parquet\")\n",
    "else:\n",
    "    if not is_prep_data_exist:\n",
    "        rawdata = RawData()\n",
    "        df_trans, df_meta, df_holiday = rawdata.get_raw_data()\n",
    "        preprocess = Preprocess(df_trans, df_meta, df_holiday)\n",
    "        df_preprocessed = preprocess.main()\n",
    "    else:\n",
    "        df_preprocessed = pd.read_parquet(\"src/df_preprocessed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1)]\n",
    "df_valid = df_preprocessed[(df_preprocessed[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN) & (df_preprocessed[\"time_idx\"] <= MAX_SEQ_LEN-1 + PRED_LEN)]\n",
    "\n",
    "data_info = DataInfo(modality_info, processing_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3252.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([2, 365, 1])\n",
      "sales_remain_idx torch.Size([2, 91])\n",
      "sales_masked_idx torch.Size([2, 274])\n",
      "sales_revert_idx torch.Size([2, 365])\n",
      "sales_remain_padding_mask torch.Size([2, 91])\n",
      "sales_masked_padding_mask torch.Size([2, 274])\n",
      "sales_revert_padding_mask torch.Size([2, 365])\n",
      "day torch.Size([2, 365])\n",
      "day_remain_idx torch.Size([2, 91])\n",
      "day_masked_idx torch.Size([2, 274])\n",
      "day_revert_idx torch.Size([2, 365])\n",
      "day_remain_padding_mask torch.Size([2, 91])\n",
      "day_masked_padding_mask torch.Size([2, 274])\n",
      "day_revert_padding_mask torch.Size([2, 365])\n",
      "dow torch.Size([2, 365])\n",
      "dow_remain_idx torch.Size([2, 91])\n",
      "dow_masked_idx torch.Size([2, 274])\n",
      "dow_revert_idx torch.Size([2, 365])\n",
      "dow_remain_padding_mask torch.Size([2, 91])\n",
      "dow_masked_padding_mask torch.Size([2, 274])\n",
      "dow_revert_padding_mask torch.Size([2, 365])\n",
      "month torch.Size([2, 365])\n",
      "month_remain_idx torch.Size([2, 91])\n",
      "month_masked_idx torch.Size([2, 274])\n",
      "month_revert_idx torch.Size([2, 365])\n",
      "month_remain_padding_mask torch.Size([2, 91])\n",
      "month_masked_padding_mask torch.Size([2, 274])\n",
      "month_revert_padding_mask torch.Size([2, 365])\n",
      "holiday torch.Size([2, 365])\n",
      "holiday_remain_idx torch.Size([2, 91])\n",
      "holiday_masked_idx torch.Size([2, 274])\n",
      "holiday_revert_idx torch.Size([2, 365])\n",
      "holiday_remain_padding_mask torch.Size([2, 91])\n",
      "holiday_masked_padding_mask torch.Size([2, 274])\n",
      "holiday_revert_padding_mask torch.Size([2, 365])\n",
      "price torch.Size([2, 365, 1])\n",
      "price_remain_idx torch.Size([2, 91])\n",
      "price_masked_idx torch.Size([2, 274])\n",
      "price_revert_idx torch.Size([2, 365])\n",
      "price_remain_padding_mask torch.Size([2, 91])\n",
      "price_masked_padding_mask torch.Size([2, 274])\n",
      "price_revert_padding_mask torch.Size([2, 365])\n",
      "sales_channel_id torch.Size([2, 1])\n",
      "prod_name torch.Size([2, 1])\n",
      "product_type_name torch.Size([2, 1])\n",
      "product_group_name torch.Size([2, 1])\n",
      "graphical_appearance_name torch.Size([2, 1])\n",
      "colour_group_name torch.Size([2, 1])\n",
      "perceived_colour_value_name torch.Size([2, 1])\n",
      "perceived_colour_master_name torch.Size([2, 1])\n",
      "department_name torch.Size([2, 1])\n",
      "index_name torch.Size([2, 1])\n",
      "index_group_name torch.Size([2, 1])\n",
      "section_name torch.Size([2, 1])\n",
      "garment_group_name torch.Size([2, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info))\n",
    "\n",
    "for data in train_dataloader:\n",
    "    [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_positional_encoding(d_model, seq_len=1000):\n",
    "    position = torch.arange(seq_len).reshape(-1,1)\n",
    "    i = torch.arange(d_model)//2\n",
    "    exp_term = 2*i/d_model\n",
    "    div_term = torch.pow(10000, exp_term).reshape(1, -1)\n",
    "    pos_encoded = position / div_term\n",
    "\n",
    "    pos_encoded[:, 0::2] = torch.sin(pos_encoded[:, 0::2])\n",
    "    pos_encoded[:, 1::2] = torch.cos(pos_encoded[:, 1::2])\n",
    "\n",
    "    return pos_encoded\n",
    "\n",
    "class NumericalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear_embedding = torch.nn.Linear(1, d_model)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        return self.linear_embedding(data)\n",
    "\n",
    "class CategoricalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_cls, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_cls, d_model)\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.embedding(data)\n",
    "\n",
    "class TemporalRemain(torch.nn.Module):\n",
    "    def __init__(self, col, pos_enc, global_token):\n",
    "        super().__init__()\n",
    "        self.col, self.pos_enc, self.global_token = col, pos_enc, global_token\n",
    "    \n",
    "    def forward(self, data, idx_dict):\n",
    "        # Apply positional encoding\n",
    "        pos_enc = self.pos_enc.unsqueeze(0).repeat(data.shape[0], 1, 1)\n",
    "        data += pos_enc[:, 1:data.shape[1]+1, :]\n",
    "\n",
    "        # Apply remain\n",
    "        remain_idx = idx_dict[f\"{self.col}_remain_idx\"]\n",
    "        remain_idx = remain_idx.unsqueeze(-1).repeat(1, 1, data.shape[-1])\n",
    "        data = torch.gather(data, index=remain_idx, dim=1)\n",
    "\n",
    "        # Apply global token\n",
    "        global_token = self.global_token.unsqueeze(0).repeat(data.shape[0], 1, 1)\n",
    "        global_token += pos_enc[:, :1, :]\n",
    "\n",
    "        data = torch.cat([global_token, data], dim=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "class ModalityEmbedding(torch.nn.Module):\n",
    "    def __init__(self, modality_embedding, idx, is_others):\n",
    "        super().__init__()\n",
    "        self.modality_embedding, self.idx, self.is_others = modality_embedding, idx, is_others\n",
    "        self.idx = idx\n",
    "        self.is_others = is_others\n",
    "\n",
    "    def forward(self, data, modality):\n",
    "        if not self.is_others:\n",
    "            modality = modality[self.idx].unsqueeze(0).repeat(data.shape[0], data.shape[1])\n",
    "        else:\n",
    "            modality = modality[self.idx].unsqueeze(0).repeat(data.shape[0], 1)\n",
    "        modality_embedding = self.modality_embedding(modality)\n",
    "        \n",
    "        return data + modality_embedding\n",
    "\n",
    "class OthersRemain(torch.nn.Module):\n",
    "    def __init__(self, data_info, remain_rto, device):\n",
    "        super().__init__()\n",
    "        self.data_info = data_info\n",
    "        self.num_remain = int(len(self.data_info.modality_info[\"others\"]) * remain_rto)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, data, idx_dict):\n",
    "        temporal_dict = {key:val for key, val in data.items() if key in self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]}\n",
    "        others_dict = {key:val for key, val in data.items() if key in self.data_info.modality_info[\"others\"]}\n",
    "\n",
    "        others_cat = torch.cat(list(others_dict.values()), dim=1)\n",
    "        noise = torch.rand(others_cat.shape[:2]).to(device)\n",
    "        shuffle = torch.argsort(noise, dim=-1)\n",
    "\n",
    "        remain_idx = shuffle[:, :self.num_remain]\n",
    "        masked_idx = shuffle[:, self.num_remain:]\n",
    "        revert_idx = torch.argsort(shuffle, dim=-1)\n",
    "\n",
    "        others_data = torch.gather(others_cat, index=remain_idx.unsqueeze(-1).repeat(1, 1, others_cat.shape[-1]), dim=1)\n",
    "        idx_dict.update({\"others_remain_idx\": remain_idx, \"others_masked_idx\":masked_idx, \"others_revert_idx\":revert_idx})\n",
    "\n",
    "        return temporal_dict, others_data, idx_dict\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, data_info, d_model, nhead, dim_feedforward, dropout, batch_first, activation, norm_first, num_layers):\n",
    "        super().__init__()\n",
    "        self.data_info = data_info\n",
    "        self.encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first, activation=activation, norm_first=norm_first), num_layers=num_layers)\n",
    "\n",
    "    def forward(self, temporal_dict, others_data, padding_mask_dict, idx_dict):\n",
    "        padding_mask = []\n",
    "\n",
    "        # Temporal padding mask\n",
    "        for col in temporal_dict.keys():\n",
    "            remain_padding_mask = padding_mask_dict[f\"{col}_remain_padding_mask\"]\n",
    "            new_remain_padding_mask = torch.ones(remain_padding_mask.shape[0], remain_padding_mask.shape[1]+1)\n",
    "            new_remain_padding_mask[:, 1:] = remain_padding_mask\n",
    "\n",
    "            padding_mask.append(new_remain_padding_mask)\n",
    "        temporal_padding_mask = torch.cat(padding_mask, dim=1)\n",
    "\n",
    "        # Others padding mask\n",
    "        if len(self.data_info.modality_info[\"others\"]) > 0:\n",
    "            others_padding_mask = padding_mask_dict[\"others_padding_mask\"].unsqueeze(0).repeat(idx_dict[\"others_remain_idx\"].shape[0], 1)\n",
    "            others_padding_mask = others_padding_mask[:, :idx_dict[\"others_remain_idx\"].shape[-1]]        \n",
    "        \n",
    "        # Total padding_mask\n",
    "        total_padding_mask = torch.cat([temporal_padding_mask.to(device), others_padding_mask], dim=1)\n",
    "        print(total_padding_mask.shape)\n",
    "\n",
    "\n",
    "        temporal_data = torch.cat(list(temporal_dict.values()), dim=1)\n",
    "        total_data = torch.cat([temporal_data, others_data], dim=1)\n",
    "        print(total_data.shape)\n",
    "        raise\n",
    "\n",
    "        encoded = self.encoder(total_data, src_key_padding_mask=encoder_padding_mask)\n",
    "\n",
    "        \n",
    "        return\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, data_info, label_encoder_dict, remain_rto,\n",
    "                    d_model, num_layers, nhead, d_ff, dropout, activation,\n",
    "                    device):\n",
    "        super().__init__()\n",
    "        self.data_info, self.label_encoder_dict = data_info, label_encoder_dict\n",
    "\n",
    "        # 1. Embedding\n",
    "        self.embedding_dict = self._init_embedding(d_model[\"encoder\"]) \n",
    "        \n",
    "        # 2. Apply temporal remain\n",
    "        encoder_pos_enc = torch.nn.Parameter(get_positional_encoding(d_model[\"encoder\"]), requires_grad=True)\n",
    "        global_token = torch.nn.Parameter(torch.rand(1, d_model[\"encoder\"]), requires_grad=True)\n",
    "        self.temporal_remain_dict = self._init_temporal_remain(encoder_pos_enc, global_token)\n",
    "\n",
    "        # 3. Apply modality embedding\n",
    "        n_modality = len(self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"] + self.data_info.modality_info[\"others\"])\n",
    "        encoder_modality_embedding = torch.nn.Embedding(n_modality, d_model[\"encoder\"])\n",
    "        self.modality_embedding_dict = self._init_modality_embedding(encoder_modality_embedding)\n",
    "\n",
    "        # 4. Apply others remain\n",
    "        self.others_remain = OthersRemain(data_info, remain_rto[\"others\"], device)\n",
    "\n",
    "        # 5. Encoding\n",
    "        self.encoder = Encoder(self.data_info, d_model=d_model[\"encoder\"], nhead=nhead[\"encoder\"], dim_feedforward=d_ff[\"encoder\"], dropout=dropout, batch_first=True, activation=activation, norm_first=True, num_layers=num_layers[\"encoder\"])\n",
    "\n",
    "    def forward(self, data_dict, idx_dict, padding_mask_dict, modality):\n",
    "        # 1. Embedding\n",
    "        data_dict = self._apply_process(data_dict, self.embedding_dict)\n",
    "\n",
    "        # 2. Apply temporal remain\n",
    "        data_dict.update(self._apply_process(data_dict, self.temporal_remain_dict, idx_dict=idx_dict))\n",
    "        \n",
    "        # 3. Apply modality embedding\n",
    "        data_dict.update(self._apply_process(data_dict, self.modality_embedding_dict, modality=modality))\n",
    "        \n",
    "        # 4. Apply others remain\n",
    "        if len(self.data_info.modality_info[\"others\"]) > 0:\n",
    "            temporal_dict, others_data, idx_dict = self.others_remain(data_dict, idx_dict)\n",
    "        else:\n",
    "            temporal_dict = data_dict\n",
    "            others_data = torch.tensor([]).to(device)\n",
    "\n",
    "        # 5. Encoding\n",
    "        self.encoder(temporal_dict, others_data, padding_mask_dict, idx_dict)\n",
    "        \n",
    "        return data_dict\n",
    "    \n",
    "    \n",
    "    def _init_embedding(self, d_model):\n",
    "        result_dict = {}\n",
    "\n",
    "        # Numerical embedding\n",
    "        numerical_cols = self.data_info.processing_info[\"scaling_cols\"]\n",
    "        for col in numerical_cols:\n",
    "            result_dict[col] = NumericalEmbedding(d_model)\n",
    "\n",
    "        # Categorical embedding\n",
    "        categorical_cols = self.data_info.processing_info[\"embedding_cols\"]\n",
    "        for col in categorical_cols:\n",
    "            num_cls = self.label_encoder_dict[col].get_num_cls()\n",
    "            result_dict[col] = CategoricalEmbedding(num_cls, d_model)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "    \n",
    "    def _init_temporal_remain(self, pos_enc, global_token):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"]\n",
    "        for col in target_cols:\n",
    "            result_dict[col] = TemporalRemain(col, pos_enc, global_token)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "    def _init_modality_embedding(self, modality_embedding):\n",
    "        result_dict = {}\n",
    "        target_cols = self.data_info.modality_info[\"target\"] + self.data_info.modality_info[\"temporal\"] + self.data_info.modality_info[\"others\"]\n",
    "        for idx, col in enumerate(target_cols):\n",
    "            is_others = True if col in self.data_info.modality_info[\"others\"] else False\n",
    "            result_dict[col] = ModalityEmbedding(modality_embedding, idx, is_others)\n",
    "        \n",
    "        return torch.nn.ModuleDict(result_dict)\n",
    "\n",
    "\n",
    "    def _apply_process(self, data_dict, module, **kwargs):\n",
    "        result_dict = {}\n",
    "\n",
    "        for key, mod in module.items():\n",
    "            result_dict[key] = mod(data_dict[key], **kwargs)\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 555])\n",
      "torch.Size([2, 555, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(data_info, train_dataset\u001b[38;5;241m.\u001b[39mlabel_encoder_dict, remain_rto,\n\u001b[1;32m     24\u001b[0m                         d_model, num_layers, nhead, d_ff, dropout, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m                         device)\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_parent_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/pytorch_model_summary/model_summary.py:128\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, batch_size, show_input, show_hierarchical, print_summary, max_depth, show_parent_layers, *inputs)\u001b[0m\n\u001b[1;32m    125\u001b[0m model_training \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    127\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 128\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_training:\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[25], line 45\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, data_dict, idx_dict, padding_mask_dict, modality)\u001b[0m\n\u001b[1;32m     42\u001b[0m     others_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 5. Encoding\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemporal_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mothers_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "Cell \u001b[0;32mIn[24], line 124\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, temporal_dict, others_data, padding_mask_dict, idx_dict)\u001b[0m\n\u001b[1;32m    122\u001b[0m total_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([temporal_data, others_data], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(total_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    126\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(total_data, src_key_padding_mask\u001b[38;5;241m=\u001b[39mencoder_padding_mask)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def to_gpu(data, data_info, device):\n",
    "    data_dict, idx_dict, padding_mask_dict = {}, {}, {}\n",
    "\n",
    "    for col in data.keys():\n",
    "        # Temporal and others data\n",
    "        if col in (data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"] + data_info.modality_info[\"others\"]):\n",
    "            data_dict[col] = data[col].to(device)\n",
    "        elif col.endswith(\"idx\"):\n",
    "            idx_dict[col] = data[col].to(device)\n",
    "        elif col.endswith(\"padding_mask\"):\n",
    "            padding_mask_dict[col] = data[col]\n",
    "\n",
    "        # Modality data\n",
    "        n_modality = len(data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"] + data_info.modality_info[\"others\"])\n",
    "        modality = torch.arange(n_modality).to(device)\n",
    "\n",
    "        # Others padding_mask\n",
    "        others_padding_mask = torch.ones(len(data_info.modality_info[\"others\"])).to(device)\n",
    "        padding_mask_dict[\"others_padding_mask\"] = others_padding_mask\n",
    "        \n",
    "    return data_dict, idx_dict, padding_mask_dict, modality\n",
    "\n",
    "model = Transformer(data_info, train_dataset.label_encoder_dict, remain_rto,\n",
    "                        d_model, num_layers, nhead, d_ff, dropout, \"gelu\",\n",
    "                        device)\n",
    "model.to(device)\n",
    "summary(model, *to_gpu(data, data_info, device), show_parent_layers=True, print_summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
