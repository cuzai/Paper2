{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Torch-related\n",
    "import torch\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "# Custom defined\n",
    "from libs.raw_data import *\n",
    "from libs.dataset import *\n",
    "from architecture.architecture import *\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test_mode = False\n",
    "mode = \"pre-train\"\n",
    "# mode = \"fine_tuning\"\n",
    "is_from_pretrained = True\n",
    "\n",
    "# Raw data\n",
    "is_prep_data_exist = True\n",
    "\n",
    "# Data loader\n",
    "MIN_MEANINGFUL_SEQ_LEN = 100\n",
    "MAX_SEQ_LEN = 100\n",
    "PRED_LEN = 50\n",
    "\n",
    "modality_info = {\n",
    "    \"group\": [\"article_id\", \"sales_channel_id\"],\n",
    "    \"target\": [\"sales\"],\n",
    "    \"temporal\": [\"day\", \"dow\", \"month\", \"holiday\", \"price\"],\n",
    "    \"img\": [\"img_path\"],\n",
    "    \"nlp\": [\"detail_desc\"]\n",
    "}\n",
    "processing_info = {\n",
    "    \"scaling_cols\": {\"sales\": StandardScaler, \"price\": StandardScaler},\n",
    "    \"embedding_cols\": [\"day\",  \"dow\", \"month\", \"holiday\"],\n",
    "    \"img_cols\": [\"img_path\"],\n",
    "    \"nlp_cols\": [\"detail_desc\"]\n",
    "}\n",
    "\n",
    "# Model\n",
    "batch_size = 16\n",
    "nhead = 4\n",
    "dropout = 0.1\n",
    "patch_size = 16\n",
    "\n",
    "d_model = {\"encoder\":64, \"decoder\":32}\n",
    "d_ff = {\"encoder\":64, \"decoder\":32}\n",
    "num_layers = {\"encoder\":2, \"decoder\":2}\n",
    "remain_rto = {\"temporal\":0.25, \"img\":0.25, \"nlp\":0.25}\n",
    "# remain_rto = {\"temporal\":1., \"img\":1., \"nlp\":1.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = DataInfo(modality_info, processing_info)\n",
    "df_prep = get_raw_data(is_test_mode, is_prep_data_exist)\n",
    "\n",
    "if mode == \"pre-train\":\n",
    "    df_train = df_prep[(df_prep[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN)\n",
    "                        &(df_prep[\"time_idx\"] <= MAX_SEQ_LEN-1)\n",
    "                        &(~pd.isna(df_prep[\"detail_desc\"]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"pre-train\":\n",
    "    train_dataset = Dataset(df_train, data_info, remain_rto)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "    joblib.dump(train_dataset.label_encoder_dict, \"./src/label_encoder_dict.pkl\")\n",
    "    for data in train_dataloader:\n",
    "        [print(key, val.shape) for key, val in data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def patchify(imgs, patch_size=16):\n",
    "    \"\"\"\n",
    "    imgs: (N, 3, H, W)\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = w = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "    x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "    x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size=16):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size**2 *3)\n",
    "    imgs: (N, 3, H, W)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = w = int(x.shape[1]**.5)\n",
    "    assert h * w == x.shape[1]\n",
    "    \n",
    "    x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "    x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "    imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def get_loss(output_dict, data_dict, idx_dict, padding_mask_dict, col_info, data_info):\n",
    "    loss_dict = {}\n",
    "    temporal_cols, img_cols, nlp_cols = col_info\n",
    "    \n",
    "    for n, ((key, pred), (key_, y)) in enumerate(zip(output_dict.items(), data_dict.items())):\n",
    "        assert key == key_\n",
    "\n",
    "        # Temporal loss\n",
    "        if key in temporal_cols:\n",
    "            ### Compute loss\n",
    "            if key in data_info.processing_info[\"scaling_cols\"]:\n",
    "                loss = mse_loss(pred, y).squeeze()\n",
    "            elif key in data_info.processing_info[\"embedding_cols\"]:\n",
    "                loss = ce_loss(pred.view(-1, pred.shape[-1]), y.view(-1).to(torch.long))\n",
    "                loss = loss.view(y.shape)\n",
    "            ### Apply mask\n",
    "            padding_mask = padding_mask_dict[\"temporal_padding_mask\"]\n",
    "            masked_idx = idx_dict[\"temporal_block_masked_idx\"]\n",
    "            masking_mask = (masked_idx==1).sum(dim=-1)\n",
    "            total_mask = torch.where((padding_mask==1) & (masking_mask==1), 1, 0)\n",
    "            loss *= total_mask\n",
    "            loss = loss.sum() / total_mask.sum()\n",
    "\n",
    "        # Img loss\n",
    "        elif key in img_cols:\n",
    "            ### Compute loss\n",
    "            pred = pred[:, 1:, :]\n",
    "            y = patchify(y, patch_size)\n",
    "            loss = mse_loss(pred, y)\n",
    "            ### Apply mask\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            masked_idx = masked_idx.unsqueeze(-1).repeat(1, 1, loss.shape[-1])\n",
    "            loss = torch.gather(loss, index=masked_idx, dim=1)\n",
    "            loss = loss.mean()\n",
    "\n",
    "        ### Nlp loss\n",
    "        elif key in nlp_cols:\n",
    "            ### Compute loss\n",
    "            pred = pred[:, 1:, :]\n",
    "            y = y[:, 1:]\n",
    "            loss = ce_loss(pred.reshape(-1, pred.shape[-1]), y.reshape(-1).to(torch.long))\n",
    "            loss = loss.view(y.shape)\n",
    "            ### Apply mask\n",
    "            masked_idx = idx_dict[f\"{key}_masked_idx\"]\n",
    "            masked_loss = torch.gather(loss, index=masked_idx, dim=1)\n",
    "\n",
    "            padding_mask = padding_mask_dict[f\"{key}_masked_padding_mask\"][:, 1:]\n",
    "\n",
    "            loss = masked_loss * padding_mask\n",
    "            loss = loss.sum() / padding_mask.sum()\n",
    "        \n",
    "        loss_dict[key] = loss\n",
    "    \n",
    "    return loss_dict\n",
    "\n",
    "\n",
    "def obtain_loss_dict_for_plot(total_loss, loss_dict, loss_li_dict, mean_loss_li_dict):\n",
    "    loss_li_dict[\"total\"].append(total_loss.item())\n",
    "    mean_loss_li_dict[\"total\"].append(np.array(loss_li_dict[\"total\"]).mean())\n",
    "\n",
    "    for key, val in loss_dict.items():\n",
    "        loss_li_dict[key].append(val.item())\n",
    "        mean_loss_li_dict[key].append(np.array(loss_li_dict[key]).mean())\n",
    "\n",
    "    return loss_li_dict, mean_loss_li_dict\n",
    "\n",
    "def plot_loss_sample(nrows, ncols, idx, mean_loss_li_dict, output_dict, data_dict, col_info, patch_size):\n",
    "    plot_idx = 1\n",
    "    for key, val in mean_loss_li_dict.items():\n",
    "        # Individual loss\n",
    "        plt.subplot(nrows, ncols, plot_idx)\n",
    "        plt.plot(val)\n",
    "        plt.title(f\"{key}: {mean_loss_li_dict[key][-1]}\")\n",
    "        plot_idx += 1\n",
    "\n",
    "        # Sample\n",
    "        temporal_cols, img_cols, nlp_cols = col_info\n",
    "        ### Temporal\n",
    "        if key in temporal_cols:\n",
    "            pred, y = output_dict[key].squeeze(), data_dict[key].squeeze()\n",
    "            if key in data_info.processing_info[\"embedding_cols\"]: pred = torch.argmax(pred, dim=-1)\n",
    "            \n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.plot(y[idx].detach().cpu())\n",
    "            plt.plot(pred[idx].detach().cpu())\n",
    "        ### Img\n",
    "        elif key in img_cols:\n",
    "            pred, y = output_dict[key], data_dict[key]\n",
    "            pred = unpatchify(pred[:, 1:, :], patch_size).permute(0,2,3,1)\n",
    "            y = y.permute(0,2,3,1)\n",
    "\n",
    "            plt.subplot(nrows, ncols, plot_idx)\n",
    "            plt.imshow(y[idx].detach().cpu())\n",
    "            \n",
    "            plt.subplot(nrows, ncols, plot_idx+1)\n",
    "            plt.imshow(pred[idx].detach().cpu())\n",
    "        \n",
    "        plot_idx += 2\n",
    "\n",
    "        \n",
    "    plt.tight_layout()        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pre_train(e, data_loader, optimizer, model, data_info, patch_size):\n",
    "    pbar = tqdm(data_loader)\n",
    "    loss_li_dict, mean_loss_li_dict = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        output_dict, data_dict, idx_dict, padding_mask_dict = model(data, remain_rto, device)\n",
    "\n",
    "        col_info = model.define_col_modalities(data_info)\n",
    "        loss_dict = get_loss(output_dict, data_dict, idx_dict, padding_mask_dict, col_info, data_info)\n",
    "        total_loss = torch.stack(list(loss_dict.values())).sum()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot\n",
    "        if n % 20 == 0:\n",
    "            nrows, ncols, idx = 10, 3, 0\n",
    "            plt.figure(figsize=(15,15))\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            loss_li_dict, mean_loss_li_dict = obtain_loss_dict_for_plot(total_loss, loss_dict, loss_li_dict, mean_loss_li_dict)\n",
    "            plot_loss_sample(nrows, ncols, idx, mean_loss_li_dict, output_dict, data_dict, col_info, patch_size)\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"pre-train\":\n",
    "    # Define model\n",
    "    label_encoder_dict = joblib.load(\"./src/label_encoder_dict.pkl\")\n",
    "    model = MaskedBlockAutoEncoder(data_info, label_encoder_dict,\n",
    "                            d_model, num_layers, nhead, d_ff, dropout, \"gelu\",\n",
    "                            patch_size, is_from_pretrained)\n",
    "    model.to(device)\n",
    "    # summary(model, data, remain_rto, device, show_parent_layers=True, print_summary=True)\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    # if \"img_model\" in name:\n",
    "    #     param.requires_grad = False\n",
    "    # elif \"nlp_model\" in name:\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # Train\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    epoch = 3\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        pre_train(e, train_dataloader, optimizer, model, data_info, patch_size)\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == \"pre-train\":\n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    path = f\"./saved_model_{now}\"\n",
    "    print(path)\n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need Positional Encoding For Forecasters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sh-sungho.park/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "path = './saved_model_2024-04-26 20:43:39.549405'\n",
    "\n",
    "label_encoder_dict = joblib.load(\"./src/label_encoder_dict.pkl\")\n",
    "mbae = MaskedBlockAutoEncoder(data_info, label_encoder_dict,\n",
    "                            d_model, num_layers, nhead, d_ff, dropout, \"gelu\",\n",
    "                            patch_size, is_from_pretrained=True)\n",
    "mbae.load_state_dict(torch.load(path))\n",
    "# mbae_encoder = mbae.mbae_encoder\n",
    "# mbae_encoder.to(device)\n",
    "model = mbae\n",
    "model.to(device)\n",
    "\n",
    "remain_rto = {\"temporal\":1, \"img\":1, \"nlp\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_prep[(df_prep[\"meaningful_size\"] >= MIN_MEANINGFUL_SEQ_LEN)\n",
    "                    &(df_prep[\"time_idx\"] <= MAX_SEQ_LEN-1+PRED_LEN)\n",
    "                    &(~pd.isna(df_prep[\"detail_desc\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm; tqdm.pandas()\n",
    "\n",
    "# Sklearn-related\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# Torch-related\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "transform_img = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "class ValidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, data_info, remain_rto, label_encoder_dict):\n",
    "        super().__init__()\n",
    "        self.data_info, self.remain_rto = data_info, remain_rto\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "        # Fit label encoder\n",
    "        self.label_encoder_dict = label_encoder_dict\n",
    "\n",
    "        # Iterate data\n",
    "        data_li = []\n",
    "        data.groupby(self.data_info.modality_info[\"group\"]).progress_apply(lambda x: data_li.append(x))\n",
    "        self.dataset = tuple(data_li)\n",
    "\n",
    "    def transform_label_encoder(self, data):\n",
    "        result_dict = {}\n",
    "        embedding_cols = self.data_info.processing_info[\"embedding_cols\"]\n",
    "        for col in embedding_cols:\n",
    "            result_dict[col] = self.label_encoder_dict[col].transform(data[col].values)\n",
    "        return result_dict\n",
    "    \n",
    "    def scale_data(self, data):\n",
    "        result_dict = {}\n",
    "        scaling_cols = self.data_info.processing_info[\"scaling_cols\"]\n",
    "        for col, scaler in scaling_cols.items():\n",
    "            scaler = scaler()\n",
    "            result_dict[col] = scaler.fit_transform(data[col].values.reshape(-1,1))\n",
    "            result_dict[f\"{col}_scaler\"] = scaler\n",
    "        return result_dict\n",
    "\n",
    "    def apply_nlp_remain(self, data):\n",
    "        result_dict = {}\n",
    "        nlp_cols = self.data_info.modality_info[\"nlp\"]\n",
    "        remain_rto = self.remain_rto[\"nlp\"]\n",
    "\n",
    "        for col in nlp_cols:\n",
    "            nlp_data = set(data[col].values); assert len(nlp_data) == 1\n",
    "            total_token = self.tokenizer(next(iter(nlp_data)), return_tensors=\"np\")[\"input_ids\"].squeeze()\n",
    "            \n",
    "            # Split global token and valid token\n",
    "            global_token = total_token[:1]\n",
    "            valid_token = total_token[1:]\n",
    "            \n",
    "            # Get remain/masked/revert indices\n",
    "            valid_token_shape = valid_token.shape; assert len(valid_token_shape)==1\n",
    "            \n",
    "            num_remain = int(valid_token_shape[0] * remain_rto)\n",
    "            noise = np.random.rand(valid_token_shape[0])\n",
    "            shuffle_idx = np.argsort(noise)\n",
    "            \n",
    "            remain_idx = shuffle_idx[:num_remain]\n",
    "            masked_idx = shuffle_idx[num_remain:]\n",
    "            revert_idx = np.argsort(shuffle_idx)\n",
    "            \n",
    "            remain_padding_mask = np.ones(remain_idx.shape[0]+1)\n",
    "            masked_padding_mask = np.ones(masked_idx.shape[0]+1)\n",
    "            revert_padding_mask = np.ones(revert_idx.shape[0]+1)\n",
    "\n",
    "            result_dict.update({f\"{col}\":total_token, f\"{col}_raw\":nlp_data,\n",
    "                                f\"{col}_remain_idx\":remain_idx, f\"{col}_masked_idx\":masked_idx, f\"{col}_revert_idx\":revert_idx,\n",
    "                                f\"{col}_remain_padding_mask\":remain_padding_mask, f\"{col}_masked_padding_mask\":masked_padding_mask, f\"{col}_revert_padding_mask\":revert_padding_mask})\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        result_dict = {}\n",
    "        data = self.dataset[idx]\n",
    "        \n",
    "        # Laben encode and scale data\n",
    "        embedding_cols = self.transform_label_encoder(data)\n",
    "        scaling_cols = self.scale_data(data)\n",
    "        result_dict.update(**embedding_cols, **scaling_cols)\n",
    "        \n",
    "        # Temporal forecast/padding mask\n",
    "        target_col = self.data_info.modality_info[\"target\"]\n",
    "        target_fcst_mask = np.ones(data[target_col].shape).squeeze()\n",
    "        target_fcst_mask[-PRED_LEN:] = 0\n",
    "        \n",
    "        temporal_padding_mask  = np.ones(data[target_col].shape).squeeze()\n",
    "        result_dict.update({\"target_fcst_mask\":target_fcst_mask, \"temporal_padding_mask\":temporal_padding_mask})\n",
    "\n",
    "        # Img\n",
    "        img_cols = self.data_info.modality_info[\"img\"]\n",
    "        for col in img_cols:\n",
    "            img_path = set(data[col].values); assert len(img_path) == 1\n",
    "            img_raw = Image.open(next(iter(img_path))).convert(\"RGB\")\n",
    "            result_dict[f\"{col}_raw\"] = img_raw\n",
    "        \n",
    "        # Nlp\n",
    "        nlp_result_dict = self.apply_nlp_remain(data)\n",
    "        result_dict.update(nlp_result_dict)\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "def valid_collate_fn(batch_li, data_info):\n",
    "    result_dict = {}\n",
    "    # Temporal\n",
    "    target_temporal_cols = data_info.modality_info[\"target\"] + data_info.modality_info[\"temporal\"]\n",
    "    for col in target_temporal_cols:\n",
    "        tensor_type = torch.int if col in data_info.processing_info[\"embedding_cols\"] else torch.float\n",
    "        data = [torch.from_numpy(batch[col]).to(tensor_type) for batch in batch_li]\n",
    "        result_dict[col] = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "    ### Temporal masks\n",
    "    target_fcst_mask = [torch.from_numpy(batch[\"target_fcst_mask\"]) for batch in batch_li]\n",
    "    temporal_padding_mask = [torch.from_numpy(batch[\"temporal_padding_mask\"]) for batch in batch_li]\n",
    "    \n",
    "    result_dict[\"target_fcst_mask\"] = torch.nn.utils.rnn.pad_sequence(target_fcst_mask, batch_first=True)\n",
    "    result_dict[\"temporal_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(temporal_padding_mask, batch_first=True)\n",
    "\n",
    "    # Img\n",
    "    img_cols = data_info.modality_info[\"img\"]\n",
    "    for col in img_cols:\n",
    "        img_raw = [batch[f\"{col}_raw\"] for batch in batch_li]\n",
    "        img_data = transform_img(img_raw, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "\n",
    "        result_dict[f\"{col}_raw\"] = img_raw\n",
    "        result_dict[f\"{col}\"] = img_data\n",
    "    \n",
    "    # Nlp\n",
    "    nlp_cols = data_info.modality_info[\"nlp\"]\n",
    "    for col in nlp_cols:\n",
    "        data = [torch.from_numpy(batch[col]).to(torch.int) for batch in batch_li]\n",
    "        data_remain_idx = [torch.from_numpy(batch[f\"{col}_remain_idx\"]).to(torch.int64) for batch in batch_li]\n",
    "        data_masked_idx = [torch.from_numpy(batch[f\"{col}_masked_idx\"]).to(torch.int64) for batch in batch_li]\n",
    "        data_revert_idx = [torch.from_numpy(batch[f\"{col}_revert_idx\"]).to(torch.int64) for batch in batch_li]\n",
    "        data_remain_padding_mask = [torch.from_numpy(batch[f\"{col}_remain_padding_mask\"]).to(tensor_type) for batch in batch_li]\n",
    "        data_masked_padding_mask = [torch.from_numpy(batch[f\"{col}_masked_padding_mask\"]).to(tensor_type) for batch in batch_li]\n",
    "        data_revert_padding_mask = [torch.from_numpy(batch[f\"{col}_revert_padding_mask\"]).to(tensor_type) for batch in batch_li]\n",
    "\n",
    "        result_dict[col] = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "        result_dict[f\"{col}_remain_idx\"] = torch.nn.utils.rnn.pad_sequence(data_remain_idx, batch_first=True)\n",
    "        result_dict[f\"{col}_masked_idx\"] = torch.nn.utils.rnn.pad_sequence(data_masked_idx, batch_first=True)\n",
    "        result_dict[f\"{col}_revert_idx\"] = torch.nn.utils.rnn.pad_sequence(data_revert_idx, batch_first=True)\n",
    "        result_dict[f\"{col}_remain_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_remain_padding_mask, batch_first=True)\n",
    "        result_dict[f\"{col}_masked_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_masked_padding_mask, batch_first=True)\n",
    "        result_dict[f\"{col}_revert_padding_mask\"] = torch.nn.utils.rnn.pad_sequence(data_revert_padding_mask, batch_first=True)\n",
    "\n",
    "        result_dict[f\"{col}_raw\"] = [batch[f\"{col}_raw\"] for batch in batch_li]\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29274/29274 [00:01<00:00, 28429.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales torch.Size([16, 150, 1])\n",
      "day torch.Size([16, 150])\n",
      "dow torch.Size([16, 150])\n",
      "month torch.Size([16, 150])\n",
      "holiday torch.Size([16, 150])\n",
      "price torch.Size([16, 150, 1])\n",
      "target_fcst_mask torch.Size([16, 150])\n",
      "temporal_padding_mask torch.Size([16, 150])\n",
      "img_path torch.Size([16, 3, 224, 224])\n",
      "detail_desc torch.Size([16, 61])\n",
      "detail_desc_remain_idx torch.Size([16, 60])\n",
      "detail_desc_masked_idx torch.Size([16, 0])\n",
      "detail_desc_revert_idx torch.Size([16, 60])\n",
      "detail_desc_remain_padding_mask torch.Size([16, 61])\n",
      "detail_desc_masked_padding_mask torch.Size([16, 1])\n",
      "detail_desc_revert_padding_mask torch.Size([16, 61])\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = ValidDataset(df_valid, data_info, remain_rto, label_encoder_dict)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: valid_collate_fn(x, data_info), pin_memory=True, num_workers=16, prefetch_factor=4)\n",
    "for valid_data in valid_dataloader:\n",
    "    [print(key, val.shape) for key, val in valid_data.items() if \"scaler\" not in key and \"raw\" not in key]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import functional as F\n",
    "\n",
    "# def _generate_square_subsequent_mask(sz, device, dtype):\n",
    "#     return torch.triu(\n",
    "#         torch.full((sz, sz), float('-inf'), dtype=dtype, device=device),\n",
    "#         diagonal=1,\n",
    "#     )\n",
    "\n",
    "# def _get_seq_len(src, batch_first):\n",
    "#     if src.is_nested:\n",
    "#         return None\n",
    "#     else:\n",
    "#         src_size = src.size()\n",
    "#         if len(src_size) == 2:\n",
    "#             # unbatched: S, E\n",
    "#             return src_size[0]\n",
    "#         else:\n",
    "#             # batched: B, S, E if batch_first else S, B, E\n",
    "#             seq_len_pos = 1 if batch_first else 0\n",
    "#             return src_size[seq_len_pos]\n",
    "\n",
    "# def _detect_is_causal_mask(mask, is_causal=None,size=None):\n",
    "#     # Prevent type refinement\n",
    "#     make_causal = (is_causal is True)\n",
    "\n",
    "#     if is_causal is None and mask is not None:\n",
    "#         sz = size if size is not None else mask.size(-2)\n",
    "#         causal_comparison = _generate_square_subsequent_mask(\n",
    "#             sz, device=mask.device, dtype=mask.dtype)\n",
    "\n",
    "#         # Do not use `torch.equal` so we handle batched masks by\n",
    "#         # broadcasting the comparison.\n",
    "#         if mask.size() == causal_comparison.size():\n",
    "#             make_causal = bool((mask == causal_comparison).all())\n",
    "#         else:\n",
    "#             make_causal = False\n",
    "\n",
    "#     return make_causal\n",
    "\n",
    "# class EncoderLayer_(torch.nn.TransformerEncoderLayer):\n",
    "#     def forward(self, src, pos_enc, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
    "#         x = src\n",
    "#         attn_output, attn_weight = self._sa_block(x, pos_enc, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
    "#         x = self.norm1(x + attn_output)\n",
    "#         x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "#         return x, attn_weight\n",
    "\n",
    "#     # self-attention block\n",
    "#     def _sa_block(self, x, pos_enc, attn_mask, key_padding_mask, is_causal=False):\n",
    "#         x, attn_weight = self.self_attn(x+pos_enc, x+pos_enc, x,\n",
    "#                            attn_mask=attn_mask,\n",
    "#                            key_padding_mask=key_padding_mask,\n",
    "#                            need_weights=True, is_causal=is_causal, average_attn_weights=False)\n",
    "#         return self.dropout1(x), attn_weight\n",
    "\n",
    "# class Encoder_(torch.nn.TransformerEncoder):\n",
    "#     def forward(self, src, pos_enc=0, mask=None, src_key_padding_mask=None, is_causal=None):\n",
    "#        ################################################################################################################\n",
    "#         src_key_padding_mask = F._canonical_mask(\n",
    "#             mask=src_key_padding_mask,\n",
    "#             mask_name=\"src_key_padding_mask\",\n",
    "#             other_type=F._none_or_dtype(mask),\n",
    "#             other_name=\"mask\",\n",
    "#             target_type=src.dtype\n",
    "#         )\n",
    "\n",
    "#         mask = F._canonical_mask(\n",
    "#             mask=mask,\n",
    "#             mask_name=\"mask\",\n",
    "#             other_type=None,\n",
    "#             other_name=\"\",\n",
    "#             target_type=src.dtype,\n",
    "#             check_other=False,\n",
    "#         )\n",
    "\n",
    "#         output = src\n",
    "#         convert_to_nested = False\n",
    "#         first_layer = self.layers[0]\n",
    "#         src_key_padding_mask_for_layers = src_key_padding_mask\n",
    "#         why_not_sparsity_fast_path = ''\n",
    "#         str_first_layer = \"self.layers[0]\"\n",
    "#         batch_first = first_layer.self_attn.batch_first\n",
    "#         if not hasattr(self, \"use_nested_tensor\"):\n",
    "#             why_not_sparsity_fast_path = \"use_nested_tensor attribute not present\"\n",
    "#         elif not self.use_nested_tensor:\n",
    "#             why_not_sparsity_fast_path = \"self.use_nested_tensor (set in init) was not True\"\n",
    "#         elif first_layer.training:\n",
    "#             why_not_sparsity_fast_path = f\"{str_first_layer} was in training mode\"\n",
    "#         elif not src.dim() == 3:\n",
    "#             why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "#         elif src_key_padding_mask is None:\n",
    "#             why_not_sparsity_fast_path = \"src_key_padding_mask was None\"\n",
    "#         elif (((not hasattr(self, \"mask_check\")) or self.mask_check)\n",
    "#                 and not torch._nested_tensor_from_mask_left_aligned(src, src_key_padding_mask.logical_not())):\n",
    "#             why_not_sparsity_fast_path = \"mask_check enabled, and src and src_key_padding_mask was not left aligned\"\n",
    "#         elif output.is_nested:\n",
    "#             why_not_sparsity_fast_path = \"NestedTensor input is not supported\"\n",
    "#         elif mask is not None:\n",
    "#             why_not_sparsity_fast_path = \"src_key_padding_mask and mask were both supplied\"\n",
    "#         elif torch.is_autocast_enabled():\n",
    "#             why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "#         if not why_not_sparsity_fast_path:\n",
    "#             tensor_args = (\n",
    "#                 src,\n",
    "#                 first_layer.self_attn.in_proj_weight,\n",
    "#                 first_layer.self_attn.in_proj_bias,\n",
    "#                 first_layer.self_attn.out_proj.weight,\n",
    "#                 first_layer.self_attn.out_proj.bias,\n",
    "#                 first_layer.norm1.weight,\n",
    "#                 first_layer.norm1.bias,\n",
    "#                 first_layer.norm2.weight,\n",
    "#                 first_layer.norm2.bias,\n",
    "#                 first_layer.linear1.weight,\n",
    "#                 first_layer.linear1.bias,\n",
    "#                 first_layer.linear2.weight,\n",
    "#                 first_layer.linear2.bias,\n",
    "#             )\n",
    "#             _supported_device_type = [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n",
    "#             if torch.overrides.has_torch_function(tensor_args):\n",
    "#                 why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "#             elif src.device.type not in _supported_device_type:\n",
    "#                 why_not_sparsity_fast_path = f\"src device is neither one of {_supported_device_type}\"\n",
    "#             elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "#                 why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "#                                               \"input/output projection weights or biases requires_grad\")\n",
    "\n",
    "#             if (not why_not_sparsity_fast_path) and (src_key_padding_mask is not None):\n",
    "#                 convert_to_nested = True\n",
    "#                 output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
    "#                 src_key_padding_mask_for_layers = None\n",
    "\n",
    "#         seq_len = _get_seq_len(src, batch_first)\n",
    "#         is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n",
    "#        ################################################################################################################\n",
    "\n",
    "#         for mod in self.layers:\n",
    "#             output, attn_weight = mod(output, pos_enc, src_mask=mask, is_causal=is_causal, src_key_padding_mask=src_key_padding_mask_for_layers)\n",
    "\n",
    "#         if convert_to_nested:\n",
    "#             output = output.to_padded_tensor(0., src.size())\n",
    "\n",
    "#         if self.norm is not None:\n",
    "#             output = self.norm(output)\n",
    "\n",
    "#         return output, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Forecaster(torch.nn.Module):\n",
    "#     def __init__(self, mbae_encoder, activation):\n",
    "#         super().__init__()\n",
    "#         self.mbae_encoder = mbae_encoder\n",
    "#         self.pos_enc = PositionalEncoding(d_model[\"encoder\"], dropout)\n",
    "#         self.linear1 = torch.nn.Linear(d_model[\"encoder\"], d_model[\"encoder\"]) \n",
    "#         # self.forecaster = Encoder_(EncoderLayer_(d_model[\"decoder\"], nhead, d_ff[\"decoder\"], dropout, activation, batch_first=True), num_layers[\"decoder\"])\n",
    "#         self.forecaster = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(d_model[\"encoder\"], nhead, d_ff[\"encoder\"], dropout, activation, batch_first=True), 1)\n",
    "#         self.linear2 = torch.nn.Linear(d_model[\"encoder\"], 1)\n",
    "    \n",
    "#     def forward(self, data_input, remain_rto, device):\n",
    "#         encoding_dict, encoding_weight_dict,\\\n",
    "#             data_dict, idx_dict, padding_mask_dict = self.mbae_encoder(data_input, remain_rto, device)\n",
    "#         encoding = encoding_dict[\"global\"]\n",
    "#         encoding = self.linear1(encoding)\n",
    "#         # encoding = self.pos_enc(encoding)\n",
    "\n",
    "#         padding_mask = torch.where((padding_mask_dict[\"temporal_padding_mask\"].squeeze()==1), 0, -torch.inf)\n",
    "#         # output, weight = self.forecaster(encoding, src_key_padding_mask=padding_mask)\n",
    "#         output = self.forecaster(encoding, src_key_padding_mask=padding_mask)\n",
    "#         weight = None\n",
    "#         output = self.linear2(output).squeeze()\n",
    "\n",
    "#         return output, data_dict, padding_mask_dict, encoding_weight_dict, weight\n",
    "\n",
    "# model = Forecaster(mbae_encoder, \"gelu\")\n",
    "# model.to(device)\n",
    "# summary(model, valid_data, remain_rto, device, show_parent_layers=True, print_summary=True)\n",
    "# \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if \"img_model\" in name:\n",
    "#         param.requires_grad = False\n",
    "#     elif \"nlp_model\" in name:\n",
    "#         param.requires_grad = False\n",
    "#     # if \"mbae_encoder\" in name:\n",
    "#     #     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1830 [00:03<45:50,  1.50s/it]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 330.00 MiB (GPU 0; 23.67 GiB total capacity; 21.83 GiB already allocated; 29.25 MiB free; 23.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# raise\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# output, data_dict, padding_mask_dict, encoding_weight_dict, weight = model(data, remain_rto, device)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m output_dict, data_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremain_rto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# loss = mse_loss(output, data_dict[\"sales\"].squeeze())\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/architecture.py:109\u001b[0m, in \u001b[0;36mMaskedBlockAutoEncoder.forward\u001b[0;34m(self, data_input, remain_rto, device)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_input, remain_rto, device):\n\u001b[1;32m    106\u001b[0m     encoding_dict, encoding_weight_dict,\\\n\u001b[1;32m    107\u001b[0m         data_dict, idx_dict, padding_mask_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmbae_encoder(data_input, remain_rto, device)\n\u001b[0;32m--> 109\u001b[0m     decoding_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmbae_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoding_dict, data_dict, idx_dict, padding_mask_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/architecture.py:84\u001b[0m, in \u001b[0;36mMBAEDecoder.forward\u001b[0;34m(self, data_dict, padding_mask_dict, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m posmod_emb_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposmod_emb(data_dict, device)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 8. Decoding\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m decoding_dict, self_attn_weight_dict, cross_attn_weight_dict_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposmod_emb_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 9. Output\u001b[39;00m\n\u001b[1;32m     86\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(decoding_dict)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/mbae_decoder_module.py:105\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, data_dict, padding_mask_dict)\u001b[0m\n\u001b[1;32m    102\u001b[0m tgt_dict, tgt_key_padding_mask_dict, memory_dict, memory_key_padding_mask_dict\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tgt_memory(col, data_dict, padding_mask_dict)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layer_dict[col]:\n\u001b[0;32m--> 105\u001b[0m     tgt_dict, self_attn_weight, cross_attn_weight_dict, tgt_modality \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m result_dict[col] \u001b[38;5;241m=\u001b[39m tgt_dict[tgt_modality]\n\u001b[1;32m    108\u001b[0m self_attn_weight_dict[col] \u001b[38;5;241m=\u001b[39m self_attn_weight\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/mbae_decoder_module.py:37\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, tgt_dict, memory_dict, tgt_key_padding_mask_dict, memory_key_padding_mask_dict)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Cross attention\u001b[39;00m\n\u001b[1;32m     36\u001b[0m tgt_dict \u001b[38;5;241m=\u001b[39m {tgt_modality: x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m tgt_modality\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x}\n\u001b[0;32m---> 37\u001b[0m cross_attn_output_dict, cross_attn_weight_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ca_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mca_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m cross_attn_output_dict[tgt_modality]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Feed forward\u001b[39;00m\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/mbae_decoder_module.py:80\u001b[0m, in \u001b[0;36mDecoderLayer._ca_block\u001b[0;34m(self, tgt_dict, memory_dict, padding_mask_dict)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ca_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, tgt_dict, memory_dict, padding_mask_dict):\n\u001b[0;32m---> 80\u001b[0m     attn_output_dict, attn_weight_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_apply(attn_output_dict, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mca_dropout), attn_weight_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/cudatest/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/shared_module.py:42\u001b[0m, in \u001b[0;36mMultiheadBlockAttention.forward\u001b[0;34m(self, query_dict, key_dict, value_dict, key_padding_mask_dict)\u001b[0m\n\u001b[1;32m     39\u001b[0m attn_output_dict, attn_weight_dict \u001b[38;5;241m=\u001b[39m {}, {}\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict\u001b[38;5;241m.\u001b[39mkeys(): \n\u001b[0;32m---> 42\u001b[0m     attn_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal\u001b[39m\u001b[38;5;124m\"\u001b[39m], attn_weight_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_side_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemporal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     45\u001b[0m     attn_output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m], attn_weight_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_side_attention(query_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m\"\u001b[39m], key_dict, value_dict, key_padding_mask_dict)\n",
      "File \u001b[0;32m~/Test/Paper/MulTiMAE_v0.102.1 (fine tuning, tidy tidy)/architecture/shared_module.py:70\u001b[0m, in \u001b[0;36mMultiheadBlockAttention.temporal_side_attention\u001b[0;34m(self, query, key_dict, value_dict, key_padding_mask_dict)\u001b[0m\n\u001b[1;32m     67\u001b[0m     Qt_Ks_weight \u001b[38;5;241m=\u001b[39m temporal_attn_weight[:, :, :, :, \u001b[38;5;241m-\u001b[39mQt_Ks\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m     69\u001b[0m     Qt_Kt_Vt \u001b[38;5;241m=\u001b[39m Qt_Kt_weight \u001b[38;5;241m@\u001b[39m Vt\n\u001b[0;32m---> 70\u001b[0m     Qt_Ks_Vs \u001b[38;5;241m=\u001b[39m \u001b[43mQt_Ks_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVs\u001b[49m\n\u001b[1;32m     72\u001b[0m     QKV \u001b[38;5;241m=\u001b[39m Qt_Kt_Vt \u001b[38;5;241m+\u001b[39m Qt_Ks_Vs\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 330.00 MiB (GPU 0; 23.67 GiB total capacity; 21.83 GiB already allocated; 29.25 MiB free; 23.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "mse_loss = torch.nn.MSELoss(reduction=\"none\")\n",
    "ce_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "def train(e):\n",
    "    pbar = tqdm(valid_dataloader)\n",
    "    loss_li, mean_loss_li = [], []\n",
    "\n",
    "    for n, data in enumerate(pbar):\n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        # output, data_dict, padding_mask_dict, encoding_weight_dict, weight = model(data, remain_rto, device)\n",
    "        output_dict, data_dict, idx_dict, padding_mask_dict = model(data, remain_rto, device)\n",
    "        output = output_dict[\"sales\"].squeeze()\n",
    "\n",
    "        # loss = mse_loss(output, data_dict[\"sales\"].squeeze())\n",
    "        loss = mse_loss(output, data[\"sales\"].squeeze().to(device))\n",
    "        mask = torch.where((padding_mask_dict[\"temporal_padding_mask\"].squeeze()==1)&(padding_mask_dict[\"target_fcst_mask\"].squeeze()==0), 1, 0)\n",
    "\n",
    "        loss *= mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        loss_li.append(loss.item())\n",
    "        mean_loss_li.append(np.array(loss_li).mean())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # # Plot\n",
    "        # if n % 20 == 0:\n",
    "        #     nrows, ncols = 3, 2\n",
    "        #     plt.figure(figsize=(15, 3))\n",
    "        #     clear_output(wait=True)\n",
    "            \n",
    "        #     # Loss\n",
    "        #     plt.subplot(nrows, ncols, 1)\n",
    "        #     plt.plot(mean_loss_li)\n",
    "        #     plt.title(mean_loss_li[-1])\n",
    "\n",
    "        #     # Sampleself_attn_weight\n",
    "        #     plt.subplot(nrows, ncols, 2)\n",
    "        #     idx = 0\n",
    "        #     pred = torch.where(mask==1, output, torch.nan)[idx]\n",
    "        #     y = torch.where(mask==1, data_dict[\"sales\"].squeeze(), torch.nan)[idx]\n",
    "        #     y = torch.where(mask==1, data[\"sales\"].squeeze().to(device), torch.nan)[idx]\n",
    "\n",
    "        #     # pred = output[idx]\n",
    "        #     # y = data_dict[\"sales\"].squeeze()[idx]\n",
    "\n",
    "        #     pred = pred[~torch.isnan(pred)]\n",
    "        #     y = y[~torch.isnan(y)]\n",
    "\n",
    "        #     plt.plot(y.detach().cpu())\n",
    "        #     plt.plot(pred.detach().cpu())\n",
    "\n",
    "        #     # Weight\n",
    "        #     temporal_weight = encoding_weight_dict[\"temporal\"]\n",
    "\n",
    "        #     mean_temporal_weight = temporal_weight.mean(dim=1)\n",
    "        #     min_temporal_weight = temporal_weight.min(dim=1).values\n",
    "            \n",
    "        #     plot_temporal_weight = min_temporal_weight[idx, :, 0, 7+2:7+2+14*14]\n",
    "        #     mask = mask[idx].unsqueeze(-1).repeat(1, plot_temporal_weight.shape[-1])\n",
    "        #     plot_temporal_weight = torch.where(mask==1, plot_temporal_weight, torch.nan)\n",
    "        #     plot_temporal_weight = plot_temporal_weight[~torch.isnan(plot_temporal_weight)].reshape(-1, plot_temporal_weight.shape[-1])            \n",
    "            \n",
    "        #     plt.subplot(nrows, ncols, 3)\n",
    "        #     # plot_temporal_weight = plot_temporal_weight.min(dim=0).values\n",
    "        #     plot_temporal_weight = plot_temporal_weight.mean(dim=0)\n",
    "        #     # plot_temporal_weight = plot_temporal_weight[-1]\n",
    "        #     plot_temporal_weight = plot_temporal_weight.reshape(224//patch_size,224//patch_size)\n",
    "        #     plt.imshow(plot_temporal_weight.detach().cpu())\n",
    "\n",
    "        #     plt.subplot(nrows, ncols, 4)\n",
    "        #     plt.imshow(data_dict[\"img_path\"].permute(0,2,3,1).detach().cpu()[idx])\n",
    "\n",
    "        #     # plt.subplot(nrows, ncols, 5)\n",
    "        #     # sns.heatmap(weight[idx].mean(dim=0).detach().cpu())\n",
    "\n",
    "        #     plt.tight_layout()\n",
    "        #     plt.show()\n",
    "\n",
    "\n",
    "epoch = 1\n",
    "for e in range(epoch):\n",
    "    train(e)\n",
    "    scheduler.step()\n",
    "    # raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
